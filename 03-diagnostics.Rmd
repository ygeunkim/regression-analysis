# Model Adequacy and Regression Diagnostics

$$Y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \epsilon_{i}, \quad \epsilon_i \iid N(0, \sigma^2)$$

From this regression model, we conduct analysis such as

- estimate $\hb$ and $\hat\sigma^2$
- inference
- predict $\hat{Y}$
- ANOVA

However, all these results make sense only when the model satisfies its assumption. @Chatterjee:2015aa categorizes the assumption into four: form of the model, error term, predictors, and observations.


## The Standard Regression Assumptions

```{r, include=FALSE}
delv <- MPV::p2.9 %>% tbl_df()
delv_fit <- lm(y ~ x, data = delv)
```

### Linearity assumption {#linassumption}

First of all, the relation between response $Y$ and predictors $X_1, \ldots, X_p$ is assumed to be linear.

$$E(Y \mid \mathbf{X} = \mathbf{x}) = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}$$

```{r linassume, fig.cap="Linearity assumption"}
delv %>% 
  add_predictions(delv_fit) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_linerange(aes(ymin = y, ymax = pred), col = gg_hcl(1)) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```

### Errors {#errassumption}

Error term $\E$ is assumed as

$$\epsilon_i \iid N(0, \sigma^2)$$

This involves the following assumptions.

1. **Mean-zero assumption**: $\epsilon_i$ has zero mean.
2. **Homoskedasticity** (or homogeneity): $\epsilon_i$ has constant variance. If variance varies according to observation, we call it as *heteroskedasticity* (or heterogeneity).
3. **Independence assumption**: $\epsilon_i$ is mutually independent.
4. **Normality assumption**: $\epsilon_i$ follows Normal distribution.

As mentioned many times, $\epsilon_j$ cannot be observed. Instead, we gain information from **residuals**.

```{r rsdassume, fig.cap="Residual plot preview - Does this model satisfy the assumptions?"}
delv %>% 
  add_predictions(delv_fit) %>% 
  add_residuals(delv_fit) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_ref_line(h = 0) +
  geom_linerange(aes(ymin = resid, ymax = 0), col = gg_hcl(1), linetype = "dotted") +
  geom_point()
```

### Predictors {#xassumption}

- **Non-random**: While $Y$ is random variable, $X_1, \ldots, X_p$ are not. They are assumed fixed or conditioned, i.e. $E(Y \mid \mathbf{X} = \mathbf{x})$.
- **measured without error**: We assume here that $x_{i1}, \ldots, x_{ip}$ are measured without error, i.e. there is *no measurement error*. If there is, the measurement error will affect every residual variance, regression coefficient, et cetera. Let $w_i$ be the measurement error of $i$-th observation. Then what we observe is $Z_i = x_i + w_i$, not $x_i$. In turn, we estimate regression coefficient and the others using $(Z_i, Y_i)$, i.e. relationship between $Z_i$ and $Y_i$. *Residual variance will increase by construction*. Additionaly, correlation coefficients will be reduced.
    - We have assumed that there is no measurement error, but we already know that *it is hardly not true*.
    - So correction for the errors can be considered. But it requires the ratio between $Var(w_i)$ and $Var(\epsilon_i)$, which we seldomly know.
    - As a result, we just sacrifice some accuracy for impossible correcting task.
- **Linear independence**: A set of predictor variables $\{ X_1, \ldots, X_p \}$ is linearly independent. Recall Theorem \@ref(thm:fullrank2). This makes the design matrix full rank and guarantee unique least squares solution. This is violated by so-called *multicollinearity* problem.

First two assumptions cannot be validated, but should be always remembered in the interpretation of the analysis results.

### Observations {#obsassumption}

```{r, include=FALSE}
tmp <- 
  tibble(x = runif(50), y = x) %>% 
  gather() %>% 
  add_row(key = "y", value = 20)
```

Every observation is equally reliable and plays an equal role in determining the results. When we estimate mean, for instance, each $X_1, \ldots, X_n$ has equal role so that

$$\hat\mu = \frac{1}{n} X_1 + \cdots + \frac{1}{n} X_n = \overline{X}$$

```{r outassume, echo=FALSE, fig.cap="Existence of Outlier"}
tmp %>% 
  ggplot(aes(y = value, group = key)) +
  geom_boxplot(aes(middle = mean(value)), outlier.color = gg_hcl(1)) +
  stat_summary(fun.y = mean, aes(x = 0), geom = "point", shape = 23, size = 2, col = I("red"), fill = I("red")) +
  coord_flip() +
  facet_grid(key ~ .) +
  theme(axis.text.y = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank())
```

```{r, include=FALSE}
rm("tmp")
```

However, if there exists *outlier*, it can be changed critically. See Figure \@ref(fig:outassume). In lower box plot, just one outlier is added: `20`. Each red dot is average. Thus, we should be careful about *outliers*. In regression literature, we would see the following observations.

- Leverage point
- Influential point


## Residuals

### Raw residual

Looking at $\epsilon_i$, we might be able to check if the model violates the assumptions directly in section \@ref(linassumption) and \@ref(errassumption). The problem is $\epsilon_i$ is non-observable. So as a surrogate, we use residuals

\begin{equation}
  e_i := Y_i - \hat{Y_i} = Y_i - (\hat\beta_0 + \hat\beta_1 x_{i1} + \cdots + \hat\beta_p x_{ip}), \quad i = 1, \ldots, n
  (\#eq:residualdef)
\end{equation}

We already know that `residuals(model)` or `model$residuals` gives residuals. `modelr::add_residuals(model)`, additionaly, mutates a colum named `resid` by default. We can change its name with `var` argument. See Figure \@ref(fig:rsdassume) and its code.

Let

$$\mathbf{e} := (e_1, \ldots, e_p)^T$$

From Proposition \@ref(prp:multresprop),

$$\mathbf{e} \sim N(\mathbf{0}, \sigma^2(I - H_X)), \quad H_X = \text{projection onto}\: R(X)$$

This implies that

$$Var(e_i) = (1 - h_{ii})\sigma^2$$

where $h_{ii}$  is the $i$-th diagonal element of $H_X$. Recall that

$$Var(\epsilon_i) = \sigma^2$$

As a backup of $\sigma^2$, $e_i$ should reflect the assumption of $\epsilon_i$. *Scaling* might be needed.


### Standardized residual

Applying usual standardization procedure, one may use

\begin{equation}
  e_i^{\ast} = \frac{e_i}{\sigma \sqrt{1 - h_{ii}}}
  (\#eq:stdunknown)
\end{equation}

$\sigma$ is unkown. So replace $\sigma$ with $\hat\sigma$.

```{definition, stdresid, name = "Standardized residual"}
Standardized residual can be obtained by replacing $\sigma$ with $\hat\sigma$.

$$d_i := \frac{e_i}{\hat\sigma \sqrt{1 - h_{ii}}}$$

where $\hat\sigma^2 = \frac{\sum\limits_{i = 1}^n e_i^2}{n - p - 1}$.
```

We can get values of $d_i$ with `rstandard()`.

```{r}
rstandard(delv_fit)
```

In fact, we can get both residuals using `broom::augment()`.

- `.fitted`: fitted values
- `.resid`: raw residuals
- `.std.resid`: standardized residuals

```{r rstdresid, fig.cap="Standardized residuals"}
delv_fit %>% 
  broom::augment() %>% 
  gather(.resid, .std.resid, key = "residual", value = "value") %>% 
  ggplot(aes(x = .fitted, y = value, colour = residual)) +
  geom_ref_line(h = 0) +
  geom_linerange(aes(ymin = value, ymax = 0), linetype = "dotted") +
  geom_point() +
  labs(
    y = "Residuals",
    x = "Prediction"
  ) +
  scale_colour_discrete(
    name = "Residuals",
    label = c("Raw", "Std")
  )
```


Does this $d_i$ follow $t(n - p - 1)$? $Z := \frac{e_i}{\sigma \sqrt{1 - h_{ii}}} \sim N(0, 1)$ and $X := \frac{\hat\sigma}{\sigma} \sim \chi^2(n - p - 1)$. In the other context, this typically leads to $t$-distribution. Here, however, $Z$ and $X$ are not independent. Denote that $\hat\sigma^2 = \frac{\sum\limits_{i = 1}^n e_i^2}{n - p - 1}$ includes $e_i$, so it cannot be independent with $e_i$. Thus, $d_i$ is not $t$ distributed.

### Studentized residual

To force $d_i$ to be $t$ random variable, we just prevent from each $e_i$ encountering each $e_i$ in $\hat\sigma^2$. This can be done by computing MSE without $i$-th observation, i.e. using data set

$$
D_{(-i)} := \begin{bmatrix}
  (\mathbf{x}_1, Y_1) \\
  (\mathbf{x}_2, Y_2) \\
  \vdots \\
  \msout{(\mathbf{x}_i, Y_i)} \\
  \vdots \\
  (\mathbf{x}_n, Y_n)
\end{bmatrix}
$$

compute MSE $\hat\sigma_{(-i)}^2$.

```{definition, studresid, name = "Studentized residual"}
The studentized residual is defined by

$$r_i := \frac{e_i}{\hat\sigma_{(-i)} \sqrt{1 - h_{ii}}}$$

where $\hat\sigma_{(-i)}^2$ is MSE from data without $i$-th observation, $D_{(-i)}$.
```

In principle, we need to fit $n$ different models to get this residuals. In fact, this is not necessary because $\hat\sigma_{(-i)}^2$ has some relation to $\hat\sigma^2$.

```{lemma, studsig}
$\hat\sigma_{(-i)}^2$ can be computed from the original $\hat\sigma^2$.

$$\hat\sigma_{(-i)}^2 = \frac{(n - p - 1)\hat\sigma^2 - \frac{e_i^2}{1 - h_{ii}}}{n - p - 1}$$
```

This lemma removes repeating procedure.

```{corollary}
Studentized residuals and standardized residuals are related by

$$r_i = d_i \frac{n - p - 2}{n - p - 1 - d_i^2}$$

where $d_i$ are standardized residuals and $r_i$ are studentized residuals.
```

`rstudent()` gives this $r_i$ in `R`.

```{r}
rstudent(delv_fit)
```

```{r}
# n = 25
# p = 1
rstandard(delv_fit) * sqrt((25 - 3) / (25 - 2 - rstandard(delv_fit)^2))
```

`broom::(augment)` does not provide `rstudent()` yet, so here `mutate` them all in hand.

```{r rstudresid, fig.cap="Studentized residuals"}
delv %>% 
  add_predictions(delv_fit) %>% 
  mutate(
    resid = residuals(delv_fit),
    std = rstandard(delv_fit),
    stud = rstudent(delv_fit)
  ) %>% 
  gather(resid, std, stud, key = "residual", value = "value") %>% 
  ggplot(aes(x = pred, y = value, colour = residual)) +
  geom_ref_line(h = 0) +
  geom_linerange(aes(ymin = value, ymax = 0), linetype = "dotted") +
  geom_point() +
  labs(
    y = "Residuals",
    x = "Prediction"
  ) +
  scale_colour_discrete(
    name = "Residuals",
    label = c("Raw", "Std", "Student")
  )
```


@Chatterjee:2015aa names the above three \@ref(eq:stdunknown) $e_i^{\ast}$, \@ref(def:stdresid) $d_i$, and \@ref(def:studresid) $r_i$ as follows.

```{remark}
The form of residual in Equation \@ref(eq:stdunknown), Definition \@ref(def:stdresid), and \@ref(def:studresid) can be called as

\begin{enumerate}
  \item $e_i^{\ast} := \frac{e_i}{\sigma \sqrt{1 - h_{ii}}}$ \textit{Standardized residual}
  \item $d_i := \frac{e_i}{\hat\sigma \sqrt{1 - h_{ii}}}$ \textit{Internally studentized residual}
  \item $r_i := \frac{e_i}{\hat\sigma_{(-i)} \sqrt{1 - h_{ii}}}$ \textit{Externally studentized residual}
\end{enumerate}
```

The word *internal* and *external* is due to direct involvement of $\hat\sigma$ in $e_i$. In externally studentized residual, $\hat\sigma_{(-i)}$ is not involved in $e_i$, i.e. external to $e_i$.


## Residual Plots

Graphical methods are able to become some kind of caveat. It is effective way to investigate model adequacy.

```{r, include=FALSE}
cem <- MPV::cement %>% tbl_df()
cem_fit <- lm(y ~ ., data = cem)
```

```{r}
gg_fit <- function(data, mapping) {
  pt <- gg_scatter(data, mapping = mapping, alpha = 1)
  pt$layers <- c(geom_smooth(method = "lm"), pt$layers)
  pt # point layer on line layer
}
#------------------------------------
draw_dot <- function(data, mapping, ...) {
  data %>% 
    ggplot(mapping = mapping) +
    geom_dotplot(...)
}
```

Since `gg_scatter()` is pre-defined function drawing a scatter plot, we can change the order of layers by indexing its `layers`. This is because every `geom_*` and `stat_*` returns `layer()`. For example,

```{r}
geom_smooth
```

`GGally::ggpairs()` easily draws a matrix of plots. We can specify each `lower`, `upper`, and `diag`.

```{r plmat, message=FALSE, fig.cap="Plot matrix for cement data set"}
cem %>% 
  GGally::ggpairs(
    lower = list(continuous = gg_fit), # regression
    diag = list(continuous = draw_dot) # dot plot
  )
```

Plots that have scatter plot in the matrix form and pairwise correlation such as Figure \@ref(fig:plmat) is called *plot matrix* or *scatter matrix*. First column let us know the relationship between `y` and each `x`. Correlation is important in that we have assumed linear independence. This would be covered later.

### Residual plot

Direct illustration of $Y$ and $X_1, \ldots, X_p$ is helpful, but has its limit to check the assumptions. Residuals might be more informative. Among the three we have defined, we often draw *externally studentized residual* \@ref(def:studresid), i.e. `rstudent()`.

$$r_i := \frac{e_i}{\hat\sigma_{(-i)} \sqrt{1 - h_{ii}}}$$

```{definition, residplot, name = "Residual plot"}
Residual plot is a plot of $r_i$ versus corresponding fitted values $\hat{Y_i}$
```

In `modelr` syntax,

```{r}
add_rstudent <- function(data, model, var = ".stud.resid") {
  data[[var]] <- rstudent(model, x = data)
  data
}
#----------------------------------------
residplot <- function(data, model, ...) {
  data %>% 
    add_predictions(model) %>% 
    add_rstudent(model) %>% 
    ggplot(data, mapping = aes(x = pred, y = .stud.resid)) +
    geom_ref_line(h = 0) +
    geom_point(...)
}
```

```{r cemrsplot, fig.cap="Residual plot for regression with cement data set"}
residplot(cem, cem_fit) +
  labs(
    x = "Fitted values",
    y = "Residuals"
  )
```

Denote that

$$r_i \stackrel{iid}{\approx} N(0, 1)$$

So we can deduct some ideal behavior of $r_i$.

```{remark}
Ideally, residual plot should be

\begin{itemize}
  \item have \textit{no systematic pattern}
  \item equal variance, i.e. \textit{variability of} $r_i$ \textit{shows constancy}, independent of $\hat{Y_i}$
  \item most $r_i$s fall between $-2$ and $2$, approximately $95\%$
\end{itemize}
```

Now consider various scenarios and see how to interpret this plot.

```{r, include=FALSE}
hetero <- 
  tibble(
    x = seq_len(500),
    y = 1 + 2 * x,
  ) %>% 
  arrange(y) %>% 
  mutate(y = y + rnorm(500, sd = 1:n()))
hetero_fit <- lm(y ~ x, data = hetero)
```

```{r heteropt, fig.cap="Heteroskedasticity"}
residplot(hetero, hetero_fit)
```

As $i$ grows, variance becomes larger. Constant variance assumption is violated.

```{r, include=FALSE}
cubic <- 
  tibble(
    x = rnorm(500),
    y = 1 + 2 * x + x^2 + 3 * x^3 + rnorm(500)
  )
cubic_fit <- lm(y ~ x, data = cubic)
```

```{r nonlinpt, fig.cap="Non-linear relationship and autocorrelation"}
residplot(cubic, cubic_fit)
```

By its definition, residuals are values after removing linear effects. If the data embeds non-linear relationships, it would not be removed and it would be still remained as pattern. Figure \@ref(fig:nonlinpt) indicates that $Y$ and $x$ have non-linear relationship.

```{r, include=FALSE}
outlier <- 
  tibble(
    x = seq_len(500),
    y = 1 + 2 * x + rnorm(500)
  ) %>% 
  add_row(x = c(-10, 12), y = c(0, 2)) %>% 
  mutate(isout = c(rep(FALSE, 500), rep(TRUE, 2)))
outlier_fit <- lm(y ~ x, data = outlier)
```

```{r outpt, fig.cap="Existence of outlier"}
residplot(outlier, outlier_fit) +
  geom_point(aes(colour = isout), na.rm = TRUE, shape = 1, size = 3, show.legend = FALSE) +
  scale_colour_manual(values = c("TRUE" = "red", "FALSE" = NA))
```

Large values of residuals implies they are outliers.

### Normal quantile-quantile plot

We expect that

$$r_i \iid N(0, 1)$$

Normal quantile-quantile plot (q-q plot) or normal probability plot can tell us whether observations of $r_i$ is closed to *Normal distribution*.

```{definition, qqpt, name = "Normal q-q plot"}
Normal Q-Q plot is the plot of \textit{ordered studentized residuals} $r_{(1)} < r_{(2)} < \cdots < r_{(n)}$ versus \textit{theoretical normal quantiles} $\Phi^{+}\bigg( \frac{i - \frac{3}{8}}{n + \frac{1}{4}} \bigg)$.
```

*If Q-Q plot is close to a  straight line, this supports the Normality of residuals.* Otherwise, we can say that the assumption is violated.

`ggplot2` provides `stat_qq_line()` and `stat_qq()`. One draws the guide line and the other draws a points of q-q plot. There is an argument `distribution`. We can compare observations with any distribution function. The default, of course, is `stats::qnorm`, i.e. normal distribution.

```{r qqcement, fig.cap="Q-Q plot for cement residuals"}
cem %>% 
  add_rstudent(cem_fit) %>% 
  ggplot(aes(sample = .stud.resid)) +
  stat_qq_line(
    distribution = stats::qnorm,
    col = I("white"),
    size = 2
  ) +
  stat_qq(distribution = stats::qnorm)
```

In general, we filter *skewness* here.

```{r}
draw_qq <- function(data, model, distribution = stats::qnorm, ...) {
  data %>% 
    add_rstudent(model) %>% 
    ggplot(aes(sample = .stud.resid)) +
    stat_qq_line(distribution = distribution, ...) +
    stat_qq(distribution = distribution) +
    xlab("Theoretical Quantiles")
}
```

```{r, include=FALSE}
ideal <- 
  tibble(
    x = seq_len(500),
    y = 1 + 2 * x + rnorm(500)
  )
ideal_fit <- lm(y ~ x, data = ideal)
```

```{r idqq, fig.cap="Q-Q plot - Ideal case"}
draw_qq(ideal, ideal_fit, col = I("white"), size = 2)
```

Figure \@ref(fig:idqq) shows the ideal case. Observations resemble the straight line.

```{r, include=FALSE}
fat <- 
  tibble(
    x = seq_len(500),
    y = 1 + 2 * x + rt(500, df = 5)
  )
fat_fit <- lm(y ~ x, data = fat)
```

```{r qqt, fig.cap="Q-Q plot - Heavy-tailed distribution"}
draw_qq(fat, fat_fit, col = I("white"), size = 2)
```

Recall that the $x$-axis is *theoretical* normal quantile and $y$-axis is the *observed* ordered studentized residuals. In case of small values of $r_i$, i.e. *left tail*, observed values indicating empirical quantiles are less than theoretical quantiles. *Smaller quantiles at left tail means heavier left tail*. At *right tail*, on the other hand, observed values indicating empirical quantiles are larger than theoretical quantiles. *Larger quantiles at right tail means heavier right tail*. In sum, Figure \@ref(fig:qqt) form is of heavy-tailed distribution.

```{r, include=FALSE}
thin <- 
  tibble(
    x = seq_len(500),
    y = 1 + 2 * x + runif(500)
  )
thin_fit <- lm(y ~ x, data = thin)
```

```{r qqunif, fig.cap="Q-Q plot - Light-tailed distribution"}
draw_qq(thin, thin_fit, col = I("white"), size = 2)
```

This is exactly opposite result with Figure \@ref(fig:qqt). Larger empirical quantiles at left tail implies light left tail. Smaller empirical quantiles at right tail implies light right tail. This is the form of light-tailed distribution.

```{r, include=FALSE}
right <- 
  tibble(
    x = seq_len(500),
    y = 1 + 2 * x + rchisq(500, df = 1)
  )
right_fit <- lm(y ~ x, data = right)
```

```{r qqchisq, fig.cap="Q-Q plot - Positive skew"}
draw_qq(right, right_fit, col = I("white"), size = 2)
```

In this case, $r_i$ is observed as heavier right tail than left. This is called *positive skew* or *right skew*, i.e. its mass leans to the left.

```{r, include=FALSE}
left <- 
  tibble(
    x = seq_len(500),
    y = 1 + 2 * x + sn::rsn(500, alpha = -5)
  )
left_fit <- lm(y ~ x, data = left)
```

```{r qqskewn, fig.cap="Q-Q plot - Negative skew"}
draw_qq(left, left_fit, col = I("white"), size = 2)
```

Left tail is observed heavier than right. This is called *negative skew* or *left skew*. Its mass leans to the right.

### Partial residual plots


## Outliers

An outlier is an extreme observation. Outliers occurr in two direction: $X$ and $Y$. There exist appropriate measures to find outliers in corresponding direction.

### Leverage

Consider the design matrix and its row vectors

$$
X = \begin{bmatrix}
  \mathbf{x}_1^T \\
  \mathbf{x}_2^T \\
  \vdots \\
  \mathbf{x}_n^T
\end{bmatrix}
$$

Note that each $\mathbf{x}_i^T = (1, x_{i1}, x_{i2}, \ldots, x_{ip})^T$ represents $i$-th observation.

```{definition, hati, name = "Leverage"}
Let $H$ be hat matrix, i.e. projection onto $Col(X)$. Then leverage is defined by its $i$-th diagonal element.

$$h_{ii} := \mathbf{x}_i^T (X^T X)^{-1} \mathbf{x}_i$$
```

Let us consider the simple linear regression model, $p = 1$. Then we have

$$
(X^T X)^{-1} = \frac{1}{\sigma^2} Var(\hb) = \begin{bmatrix}
  \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{xx}}) & - \frac{\overline{x}}{S_{xx}} \\
  \frac{\overline{x}}{S_{xx}} & \frac{1}{S_{xx}}
\end{bmatrix}
$$

It follows that

\begin{equation*}
  \begin{split}
    h_{ii} & = \begin{bmatrix}
      1 & x_i
    \end{bmatrix} \begin{bmatrix}
    \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{xx}}) & - \frac{\overline{x}}{S_{xx}} \\
    \frac{\overline{x}}{S_{xx}} & \frac{1}{S_{xx}}
  \end{bmatrix} \begin{bmatrix}
    1 \\
    x_i
  \end{bmatrix} \\
  & = \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{xx}}
  \end{split}
\end{equation*}

$(x_i - \overline{x})^2$ implies that $h_{ii}$ becomes larger as $x_i$ is far from $\overline{x}$. In other words, $h_{ii}$ represents how far $x_i$ is away from the center. Extending to $p$, $h_{ii}$ represents *how far* $\mathbf{x}_i$ *is away from the center of observations*.

```{proposition, hii, name = "Properties of $h_{ii}$"}
Leverage values $h_{ii}$ possess several properties

\begin{enumerate}
  \item $\frac{1}{n} \le h_{ii} \le 1$
  \item $\sum\limits_{i = 1}^n h_{ii} = p + 1 \Rightarrow \overline{h} = \frac{1}{n} \sum h_{ii} = \frac{p + 1}{n}$
  \item $Var(\hat{Y_i}) = h_{ii}\sigma^2$
\end{enumerate}
```

```{conjecture, highlev, name = "High leverage point"}
If $h_{ii} > 2\overline{h} = \frac{2(p + 1)}{n}$ (twice the average value), then we regard $i$-th observation as \textbf{\textit{high leverage point}}.

If $i$-th observation is a high leverage point, we can consider that this observation is \textbf{\textit{unusual}} in $X$-space.
```

In `R`, $h_{ii}$ can be get in various ways. `influence()` returns `list` including `hat` which is leverage values. `broom::augment()` has a column `.hat` and this is the leverage values.

```{r}
broom::augment(cem_fit) %>% 
  select(.hat)
```

High leverage point is potentially dangerous for estimation of regression coefficients.

$$\hat{Y}_i = h_{i1} Y_1 + h_{i2} Y_2 + \cdots + {\color{red}{h_{ii}}} Y_i + \cdots + h_{in} Y_n$$

$h_{ii}$ is a function of $\mathbf{x}_i$. By construction, it measures a role of each $\mathbf{x}_i$ in weight of observation $Y_i$ in determining $\hat{Y}_i$. So a small change of $Y_i$ corresponding to a high leverage can dramatically change the estimators. However, *high leverage points are not always influential points*.

### Influence measure

Influence points are the points that can change the values of estimates by their existence. To see the influence of each data point, we can focus on this meaning. *How much would the regression results change if the* $i$*-th observation were deleted?*

Consdier $D_{(-i)}$. Let $X_{(-i)} \in (n - 1) \times (p + 1)$ be the design matrix from this data set, i.e. design matrix without $i$-th observation.

```{lemma, xtranx}
Let $X$ be any design matrix. Then

$$
X = \begin{bmatrix}
  \mathbf{x}_1^T \\
  \vdots \\
  \mathbf{x}_i^T \\
  \vdots \\
  \mathbf{x}_n^T
\end{bmatrix}
$$

where $\mathbf{x}_i = (1, \mathbf{x}_{i1}, \ldots, \mathbf{x}_{ip})^T$. By construction,

$$X^T X = \sum_{i = 1}^n \mathbf{x}_i \mathbf{x}_i^T$$

Let $Y = (Y_1, \ldots, Y_n)^T$ be any observation vector. Then

$$X^T \mathbf{Y} = \sum_{i = 1}^n \mathbf{x}_i Y_i$$
```

```{proof}
It is just arithmetic.

$$
X^TX = \begin{bmatrix}
  \mathbf{x}_1 & \cdots & \mathbf{x}_i & \cdots & \mathbf{x}_n
\end{bmatrix} \begin{bmatrix}
  \mathbf{x}_1^T \\
  \vdots \\
  \mathbf{x}_i^T \\
  \vdots \\
  \mathbf{x}_n^T
\end{bmatrix} = \sum_{i = 1}^n \mathbf{x}_i \mathbf{x}_i^T
$$

and

$$
X^T \mathbf{Y} = \begin{bmatrix}
  \mathbf{x}_1 & \cdots & \mathbf{x}_i & \cdots & \mathbf{x}_n
\end{bmatrix} \begin{bmatrix}
  Y_1 \\
  \vdots \\
  Y_i \\
  \vdots \\
  Y_n
\end{bmatrix} = \sum_{i = 1}^n \mathbf{x}_i Y_i
$$
```

```{lemma, suminv}
It can be shown that

$$(A + BCB)^{-1} = A^{-1} - A^{-1}B(C^{-1} + B^TA^{-1}B)^{-1}B^TA^{-1}$$
```

From Lemma \@ref(lem:xtranx),

\begin{equation}
  \begin{split}
    \hb_{(-i)} & = (X_{(-i)}^T X_{(-i)})^{-1} X_{(-i)}^T \mathbf{Y}_{(-i)} \\
    & = \bigg(\sum_{j \neq i} \mathbf{x}_j \mathbf{x}_j^T \bigg)^{-1} \bigg( \sum_{j \neq i} \mathbf{x}_j Y_j \bigg) \\
    & = (X^TX - \mathbf{x}_i \mathbf{x}_i^T)^{-1} (X^T\mathbf{Y} - \mathbf{x}_i Y_i)
  \end{split}
  (\#eq:hbnoi)
\end{equation}

In Lemma \@ref(lem:suminv), take $A = X^TX$, $B = \mathbf{x}_i$, and $C = -1$. It gives that

\begin{equation*}
  \begin{split}
    (X^TX - \mathbf{x}_i \mathbf{x}_i^T)^{-1} & = (X^TX)^{-1} - (X^TX)^{-1}\mathbf{x}_i (-1 + \mathbf{x}_i^T (X^TX)^{-1}\mathbf{x}_i )^{-1} \mathbf{x}_i^T (X^TX)^{-1} \\
    & = (X^TX)^{-1} + \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i \mathbf{x}_i^T (X^TX)^{-1}
  \end{split}
\end{equation*}

Thus,

\begin{equation*}
  \begin{split}
    \hb_{(-i)} & = (X^TX - \mathbf{x}_i \mathbf{x}_i^T)^{-1} (X^T\mathbf{Y} - \mathbf{x}_i Y_i) \\
    & = \bigg[ (X^TX)^{-1} + \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i \mathbf{x}_i^T (X^TX)^{-1} \bigg] (X^T\mathbf{Y} - \mathbf{x}_i Y_i) \\
    & = (X^TX)^{-1}X^T\mathbf{Y} - (X^TX)^{-1}\mathbf{x}_i Y_i \\
    & \qquad + \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i \mathbf{x}_i^T (X^TX)^{-1} X^T \mathbf{Y} -  \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i \mathbf{x}_i^T (X^TX)^{-1} \mathbf{x}_i Y_i \\
    & = \hb - (X^TX)^{-1}\mathbf{x}_i Y_i + \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i \mathbf{x}_i^T \hb - \frac{h_{ii}}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i Y_i \\
    & = \hb - \bigg[ 1 + \frac{h_{ii}}{1 - h_{ii}} \bigg] (X^TX)^{-1} \mathbf{x}_i Y_i + \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i \hat{Y}_i \quad \because \mathbf{x}_i^T \hb = \hat{Y}_i \\
    & = \hb - \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i ( Y_i - \hat{Y}_i) \quad \because 1 + \frac{h_{ii}}{1 - h_{ii}} = \frac{1}{1 - h_{ii}} \\
    & = \hb - \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i e_i
  \end{split}
\end{equation*}

```{theorem, hbnoithm}
Regression coefficient without $i$-th observation can come from the original desing matrix. Also, it is affected by $i$-th residual.

$$\hb_{(-i)} = \hb - \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i e_i$$
```

### PRESS residual

See Theorem \@ref(thm:hbnoithm). Previous residuals might be able to detect outliers. However, other kind of residuals can be considered here.

```{definition, prsresid, name = "PRESS residuals"}
$PRESS$ residuals are defined by

$$e_{i, -i} = Y_i - \hat{Y}_{i, -i}$$

where $\hat{Y}_{i, -i} = \mathbf{x}_i^T \hb_{(-i)}$ is the fitted value of the $i$-th response without $i$-th observation.
```

Note that

$$\hat{Y}_{i, -i} = \mathbf{x}_i^T \hb_{(-i)}$$

From Theorem \@ref(thm:hbnoithm), we can get a useful identity about PRESS residual.

```{theorem, prsraw}
PRESS residuals are related to Raw residuals $e_i$ and leverage values $h_{ii}$.

$$e_{i, -i} = \frac{e_i}{1 - h_{ii}}$$
```

```{proof}
Theorem \@ref(thm:hbnoithm) implies that

\begin{equation*}
  \begin{split}
    e_{i, -i} & = Y_i - \hat{Y}_{i, -i} \\
    & = Y_i - \mathbf{x}_i^T \hb_{(-i)} \\
    & = Y_i - \mathbf{x}_i^T \bigg[  \hb - \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i e_i \bigg] \\
    & = Y_i - \mathbf{x}_i^T \hb + \frac{h_{ii}}{1 - h_{ii}}e_i \\
    & = \frac{e_i}{1 - h_{ii}} \quad \because Y_i - \mathbf{x}_i^T \hb = e_i
  \end{split}
\end{equation*}
```

Althogh the definition of PRESS requires refitting different models, this Thoerem \@ref(thm:prsraw) let us easily compute $e_{i, -i}$ without refitting data.

```{remark, name = "Standardized PRESS residual"}
We standardize PRESS residuals $e_{i, -i}$ by

$$\frac{e_{i, -i}}{\sqrt{Var(e_{i, -i})}} = \frac{e_i / 1 - h_{ii}}{\sqrt{\sigma^2 / (1 - h_{ii})}} = \frac{e_i}{\sigma \sqrt{1 - h_{ii}}}$$

It is same as \textit{standardized residuals} if replacing $\sigma^2$ with $\hat\sigma^2$.
```


### DFFITS

```{definition, dff, name = "DFFITS"}
$$(DFFITS)_i := \frac{\hat{Y}_i - \hat{Y}_{i, -i}}{\hat\sigma_{(-i)} \sqrt{h_{ii}}}$$

where $\hat{Y}_i = \mathbf{x}_i^T \hb$ and $\hat{Y}_{i, -i} = \mathbf{x}_i^T \hb_{(-i)}$
```

```{corollary, dffrepresent}
$$(DFFITS)_i = \frac{e_i}{\hat\sigma_{(-i)\sqrt{h_{ii}}}} \times \sqrt{\frac{h_{ii}}{1 - h_{ii}}} = \text{studentized residual} \times \text{leverage measure}$$
```

```{proof}
\begin{equation*}
  \begin{split}
    \hat{Y}_i - \hat{Y}_{i, -i} & = (Y_i - \hat{Y}_i) - (Y_i - \hat{Y}_{i, -i}) \\
    & = e_{i, -i} - e_i \\
    & = \frac{h_{ii}}{1 - h_{ii}} e_i
  \end{split}
\end{equation*}
```

The second part represents leverage. As leverage values become large, i.e. $h_{ii} \rightarrow 1$ (Proposition \@ref(prp:hii)), this becomes $\infty$. This form of function expressed by $h_{ii}$ is called *potential function*

```{r levmeasure, echo=FALSE, fig.cap="Shape of potential function"}
tibble(x = c(0, .99)) %>% 
  ggplot(aes(x = x)) +
  stat_function(fun = function(x) {
    sqrt(x / (1 - x))
  }) +
  xlab(expression(h[ii])) +
  ylab(expression(sqrt(h[ii] / (1 - h[ii]))))
```

```{conjecture, dffthumb, name = "Rule of thumb"}
If $\lvert (DFFITS)_i \rvert > 2 \sqrt{\frac{p + 1}{n - p - 1}}$, then $i$-th observation is considered to be influential.
```

There is `dffits()` so that we can easilty get only $(DFFITS)_i$ for every $i$

```{r dffdat, fig.cap="DFFITS for fitted cement dataset"}
tibble(DFFITS = dffits(cem_fit)) %>% 
  mutate(i = 1:n()) %>% 
  ggplot(aes(x = i, y = DFFITS)) +
  geom_ref_line(h = 2 * sqrt( (4 + 1) / (nrow(cem) - 4 - 1) )) +
  geom_point() +
  geom_linerange(aes(ymin = DFFITS, ymax = 0), col = gg_hcl(1), linetype = "dotted")
```


### Cook's Distance

```{definition, cook, name = "Cook\\'s distance"}
$$C_i := \frac{\sum\limits_{j = 1}^n (\hat{Y}_j - \hat{Y}_{j, -i})^2}{\hat\sigma (p + 1)}$$

where $\hat{Y}_{j, -i} = \mathbf{x}_j^T \hb_{(-i)}$.
```

Cook's distnace is related to the distance between $\mathbf{\hat{Y}}$ and $\mathbf{\hat{Y}}_{(-i)}$.

```{corollary, cookrepresent}
Cooks's distance is related to the distance between $\hb$ and $\hb_{(-i)}$.

$$C_i = \frac{(\hb - \hb_{(-i)})^T X^T X (\hb - \hb_{(-i)})}{\hat\sigma (p + 1)}$$
```

```{proof}
It is just arithmetic if plug-in $\hat{Y}_{j} = \mathbf{x}_j^T \hb$ $\hat{Y}_{j, -i} = \mathbf{x}_j^T \hb_{(-i)}$.

\begin{equation*}
  \begin{split}
    C_i & = \frac{(\mathbf{Y} - \mathbf{Y}_{-i})^T(\mathbf{Y} - \mathbf{Y}_{-i})}{\hat\sigma^2 (p + 1)} \\
    & = \frac{(X\hb - X\hb_{(-i)})^T(X\hb - X\hb_{(-i)})}{\hat\sigma^2 (p + 1)} \\
    & = \frac{(\hb - \hb_{(-i)})^T X^TX (\hb - \hb_{(-i)})}{\hat\sigma^2 (p + 1)}
  \end{split}
\end{equation*}
```

```{remark}
$C_i$ can be represented by internally studentized residual and potential function

$$C_i = \bigg( \frac{e_i}{\hat\sigma \sqrt{1 - h_{ii}}} \bigg)^2 \times \frac{1}{p + 1} \times \frac{h_{ii}}{1 - h_{ii}}$$
```

```{proof}
From Theorem \@ref(thm:hbnoithm) ($\hb_{(-i)} = \hb - \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i e_i$) and Corollary \@ref(cor:cookrepresent),

\begin{equation*}
  \begin{split}
    C_i & = \frac{(\hb - \hb_{(-i)})^T X^TX (\hb - \hb_{(-i)})}{\hat\sigma^2 (p + 1)} \\
    & = \frac{\Big[ \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i e_i \Big]^T X^TX \Big[ \frac{1}{1 - h_{ii}} (X^TX)^{-1} \mathbf{x}_i e_i \Big]}{\hat\sigma^2 (p + 1)} \\
    & = \frac{\frac{e_i^2}{(1 - h_{ii})^2} \mathbf{x}_i (X^TX)^{-1} \mathbf{x}_i}{\hat\sigma^2 (p + 1)} \\
    & = \bigg( \frac{e_i}{\hat\sigma \sqrt{1 - h_{ii}}} \bigg)^2 \times \frac{1}{p + 1} \times \frac{h_{ii}}{1 - h_{ii}} \quad \because \mathbf{x}_i (X^TX)^{-1} \mathbf{x}_i = h_{ii} \\
    & = (\text{internally studentized residual})^2 \times \frac{1}{p + 1} \times \text{leverage measure}
  \end{split}
\end{equation*}
```

```{conjecture, cookthumb}
In practice, if $C_i > 1$, $i$-th observation is considered to be influential.
```

Corollary \@ref(cor:cookrepresent) is form of $F$-statistic. More specifically, $F(p, n - p)$. $1$ is actually come from $F_{0.5} (p, n - p)$. If the case is out of 50th percentile of $F$-distribution, then we will say that it is influential point.

In `R`, `cooks.distance()` gives $C_i$ alone.

```{r cidat, fig.cap="Cook\\'s Distance for fitted cement dataset"}
tibble(cook = cooks.distance(cem_fit)) %>% 
  mutate(i = 1:n()) %>% 
  ggplot(aes(x = i, y = cook)) +
  geom_ref_line(h = 1) +
  geom_point() +
  geom_linerange(aes(ymin = cook, ymax = 0), col = gg_hcl(1), linetype = "dotted")
```

In fact, we can get every influential statistic at once. `influence()` gives as `list` object.

```{r}
influence(cem_fit)
```

On the other hand, `influence.measures()` gives `matrix`. Its class is `infl` and we can extract `matrix` by `$infmat`.

```{r}
influence.measures(cem_fit)
```

In `broom` package, `broom::augment()` includes influene measures. To use `tibble`, we can use this function.

```{r}
broom::augment(cem_fit)
```


## Remedial Measures

In many times, given data set violates our assumptions. We have seen some of those scenarios. In that case, typical procedure will not be welcomed.

### Transformations

As quick remedies for violations, transformations of variables can be conducted.

First, See Figure \@ref(fig:heteropt). When **variance becomes larger** as $\hat{Y}_i$ increases, transform $Y_i$ into

$$\ln Y_i \quad \text{or} \quad \sqrt{Y_i}$$

i.e. the model becomes

$$
\begin{cases}
\ln Y_i = \beta_0 + \beta_1 x_{i1} + \cdots \beta_p x_{ip} + \epsilon_i \\
\sqrt{Y_i} = \beta_0 + \beta_1 x_{i1} + \cdots \beta_p x_{ip} + \epsilon_i
\end{cases}
$$

Since $\ln(.)$ and $\sqrt{.}$ is increasing with decreasing derivative, it might be able to suppress increasing variance which is depending on $\hat{Y}_i$.

```{r}
hetero_scale <- 
  hetero %>% 
  mutate(
    logy = log(y - min(y) + .1),
    sqy = sqrt(y - min(y) + .1)
  )
#--------------
hetero_log <- lm(logy ~ x, data = hetero_scale)
hetero_sqrt <- lm(sqy ~ x, data = hetero_scale)
```

```{r heterolog, fig.cap="Log transformation against increasing variance"}
residplot(hetero_scale, hetero_log)
```

```{r heterosqrt, fig.cap="Square root transformation against increasing variance"}
residplot(hetero_scale, hetero_sqrt)
```

See Figures \@ref(fig:heterolog) and \@ref(fig:heterosqrt). In this example, log-transformation was more affective. This might be because log function is smaller than square root function in large values.

Second, see Figure \@ref(fig:nonlinpt). Sometimes we observe a **certain pattern** in residual plot. Here, non-linear - cubic. Then transform $x_i$ into corresponding non-linear function of $x_i$.

```{r cubicregress, fig.cap="Cubic transformation against cubic pattern"}
cubic_poly <- lm(y ~ poly(x, 3), data = cubic) # cubic polynomial
residplot(cubic, cubic_poly)
```

Third, consider the case when Q-Q plot shows **non-normality**. Of course it depends on a situation, but

$$\ln Y_i$$

helpful sometimes.

### Variance stabilizing transformation

Suppose that the variance is **not constant** and it is the **function of mean**.

$$Y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \epsilon_{i}, \quad \epsilon_i \sim N(0, f^2(\mu_i))$$

where $\mu_i = E(Y_i)$ and $f$ is a smooth function for *standard deviation*. Before looking at how to deal with this heteroskedasticity, review a theorem from mathematical statistics.

```{theorem, delthm, name = "Delta Method of moments"}
Let $h(Y_i)$ be any change of variable.

$$E[h(Y_i)] \approx h(Y_i)$$

and

$$Var[h(Y_i)] \approx \Big(h^{\prime}(\mu_i)\Big)^2 Var(Y_i)$$
```

```{proof}
Taylor formula implies that

$$h(Y_i) \approx h(\mu_i) + h^{\prime}(\mu_i)(Y_i - \mu_i)$$

Then by taking mean and variance,

$$Eh(Y_i) \approx h(Y_i)$$

$$Var h(Y_i) \approx \Big( h^{\prime}(\mu_i) \Big)^2 Var(Y_i)$$
```

We try to find some *variance stabilizing transformation* $h(Y_i)$ such that

$$Var[h(Y_i)] = \sigma^2$$

From Theorem \@ref(thm:delthm),

\begin{equation}
  \sigma^2 \approx \Big( h^{\prime}(\mu_i) \Big)^2 f^2 (\mu_i)
  (\#eq:vstsig)
\end{equation}

It follows that

\begin{equation*}
  \begin{split}
    & h^{\prime}(\mu_i) \approx \frac{\sigma}{f(\mu_i)} \\
    & \Rightarrow h^{\prime}(\mu_i) \propto \frac{1}{f(\mu_i)} \\
    & \Rightarrow h(\mu_i) \propto \int \frac{1}{f(\mu_i)} d\mu_i
  \end{split}
\end{equation*}

```{theorem, vst, name = "Variance stabilizing transformation"}
$h$ can be calculated by

$$h(\mu_i) \propto \int \frac{1}{f(\mu_i)} d\mu_i$$
```

Theorem \@ref(thm:vst) shows how we can stabilize variance using the direct pattern of variance.

```{example, poiserr, name = "Poisson error term"}
$$Y_i \mid \mathbf{x}_i \indep Poisson(\lambda(\mathbf{x}_i))$$
```

```{r, include=FALSE}
vst_pois <- 
  tibble(
    x = seq_len(500),
    y = rpois(500, lambda = 1 + 2 * x),
  )
pois_fit <- lm(y ~ x, data = vst_pois)
```

```{r vstpois1, fig.cap="Residual plot after fitting Poisson error model"}
residplot(vst_pois, pois_fit)
```

```{solution}
Poisson error term results in non-constant variance. It is natural because the variance is the same as mean

$$Var(Y_i \mid \mathbf{x}_i) = E(Y_i \mid \mathbf{x}_i) = \lambda(\mathbf{x}_i)$$

Let $\lambda_i \equiv \lambda(\mathbf{x}_i)$. Applying VST,

$$h(\lambda_i) \propto \int \frac{1}{\sqrt{\lambda_i}} d\lambda_i \propto 2 \sqrt{\lambda_i}$$

Hence, we set VST by

$$h(Y_i) = \sqrt{Y_i}$$
```

```{r vstpois2, fig.cap="Poisson error term model residuals after VST"}
vst_pois <- 
  vst_pois %>% 
  mutate(ysq = sqrt(y))
vstpois_fit <- lm(ysq ~ x, data = vst_pois)
#--------------
residplot(vst_pois, vstpois_fit)
```

In fact, a *better way is to fit Poisson regression model, as a special case of generalized linera models*. This can explain Poisson random error with random component.

<!-- ```{example, binerr, name = "Binomial error term"} -->
<!-- Let -->

<!-- $$ -->
<!-- Y_i = \begin{cases} -->
<!-- 1 & \text{if pass with probability}\: p(x_i) \\ -->
<!-- 0 & \text{if fail with probability}\: 1 - p(x_i) -->
<!-- \end{cases} -->
<!-- $$ -->

<!-- Then -->

<!-- $$Y_i \mid x_i \indep Bernoulli(p(x_i))$$ -->
<!-- ``` -->

<!-- ```{solution} -->

<!-- ``` -->

<!-- Note that -->

<!-- $$E(Y_i \mid x_i) = p(x_i)$$ -->

<!-- and -->

<!-- $$Var(Y_i \mid x_i) = p(x_i)(1 - p(x_i))$$ -->

<!-- Then apply VST for $f = p_i(1 - p_i)$ -->

<!-- $$h(p_i) \propto \int \frac{1}{p_i(1 - p_i)} dp_i \propto$$ -->

In general, we do not know $f$. In this case, we just

1. trial and error
2. previous results
3. If replications (multiple observations of $Y_i$ at same $\mathbf{x}_i$ values) exist
    1. compute sd $S_i$ and average $\overline{Y}_i$
    2. simple linear regression $S_i = \alpha + \lambda \overline{Y}_i$
    3. $f(\mu_i) = \hat\alpha \mu_i^{\hat\lambda}$
4. If no replication, group

### Box-Cox Transformation

*Box-Cox transformation* is usually considered when Normality assumption is violated.

$$
Y^{\prime} = \begin{cases}
  \frac{Y^{\lambda} - 1}{\lambda} & \text{if}\: \lambda \neq 0 \\
  \ln Y & \text{if}\: \lambda = 0
\end{cases}
$$

How can we decide $\lambda$? $\lambda$ can be estimated by ML method. Note that $Y^{\prime}$ become gaussian. Assume that

$$Y^{\prime} \sim N(\mathbf{x}_i \B, \sigma^2)$$

Then

$$L(\lambda) = \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \bigg( - \frac{(Y_i^{\prime}(\lambda) - \mathbf{x}_i^T \B)^2}{\sigma^2} \bigg)$$


## Generalized and Weighted Least Squares

We can meet diverse values of $\sigma_i^2$. As advanced approaches, we can fit method of **generalized least squares** (GLS). One of special cases of GLS, **weighted least squares** is also widely used.

### Generalized least squares

When the errors do not have equal variance or they are not independent, we change our previous least squares procedure, so-called *ordinary least squres* (OLS) method. Our assumption for variance is $\sigma^2 I$, i.e. constant variance. Combined with normality, this also implies independence. Suppose that these two assumptions are violated. Let

$$Var(\E) = \sigma^2 V$$

where $V \neq I$ known. Assume that $V$ is *positive definite*. Consider the following transformation

\begin{equation}
  \underset{\LARGE \mathbf{Y}^{\prime}}{V^{- \frac{1}{2}} \mathbf{Y}} = \underset{\LARGE X^{\prime}}{V^{- \frac{1}{2}} X} \underset{\LARGE \B}{\B} + \underset{\LARGE \E^{\prime}}{V^{- \frac{1}{2}} \E}
  (\#eq:glsmod)
\end{equation}

Then

\begin{equation}
  \begin{split}
    Var(\E^{\prime}) & = Var(V^{- \frac{1}{2}} \E) \\
    & = V^{- \frac{1}{2}} Var(\E) V^{- \frac{1}{2}} \quad \because V \:\text{symmetric} \\
    & = \sigma^2 I
  \end{split}
  (\#eq:glssig)
\end{equation}

i.e. the errors become to have constant variance and independent. We now decide to find solution for our estimator. As in OLS,

$$\hb_G := \argmin_{\B \in \R^{p + 1}} (\mathbf{Y}^{\prime} - X^{\prime}\B)^T(\mathbf{Y}^{\prime} - X^{\prime}\B)$$

From Equation \@ref(eq:glsmod),

$$(\mathbf{Y}^{\prime} - X^{\prime}\B)^T(\mathbf{Y}^{\prime} - X^{\prime}\B) = (\mathbf{Y} - X\B)^T V^{-1} (\mathbf{Y} - X\B)$$

```{corollary, glsc, name = "GLS criterion"}
GLS estimator finds

$$\hb_G = \argmin_{\B \in \R^{p + 1}} (\mathbf{Y} - X\B)^T V^{-1} (\mathbf{Y} - X\B)$$
```

From Corollary \@ref(cor:glsc), GLS estimator is

\begin{equation}
  \hb_G = (X^T V^{-1} X)^{-1} X^T V^{-1} \mathbf{Y}
  (\#eq:glsest)
\end{equation}

```{proposition, glsprop, name = "Properties of $\\hb_G$"}
$\hb_G$ satisfies following properties.

\begin{enumerate}[label=\roman*]
  \item $E\hb_G = \B$ and $Var\hb_G = \sigma^2(X^T V^{-1} X)^{-1}$
  \item $X \hb_G$ is the projection of $\mathbf{Y}$ onto $Col(X)$ when we define a normed space $(\R^n, \lVert \cdot \rVert_{V^{-1}})$ with $\lVert \mathbf{u} \rVert_{V^{-1}}^2 := \mathbf{u}^T V^{-1} \mathbf{u}$
\end{enumerate}
```

```{proof}
(i)-1

\begin{equation*}
  \begin{split}
    E\hb_G & = E \Big[(X^T V^{-1} X)^{-1} X^T V^{-1} \mathbf{Y} \Big] \\
    & = (X^T V^{-1} X)^{-1} X^T V^{-1} E\mathbf{Y} \\
    & = (X^T V^{-1} X)^{-1} X^T V^{-1} X\B \\
    & = \B
  \end{split}
\end{equation*}

(i)-2

Note that

$$Var\mathbf{Y} = \sigma^2 V$$

Then

\begin{equation*}
  \begin{split}
    Var\hb_G & = Var \Big[(X^T V^{-1} X)^{-1} X^T V^{-1} \mathbf{Y} \Big] \\
    & = (X^T V^{-1} X)^{-1} X^T V^{-1} Var(\mathbf{Y}) V^{-1}X (X^T V^{-1} X)^{-1} \\
    & = \sigma^2 (X^T V^{-1} X)^{-1} (X^T V^{-1} X) (X^T V^{-1} X)^{-1} \\
    & = \sigma^2 (X^T V^{-1} X)^{-1}
  \end{split}
\end{equation*}
```

Remember the definition of normed space.

```{definition, normed, name = "Normed space"}
A \textit{normed space} is a vector space $X$ with a \textbf{\textit{norm}} $\lVert \cdot \rVert: X \rightarrow \R$ satisfying for all $x, y \in X$ and $\alpha \in K$

\begin{enumerate}[label={(N\arabic*)}]
  \item \label{item:n1} $\lVert x \rVert \ge 0$
  \item \label{item:n2} $\lVert x \rVert = 0 \Leftrightarrow x = 0$
  \item \label{item:n3} $\lVert \alpha x \rVert = \lvert \alpha \rvert \lVert x \rVert$
  \item \label{item:n4} $\lVert x + y \rVert \le \lVert x \rVert + \lVert y \rVert$
\end{enumerate}
```

```{remark}
Consider Definition \@ref(def:normed).

\begin{enumerate}
  \item $\ref{item:n3}$ and $\ref{item:n4}$ imply $\ref{item:n1}$. Hence, it is enough for any $X$ to satisfy $\ref{item:n2}$, $\ref{item:n3}$ and $\ref{item:n4}$ to show that it is normed space.
  \item $x = 0$ implies $\lVert x \rVert = 0$, i.e. proving \textit{only if} part is enough for $\ref{item:n2}$.
\end{enumerate}
```

```{proof}
1

Let $x \in X$. Then

\begin{equation*}
  \begin{split}
    0 = \lVert 0 \Vert & = \lVert x + (-x) \Vert \\
    & \le \lVert x \Vert + \lVert -x \Vert \quad \because \ref{item:n4} \\
    & = 2 \lVert x \Vert \quad \because \ref{item:n3}
  \end{split}
\end{equation*}

Thus,

$$\lVert x \Vert \ge 0$$

2

$$\lVert 0 \rVert = \lVert 0 x \rVert \stackrel{\ref{item:n3}}{=} \lvert 0 \rvert \lVert x \rVert = 0$$
```

Denote that in our regression setting, $K = \R$. Now continue the proof of Proposition \@ref(prp:glsprop).

```{proof}
(ii) normed space

Let $\mathbf{u} \in \R^n$ and let $\lVert \mathbf{u} \rVert_{V^{-1}}^2 := \mathbf{u}^T V^{-1} \mathbf{u}$.

Want 1: $\lVert \mathbf{u} \rVert = 0 \Rightarrow \mathbf{u} = \mathbf{0}$

Suppose that $\lVert \mathbf{u} \rVert = 0$. Then by definition,

$$\mathbf{u}^T V^{-1} \mathbf{u} = 0$$

Since $V$ is positive defintie, $V^{-1}$ is also positive definite. Thus,

$$\mathbf{u} = 0$$

and $\ref{item:n1}$ holds.

Want 2: $\ref{item:n3}$

Let $c \in \R$. Then

\begin{equation*}
  \begin{split}
    \lVert c \mathbf{u} \rVert_{V^{-1}}^2 & = (c \mathbf{u})^T V^{-1} (c\mathbf{u}) \\
    & = c^2 \mathbf{u}^T V^{-1} \mathbf{u}
  \end{split}
\end{equation*}

Thus,

$$\lVert c \mathbf{u} \rVert_{V^{-1}} = \lvert c \rvert \lVert \mathbf{u} \rVert_{V^{-1}}$$

Want 3: $\ref{item:n4}$

Let $\mathbf{u}, \mathbf{v} \in \R^n$. Then we have

\begin{equation*}
  \begin{split}
    \lVert \mathbf{u} + \mathbf{v} \rVert_{V^{-1}}^2 & = (\mathbf{u} + \mathbf{v})^T V^{-1} (\mathbf{u} + \mathbf{v}) \\
    & = \mathbf{u}^TV^{-1}\mathbf{u} + \mathbf{v}^TV^{-1}\mathbf{v} + \mathbf{u}^TV^{-1}\mathbf{v} + \mathbf{v}^TV^{-1}\mathbf{u} \\
    & = \mathbf{u}^TV^{-1}\mathbf{u} + \mathbf{v}^TV^{-1}\mathbf{v} + (V^{-\frac{1}{2}}\mathbf{u})^TV^{-\frac{1}{2}}\mathbf{v} + (V^{-\frac{1}{2}}\mathbf{v})^TV^{-\frac{1}{2}}\mathbf{u} \\
    & \le \lVert \mathbf{u} \rVert_{V^{-1}} + \lVert \mathbf{v} \rVert_{V^{-1}} + 2 \lVert V^{-\frac{1}{2}}\mathbf{u} \rVert \cdot \lVert V^{-\frac{1}{2}}\mathbf{v} \rVert \quad \leftarrow \text{Cauchy-Schwarz inequality in}\: (\R^n, < \cdot, \cdot >) \\
    & = \lVert \mathbf{u} \rVert_{V^{-1}} + \lVert \mathbf{v} \rVert_{V^{-1}} + 2 \bigg( \mathbf{u}^TV^{-1}\mathbf{u} \cdot \mathbf{v}^TV^{-1}\mathbf{v} \bigg) \\
    & = \lVert \mathbf{u} \rVert_{V^{-1}} + \lVert \mathbf{v} \rVert_{V^{-1}} + 2 \lVert \mathbf{u} \rVert_{V^{-1}}  \lVert \mathbf{v} \rVert_{V^{-1}} \\
    & = (\lVert \mathbf{u} \rVert_{V^{-1}} + \lVert \mathbf{v} \rVert_{V^{-1}})^2
  \end{split}
\end{equation*}

This completes the proof.
```





Refer to the GLS model \@ref(eq:glsmod). In fact, this is a transformed model, so $\B$ here is the different coefficient with the original model. By Equation \@ref(eq:glsest) and Proposition \@ref(prp:glsprop), $\hb_G$ is a linear unbiased estimator for $\B$. Then is this the BLUE, i.e. is

$$Var\hb_G = \sigma^2(X^T V^{-1} X)^{-1}$$

the smallest variance among of the linear unbiased estimators?

```{theorem, glsblue}
GLS estimator $\hb_G$ is the best linear unbiased estimator of $\B$, i.e. \textbf{\textit{BLUE}}.
```

```{proof}
One proceeds in a similar way to the proof of Theorem \@ref(thm:multgm).

Let $\tilde{\B} \in \Omega \equiv \{ \tilde{\B} : \tilde{\B} = C \mathbf{Y}, E\tilde{\B} = \B \}$.

Claim: $Var\tilde{\B} - Var\hb_G$ is non-negative definite.

Set $D := C - (X^T V^{-1} X)^{-1} X^T V^{- \frac{1}{2}}$. From unbiasedness,

\begin{equation*}
  \begin{split}
    E\tilde{\B} & = C E\mathbf{Y} \\
    & = C V^{ -\frac{1}{2}} X \B \\
    & = \Big( (X^T V^{-1} X)^{-1} X^T V^{- \frac{1}{2}} + D \Big) V^{- \frac{1}{2}} X \B \\
    & = \B + D V^{- \frac{1}{2}} X \B \\
    & = \B
  \end{split}
\end{equation*}

It implies that

$$D V^{- \frac{1}{2}} X = 0$$

\begin{equation*}
  \begin{split}
    Var\tilde{\B} & = Var(C\mathbf{Y}) \\
    & = \sigma^2 CC^T \\
    & = \sigma^2 \Big( (X^T V^{-1} X)^{-1} X^T V^{- \frac{1}{2}} + D \Big) \Big( (X^T V^{-1} X)^{-1} X^T V^{- \frac{1}{2}} + D \Big)^T \\
    & = \sigma^2 \Big( (X^T V^{-1} X)^{-1} + DV^{- \frac{1}{2}}X(X^T V^{-1} X)^{-1} + (X^T V^{-1} X)^{-1} X^T V^{- \frac{1}{2}} D^T + DD^T \Big) \\
    & = \sigma^2 \Big( (X^T V^{-1} X)^{-1} + DD^T \Big) \\
    & = Var\hb_G + \sigma^2 DD^T
  \end{split}
\end{equation*}

Note that $\sigma^2 DD^T$ is a non-negative definite matrix. Hence,

$$Var\tilde{\B} - Var\hb_G = \sigma^2 DD^T$$

is non-negative. This completes the proof.
```

In practice, however, we do not know what $V$ is.


### Weighted least squares

As mentioned, WLS is a special case of GLS. Set $W := V^{-1}$ in GLS by

$$W = diag\Big(\frac{1}{w_1}, \frac{1}{w_2}, \ldots, \frac{1}{w_n}\Big)$$

with $w_i > 0$. This method is appropriate to the *uncorrelated error term of unequal variances with*

$$Var(\epsilon_i) = \frac{\sigma^2}{w_i}$$

As in GLS, weighted least squares deals with the model of the form

\begin{equation}
  W^{\frac{1}{2}} \mathbf{Y} = W^{\frac{1}{2}} X \B + W^{\frac{1}{2}} \E
  (\#eq:wlsmod)
\end{equation}

From Corollary \@ref(cor:glsc), it minimizes

$$(\mathbf{Y} - X\B)^T W (\mathbf{Y} - X\B)$$

This is equivalent to the next corollary.

```{corollary, wlsc, name = "WLS criterion"}
WLS minimizes the weighted sum of squred errors

$$\sum_{i = 1}^n w_i (Y_i - \beta_0 - \beta_1 x_{i1} - \cdots - \beta_p x_{ip})^2$$
```

Corollary \@ref(cor:wlsc) means that each data point is *weighted by inversely proportional to the variance of the corresponding response*. The estimator would be computed as

\begin{equation}
  \hb^{WLS} = (X^T W X) X^T W \mathbf{Y}
  (\#eq:wlsest)
\end{equation}

Since we do not know each $\sigma_i$ in typical analysis, we should estimate each $w_i$.

This weights of WLS is specified in `weights` argument of `lm()`.




