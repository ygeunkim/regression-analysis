# Variable Selection {#sel}

## Motivation of Variable Selection

Large number of variables causes problem. Extremely, consider

$$n < p$$

Then we have

$$rank(X) \le n$$

In this case, $(X^T X)^{-1}$ does not exist and OLS becomes to have no unique solution. This situation gives us

$$Var \hb \rightarrow \infty$$

### Full model

Subsetting variables among moderate number of variables gives similar result. Assume that true regression model of no intercept with two covariates $X_1$ and $X_2$ for the data $\{ (x_{i1}, x_{i2}, Y_i) : i = 1, \ldots, n \}$

\begin{equation}
  Y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i
  (\#eq:truefull)
\end{equation}

For simplicity, let the model satisfy that

$$\sum_i x_{i1} = \sum_i x_{i2} = 0 \quad \text{and} \quad \sum_i x_{i1}^2 = \sum_i x_{i2}^2 = 1$$

i.e. *inputs are centered* and $s_{11} = s_{22} = 1$. In the full model \@ref(eq:truefull), LSE $\hb_{F} = (\hat\beta_{F,1}, \hat\beta_{F, 2})^T$ is

$$\hb_{F} = (X^T X)^{-1} X^T \mathbf{Y}$$

with design matrix $X$ of the model. Denote that by the condition,

$$X \perp \mathbf{1}$$

Equation \@ref(eq:sjjr) implies that for each $j = 1, 2$

$$\mathbf{x}_1^T \mathbf{x}_1 = 1 - R_j^2$$

and that

\begin{equation}
  \begin{split}
    Var(\hat\beta_{F,j}) & = \frac{\sigma^2}{1 - R_j^2} \\
    & = \frac{\sigma^2}{1 - r_{12}^2}
  \end{split}
  (\#eq:fullbetavar)
\end{equation}

with $r_{12}$ is sample correlation coefficient. Furthermore, each coefficient estimate is unbiased.

$$E(\hb_{F}) = (X^T X)^{-1} X^T X \B_{F} = \B_{F}$$

### Subset model

Now consider the subset model using only $x_{i1}$.

\begin{equation}
  Y_i = \beta_1 x_{i1} + \epsilon_i
  (\#eq:truefull2)
\end{equation}

By solving normal equation, OLS gives estimate for $\beta_1$

$$\hat\beta_{S, 1} = \sum_{i = 1}^n x_{i1} Y_i$$

Recall that our true model is Full model \@ref(eq:truefull). Then

\begin{equation}
  \begin{split}
    E(\hat\beta_{S, 1}) & = \sum_i x_{i1} E(Y_i) \\
    & = \sum_i x_{i1} (\beta_1 x_{i1} + \beta_2 x_{i2}) \\
    & = \beta_1 + r_{12} \beta_2 \quad \because \text{assumption for inputs} \\
    & \neq \beta_1
  \end{split}
  (\#eq:subsetbiased)
\end{equation}

i.e. $\hat\beta_{S,1}$ is *biased*. Additionally,

\begin{equation}
  Var(\hat\beta_{S,1}) = \sum_i x_{i1}^2 \sigma^2 = \sigma^2
  (\#eq:subsetvariance)
\end{equation}

### Comparison

Now compare two different estimators of $\beta_1$ from the full and the subset models. Compute *mean squared error* (MSE). The following lemma about variance and bias can be used here.

```{lemma, bvtd, name = "Bias-variance trade-off"}
For any estimator $\hat\beta$ for $\beta$,

$$
\begin{aligned}
  MSE(\hat\beta) & = \Big( E\hat\beta - \beta \Big)^2 + E(\hat\beta - E\hat\beta)^2 \\
  & = bias(\hat\beta)^2 + Var(\hat\beta)
\end{aligned}
$$
```

```{proof}
Plus and minus $E\hat\beta$.

\begin{equation}
  \begin{split}
    MSE (\hat\beta) & = E (\hat\beta - \beta)^2 \\
    & = E (\hat\beta - E\hat\beta + E\hat\beta - \beta )^2 \\
    & = E \Big[ \underset{\text{r.v.}}{\underline{( \hat\beta - E\hat\beta )^2}} + (E\hat\beta - \beta )^2 + 2 \underset{\text{r.v.}}{\underline{( \hat\beta - E\hat\beta )}} (E\hat\beta - \beta ) \Big] \\
    & = E(\hat\beta - E\hat\beta)^2 + (E\hat\beta - \beta)^2 + 2 (E\hat\beta - \beta ) \underset{= 0}{\underline{(E\hat\beta - E\hat\beta)}} \\
    & = Var(\hat\beta) + bias(\hat\beta)^2
  \end{split}
\end{equation}
```

We now apply this lemma to each model. Note that $\hat\beta_{F,1}$ of the full model \@ref(eq:truefull) is unbiased, i.e.

$$E\hat\beta_{F,1} = \beta_1$$

Then from Equation \@ref(eq:fullbetavar),

\begin{equation}
  \begin{split}
    MSE(\hat\beta_{F,1}) & = Var(\hat\beta_{F,1}) \\
    & = \frac{\sigma^2}{1 - r_{12}^2}
  \end{split}
  (\#eq:truefullmse)
\end{equation}

On the other hand, $\hat\beta_{S, 1}$ of the subset model \@ref(eq:truefull2) is biased. Then from Equations \@ref(eq:subsetbiased) and \@ref(eq:subsetvariance),

\begin{equation}
  \begin{split}
    MSE(\hat\beta_{S,1}) & = Var(\hat\beta_{S,1}) + \Big( E\hat\beta_{S,1} - \beta_1 \Big)^2 \\
    & = \sigma^2 + (r_{12} \beta_2)^2
  \end{split}
  (\#eq:truefullmse2)
\end{equation}

The less $MSE$ is, the better. If

\begin{equation}
  1 + \frac{\lvert \beta_2 \rvert}{\sigma} < \frac{1}{\sqrt{1 - r_{12}^2}}
  (\#eq:fullmsecomp)
\end{equation}

then the subset model will estimate more precisely. Note that

$$\frac{1}{\sqrt{1 - r_{12}^2}} > 1$$

```{remark}
We can divide in two case for $\beta_2$.

\begin{enumerate}
  \item If $\lvert \beta_2 \rvert = 0$, then the subset model is understood as the true model so that it is preferred.
  \item Even if $\beta_2 \neq 0$, $r_{12}^2 \approx 1$ (multicollinearity) results in the superiority of the subset model over the full model.
\end{enumerate}
```

Sometimes we try to remove some variables. If the $\beta_j$s and $\sigma^2$ were known, deletion of variables with small $\lvert \beta_j \rvert$ compared to $\sigma$, i.e. $\frac{\lvert \beta_j \rvert}{\sigma}$ would be desirable.


## Criteria for Selecting Subsets {#modsel}

From now on, we subset variables using various criteria. This requires procedures deciding if subset is better than another, and some criteria are implemented. This is often called **model selection**. These criteria helps to choose the optimal subset size.

A general approach is to implement assumed Gaussian likelihood [@Hastie:2013aa].

$$\text{likelihood} + \text{penalty for the number of parameters}$$

The following correspond to likelihood term.

```{remark}
Likelihood means training error based on normal distribution error.

\begin{itemize}
  \item Coefficient of determination $R^2 := \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$
  \item Residual sum of squares $SSE := \sum (y_i - \hat{y}_i)^2$
  \item Deviance $-2 l(\hb, \hat\sigma^2)$
\end{itemize}
```

As the number of parameters grows, each error

- $1 - R^2$
- $SSE$
- $-2 l(\hb, \hat\sigma^2)$

becomes smaller, i.e. we can regard the model better than before. However, previous section has shown that this is not the case, so we add or multiply a penalty for the number.

The following is the example dataset from @Chatterjee:2015aa. The reference had studied relation between total mortality (`MORT`) and climate, socioeconomic, and pollution variables (other 15 predictors). Response is total age-adjusted mortality from all causes. There are quite many predictors.

```{r, message=FALSE}
(death <- bestglm::mcdonald %>% as_tibble())
```

For each predictor, see the below description. 60 observations indicate 60 SMSAs.

| predictors | description |  
|:----------:|:-----------:|  
| `PREC` | mean annual precipitation, in inches |  
| `JANT` | mean January temperature, degrees Farenheit |  
| `JULT` | mean July temperature, degrees Fareheit |  
| `OVR65` | percent of population aged 65 or older than 65 |  
| `POPN` | population per household |  
| `EDUC` | median school years completed |  
| `HOUS` | percent of housing units that are sound |  
| `DENS` | population per square mile |  
| `NONW` | percent of non-white population |  
| `WWDRK` | percent employment of white-collor job |  
| `POOR` | percent of families with income less than \$3000 |  
| `HC` | relative pollution potential of hydrocarbon |  
| `NOX` | of nitric oxides |  
| `SOx` | of sulphur dioxide |  
| `HUMID` | percent relative humidity |  

```{r}
(death_fit <- lm(MORT ~ ., data = death)) %>% 
  summary()
```

As we can see, some standard errors are very large.

```{r}
car::vif(death_fit)
```

We can observe multicollinearity by $VIF_j > 1$.

### Adjusted $R^2$

First consider $R^2 = \frac{SSR}{SST}$. Denote that this is *nondecreasing as a new predictor enters the model*. It it just matter of how it increases.

We want to select a subset $S$ of the index set $\{ 1, \ldots p \}$. Write $\lvert S \rvert$ as the cardinality of the set and set $q$ by the number of regression coeffcients.

$$q := \lvert S \rvert + 1$$

Let $SSE(S)$ and $R^2 (S)$ be residual sum of squares and coefficient of determination corresponding to the model $Y$ regressed on $\{ x_j : j \in S \}$ including intercept term. Recall that

$$R^2(S) = \frac{SSR(S)}{SST} = 1 - \frac{SSE(S)}{SST}$$

Just compute adjusted $R^2$ for this $S$ using Definition \@ref(def:ra).

```{definition, adjr2, name = "Adjusted $R^2$"}
Let $S$ be a subset of variables and let $R^2$ be corresponding $R^2$. Then the adjusted $R^2$ for $S$ is

$$R_a^2(S) := 1 - \bigg( \frac{n - 1}{n - q} \bigg) (1 - R^2(S)) = 1 - \frac{SSE / (n - q)}{SST / (n - 1)}$$
```

Different with $R^2(S)$, $R_a^2(S)$ increases and decreases at some point as $\lvert S \rvert$ increases. The point that maximizes $R_a^2(S)$ can be said to be an optimal subset.

```{conjecture, raopt, name = "Optimal number of variables w.r.t. $R_a^2(S)$"}
Choose $S$ that maximizes $R_a^2(S)$.
```

### Residual mean square

Residual mean square $MSE(S)$ is related to $R_a^2(S)$.

```{definition, mses, name = "Residual mean square"}
Let $S$ be a subset of variables and let $q = \lvert S \rvert + 1$ be the degrees of freedom of $SSE(S)$. Then the residual mean square according to $S$ is

$$MSE(S) := \frac{SSE(S)}{n - q}$$
```

By construction, $R_a^2(S)$ can be re-expressed by $MSE(S)$.

```{remark}
From Definition \@ref(def:mses),

$$R_a^2(S) = 1 - \frac{MSE(S)}{SST / (n - 1)}$$
```

Since $\frac{SST}{n - 1}$ is constant, maximimizing $R_a^2(S)$ as in Conjecture \@ref(cnj:raopt) is equivalent to minimizing $MSE(S)$.

```{conjecture, mseopt, name = "Optimal number of variables w.r.t. $MSE(S)$"}
Choose $S$ that minimizes $MSE(S)$.
```

### Mallow's $C_p$

Previously, we multiplied penalty. From now on, we add it. Consider $SSE(S)$.

```{definition, mcp, name = "Mallow's $C_p$"}
Let $F = \{ 1, \ldots, p \}$ be a full set of variables, let $S$ be a subset of variables, and let $q = \lvert S \rvert + 1$. Then the Mallow's $C_p$ for $S$ is

$$C_p(S) := \frac{SSE(S)}{\hat\sigma^2} + (2q - n)$$

where $\hat\sigma^2 = MSE(F) = \frac{SSE(F)}{n - p - 1}$, i.e. $MSE$ obtained from the full model.
```

$\frac{SSE(S)}{\hat\sigma^2}$ measures $SSE(S)$ compared to $MSE$ of full model. When the model becomes closed to the full model, $SSE(S)$ gets smaller and goes to $SSE(F)$. In the full model,

$$\frac{SSE(F)}{\hat\sigma^2} = n - p - 1$$

Since $q = p + 1$ in this model, we have

$$C_p(F) = n - p - 1 + 2 (p + 1) - n = p + 1$$

i.e. same value as $q$. In fact, this occurs when the model is unbiased.

```{remark}
Suppose that the fitted values of subset model are unbiased. Then

$$E(C_p(S)) = q$$
```

```{proof}
From Lemma \@ref(lem:bvtd), mean squared error can be decomposed into bias and variance.

$$\sum_i E(\hat{Y}_{S, i} - \mu_{S, i}) = \sum_i \Big( E \hat{Y}_{S, i} - \mu_{S, i} \Big)^2 + \sum_i E(\hat{Y}_{S, i} - E \hat{Y}_{S, i})^2$$

Note that the left hand side is constant. If there is no bias in the model, we have

$$MSE(F) = MSE(S)$$

Thus,

$$C_p = \frac{(n - q) MSE(S)}{MSE(F)} + 2q - n = q$$
```

<!-- From Proposition \@ref(prp:ssdist), -->

<!-- $$\frac{SSE(S)}{\sigma^2} \sim \chi^2(n - q)$$ -->

<!-- for each $q = 2, \ldots, p + 1$. Then -->

<!-- $$\frac{SSE(S)}{\hat\sigma^2} / (n - q) = \frac{\frac{SSE(S)}{\sigma^2} / (n - q)}{\frac{SSE(F)}{\sigma^2} / (n - p - 1)} \sim F(n - q, n - p - 1)$$ -->

<!-- It follows that -->

<!-- \begin{equation} -->
<!--   \begin{split} -->
<!--     E(C_p(S)) & = E\bigg[ \frac{SSE(S)}{\hat\sigma^2} \bigg] + (2q - n) \\ -->
<!--     & = (n - q) \frac{n - p - 1}{n - p - 3} + 2q - n \\ -->
<!--     & = \frac{n(n - p - 1) - q(n - p - 1) + 2q(n - p - 3) - n(n - p - 3)}{n - p - 3} \\ -->
<!--     & = \frac{2n + q(n - p - 5)}{n - p - 3} -->
<!--   \end{split} -->
<!-- \end{equation} -->

This remark implies that the good model has small value of

$$C_p(S) \approx q$$

We need to find this model.

```{conjecture, cpopt, name = "Optimal number of variables w.r.t. $C_p$"}
Choose $S$ with the smallest $q = \lvert S \rvert + 1$ that has $C_p(S) \approx q$.
```

### Akaike Information criterion

Information criteria implements deviance $-2 \text{log-likelihood}$. There are two popular criteria called **Akaike information criterion (AIC)** and **Bayesian information criterion (BIC)**. Each adds penalty of form

$$-2l(\hb, \hat\sigma^2) + \text{constant} \times q$$

AIC gives $2$ as a penalty factor.

```{definition, aics, name = "Akaike information criterion"}
Let $S$ be a subset of variables and let $q = \lvert S \rvert + 1$ be the number of parameters. Then AIC for S is

$$AIC(S) := -\frac{2}{n} l(\hb_{S}, \hat\sigma_S^2) + \frac{2}{n} q$$
```

This definition is actually quite general formulation. In case of OLS, Gaussian error term is given. Since the form of $l(\hb_{S}, \hat\sigma^2)$ is known, we can compute it.

```{corollary, aicols, name = "AIC for OLS with Gaussian error"}
Suppose that $\epsilon_i \iid N(0, \sigma^2)$. Then

$$AIC(S) = \ln SSE(S) + \frac{2 \lvert S \rvert}{n}$$
```

```{proof}
Let

$$\hat\sigma_S^2 = \frac{1}{n} \lVert \mathbf{Y} - X \hb_{S} \rVert^2$$

be the MLE of $\sigma^2$. Then we now have

\begin{equation*}
  \begin{split}
    l(\hb_{S}, \hat\sigma_S^2) & = - \frac{n}{2} \ln (2 \pi \hat\sigma_S^2) - \frac{1}{2 \hat\sigma_S^2} \lVert \mathbf{Y} - X \hb_{S} \rVert^2 \\
    & = - \frac{n}{2} \ln(2 \pi) + \frac{n}{2} \ln n - \frac{n}{2} \ln SSE(S) - \frac{n}{2}
  \end{split}
\end{equation*}

Thus,

\begin{equation*}
  \begin{split}
    AIC(S) & = \ln SSE(S) + \ln(2\pi) - \ln n + 1 + \frac{2 \lvert S \rvert}{n} + \frac{2}{n} \\
    & \propto \ln SSE(S) + \frac{2 \lvert S \rvert}{n} \quad \leftarrow \text{ignore constants}
  \end{split}
\end{equation*}

and so $AIC(S)$ is equivalent to

$$\ln SSE(S) + \frac{2 \lvert S \rvert}{n}$$
```

$SSE$ is involved in OLS, so smaller $AIC$ is preferred.

```{conjecture, aicopt, name = "Optimal number of variables w.r.t. AIC"}
Choose $S$ that minimizes $AIC(S)$.
```

### Bayesian information criterion

Instead of $2$, Bayesian information criterion (BIC) uses $\ln n$.

```{definition, bics, name = "Bayesian information criterion"}
Let $S$ be a subset of variables and let $q = \lvert S \rvert + 1$ be the number of parameters. Then BIC for S is

$$BIC(S) := -\frac{2}{n} l(\hb_{S}, \hat\sigma_S^2) + \frac{\ln n}{n} q$$
```

One proceeds in a similar way for OLS likelihood.

```{corollary, bicols, name = "BIC for OLS with Gaussian error"}
Suppose that $\epsilon_i \iid N(0, \sigma^2)$. Then

$$BIC(S) = \ln SSE(S) + \frac{\lvert S \rvert \ln n}{n}$$
```

Of course it is the same form, so we might choose the small one.

```{conjecture, bicopt, name = "Optimal number of variables w.r.t. BIC"}
Choose $S$ that minimizes $BIC(S)$.
```

```{r aicbic, echo=FALSE, fig.cap="Penalties of AIC and BIC"}
tibble(x = seq_len(100)) %>% 
  mutate_at(.vars = vars(x), .funs = list(aic = function(x) 2, bic = log)) %>% 
  gather(-x, key = "penalty", value = "value") %>% 
  ggplot(aes(x = x, y = value, colour = penalty)) +
  geom_path() +
  labs(
    x = "n",
    y = "Factor value",
    colour = "Factor"
  )
```

Figure \@ref(fig:aicbic) presents the difference between AIC and BIC, $2$ and $\ln n$. In the most of the domain,

$$\ln n > 2$$

In this case, BIC penalizes larger models more heavily than AIC. In turn, it prefers *smaller models in comparison with AIC*.


## Computational Techniques

Using only criteria in the previous section, it is hard to find the best subset of the variables. We should know how to effectively use it, i.e. algorithm.

### All possible regressions

The most basic way is investigating all possible models. Each submodel is

\begin{equation}
  Y_i = \beta_0 + \sum_{j \in S} \beta_j x_{ij} + \epsilon_i, \quad S \subseteq \{ 1, \ldots p \}
  (\#eq:bestsub)
\end{equation}

For example, if $p = 3$,

| $\lvert S \rvert$ | $S$ | submodels |  
|:-----------------:|:---:|:---------:|  
| 0 | $\varnothing$ | $Y = \beta_0 + \epsilon$ |  
| 1 | $\{ 1 \}$ | $Y = X_1 + \epsilon$ |  
| 1 | $\{ 2 \}$ | $Y = X_2 + \epsilon$ |  
| 1 | $\{ 3 \}$ | $Y = X_3 + \epsilon$ |  
| 2 | $\{ 1, 2 \}$ | $Y = X_1 + X_2 + \epsilon$ |  
| 2 | $\{ 1, 3 \}$ | $Y = X_1 + X_3 + \epsilon$ |  
| 2 | $\{ 2, 3 \}$ | $Y = X_2 + X_3 + \epsilon$ |  
| 3 | $\{ 1, 2, 3 \}$ | $Y = X_1 + X_2 + X_3 + \epsilon$ |  

For each submodel, compute one of $R_a^2$, $C_p$, $AIC$, and $BIC$. Submodel with the best value of criterion would be chosen. This requires fitting

$$\binom{p}{0} + \binom{p}{1} + \cdots \binom{p}{p} = 2^p$$

@James:2013aa summarizes the procedure as follows.

```{r, include=FALSE}
alg_num <- alg_num + 1
bestsubset <- alg_num
bestsubsetstr <- "
| **Data**: $Y_i$ and every predictor $x_{i1}, \\ldots x_{ip}$
| Initialize null model $\\mathcal{M}_0$ by $Y_i = \\beta_0 + \\epsilon_i$;
| **For** $k \\leftarrow 1$ **to** $p$:
|   Fit all $\\binom{p}{k}$ models with $k$ predictors $Y_i = \\beta_0 + \\sum_{j \\in S} \\beta_j x_{ij} + \\epsilon_i$ with $\\lvert S \\rvert = k$;
|   Denote $\\mathcal{M}_k$ a model with the smallest $SSE$;
| **end**
| Select a single best among $\\mathcal{M}_0, \\mathcal{M}_1, \\ldots, \\mathcal{M}_p$ using $R_a^2$, $C_p$, $AIC$, or $BIC$;
| **Output**: $\\mathcal{M}_{k^{\\prime}}$ with the best criterion value
|"
bestsubsetstr <- paste0("Algorithm", bestsubset, ": All possible regressions")
```

\begin{algorithm}[H] \label{alg:bestsubset}
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \KwData{$Y_i$ and every predictor $x_{i1}, \ldots x_{ip}$}
  Initialize null model $\mathcal{M}_0$ by $$Y_i = \beta_0 + \epsilon_i$$\;
  \For{$k \leftarrow 1$ \KwTo $p$}{
    Fit all $\binom{p}{k}$ models with $k$ predictors $$Y_i = \beta_0 + \sum_{j \in S} \beta_j x_{ij} + \epsilon_i$$ with $\lvert S \rvert = k$\;
    Denote $\mathcal{M}_k$ a model with the smallest $SSE$\;
  }
  Select a single best among $\mathcal{M}_0, \mathcal{M}_1, \ldots, \mathcal{M}_p$ using $R_a^2$, $C_p$, $AIC$, or $BIC$\;
  \Output{$\mathcal{M}_{k^{\prime}}$ with the best criterion value}
  \caption{All possible regressions}
\end{algorithm}

`r if(is_html) bestsubsetstr`

However, as the full number of variables $p$ increases, the number of submodels to be conducted $2^p$ increases rapidly. It is computational burden. Against this kind of efficiency problem, some simpler methods are developed.

### Forward selection

Beginning with the null model, forward selection *adds predictors one-at-a-time*. We might add the most important predictor at each step by a settled condition.

```{r fstab, echo=FALSE, results='asis'}
tribble(
  ~x1, ~x2, ~x3, ~best,
  #--/----/----
  0, 0, 0, FALSE,
  1, 0, 0, FALSE,
  0, 1, 0, TRUE,
  0, 0, 1, FALSE,
  2, 1, 0, FALSE,
  0, 1, 2, TRUE,
  3, 1, 1, TRUE
) %>% 
  mutate_if(
    is.numeric,
    function(x) {
      kableExtra::cell_spec(
        x,
        format = "latex",
        color = ifelse(.$best, "red", "black")
      )
    }
  ) %>% 
  select(-best) %>%
  knitr::kable(
    format = "latex", 
    escape = FALSE, 
    align = "c",
    longtable = TRUE,
    caption = "Illustration of Forward selection"
  ) %>% 
  kableExtra::kable_styling(c("striped", "condensed"), full_width = FALSE) %>% 
  kableExtra::pack_rows("p=1", start_row = 2, end_row = 4) %>% 
  kableExtra::pack_rows("p=2", start_row = 5, end_row = 6) %>% 
  kableExtra::pack_rows("p=3", start_row = 7, end_row = 7)
```

See Table \@ref(tab:fstab). In each step, i.e. $p = k$, the most relavant variable is added to the model. It is remained to the end. Next, we find the most relavant variables given the previously chosen variables already in the model.

Here we need to decide some measures.

- that finds the most relavant variables among the remains
- that determines stopping rules of the algorithm

The algorithm starts with null model, which has no predictors. Set

$$S_0 = \varnothing \quad \text{and} \quad C_0 = Col(\mathbf{1}) = \{ \beta_0 \mathbf{1} : \beta_0 \in \R \}$$

Next for each step $k = 1, 2, \ldots$, let $C_{k - 1}$ be the design space in the previous step. We now add to the given model the predictor that has *the highest size of sample partial correlation* with the response. Recall that from Theorem \@ref(thm:corrangle), sample correlation between $\mathbf{x}_j$ and $\mathbf{Y}$ is same as cosine of the angle between the two centered vectors, i.e.

$$r_j = \frac{(\mathbf{x}_j - \overline{x}_j \mathbf{1})^T (\mathbf{Y} - \overline{Y} \mathbf{1})}{\lVert \mathbf{x}_j - \overline{x}_j \mathbf{1} \rVert \lVert \mathbf{Y} - \overline{Y} \mathbf{1} \rVert}$$

In fact, each $\overline{x}_j \mathbf{1}$ and $\overline{Y} \mathbf{1}$ indicates $\Pi(\mathbf{x}_j \mid \mathbf{1}) = \Pi(\mathbf{x}_j \mid C_0)$ and $\Pi(\mathbf{Y} \mid \mathbf{1}) = \Pi(\mathbf{Y} \mid C_0)$. In step $k = 1$, we might find the variable that maximizes the size of correlation. However, in the other step, the former variables are already in the model. Variables would be compared with respect to partial correlation, not correlation. Thus, each $\mathbf{x}_j$ and $\mathbf{Y}$ is projected to $C_{k - 1}$.

\begin{equation}
  r_{k, j} = \frac{\Big(\mathbf{x}_j - \Pi(\mathbf{x}_j \mid C_{k - 1})\Big)^T \Big(\mathbf{Y} - \Pi(\mathbf{Y} \mid C_{k - 1})\Big)}{\lVert \mathbf{x}_j - \Pi(\mathbf{x}_j \mid C_{k - 1}) \rVert \lVert \mathbf{Y} - \Pi(\mathbf{Y} \mid C_{k - 1}) \rVert}
  (\#eq:parcor)
\end{equation}

Find the variable with an index $j = j_k$ that maximizes $r_{k, j}^2$ and update the subset

$$S_k = S_{k - 1} \cup \{ j_k \}$$

and the column space

$$C_k = Col(\mathbf{1}, \mathbf{x}_l : l \in S_k)$$

This procedure continues until it stops by our rule. For instance,

1. $\lvert S_k \rvert$ exceeds a predetermined size $p^{\ast}$.
2. Partial $F$-statistic $F_{k,j}$ testing $H_0: \beta_{j,k} = 0$ versus predetermined number $F_{IN}$ called *F-to-enter*.

See the second one. From the chosen $k$-th submodel, saying $\mathcal{M}_k$, we can compute partial $F$-statistic.

\begin{equation}
  F_{k,j} = \frac{SSR(X_j \mid X_l : l \in S_{k - 1})}{SSE(X_l : l \in S_k) / (n - k - 1)} < F_{IN}
  (\#eq:fsstop)
\end{equation}

Should we compute both $r_{k,j}$ and $F_{k,j}$ in each step?

```{remark}
$F_{k,j}$ is a monotonic function of $r_{k,j}^2$, that is,

$$F_{k,j} = (n - k - 1) \frac{r_{k,j}^2}{1 - r_{k,j}^2}$$

with $0 \le r_{k,j}^2 \le 1$
```

```{proof}
Consider the model with $S_k$ and $S_{k - 1}$. It can be shown that

\begin{equation}
  r_{k,j}^2 = \frac{SSR(X_j \mid X_l : l \in S_{k - 1})}{SSE(S_{k - 1})}
  (\#eq:rkjss)
\end{equation}

It follows that

\begin{equation*}
  \begin{split}
   F_{k,j} & = \frac{SSR(X_j \mid X_l : l \in S_{k - 1})}{SSE(S_k) / (n - k - 1)} \\
   & = (n - k - 1) \frac{SSR(X_j \mid X_l : l \in S_{k - 1}) / SSE(S_{k - 1})}{SSE(S_k) / SSE(S_{k - 1})} \\
   & = (n - k - 1) \frac{r_{k,j}^2}{(SSE(S_{k - 1}) - SSR(X_j \mid X_l : l \in S_{k - 1})) / SSE(S_{k - 1}))} \\
   & = (n - k - 1) \frac{r_{k, j}^2}{1 - r_{k,j}^2}
  \end{split}
\end{equation*}
```

Thus, choosing $j$ that maximizes $r_{k,j}^2$ is equivalent to that maximizes $F_{k,j}$. It also equivalent to choosing $j$ maximizing the coefficient of determination $R^2$ when the variable $X_j$ is added to the existing group $S_{k - 1}$. The procedure is enough to use only $F_{k,j}$ in each step.

```{r, include=FALSE}
alg_num <- alg_num + 1
fwsel <- alg_num
fwselstr <- "
| **Data**: $Y_i$ and every predictor $x_{i1}, \\ldots x_{ip}$
| **Input**: F-to-enter $F_{IN}$
| Initialize subset of variables $S_0 = \\varnothing$;
| Initialize design column space $C_0 = Col(\\mathbf{1})$;
| Initialize null model $\\mathcal{M}_0$ by $Y_i = \\beta_0 + \\epsilon_i$;
| **For** $k \\leftarrow 1$ **to** $p$:
|   For remained $(p - k + 1)$ predictors, compute partial $F$-statistic $F_{k,j} = \\frac{SSR(X_j \\mid X_l : l \\in S_{k - 1})}{SSE(X_l : l \\in S_k) / (n - k - 1)}$;
|   $j_k = \\argmax_j F_{k,j}$
|   **If** $F_{k, j_k} \\ge F_{IN}$:
|      Update the subset $S_k = S_{k - 1} \\cup \\{ j_k \\}$;
|      Update the column space $C_k = Col( \\mathbf{1}, \\mathbf{x}_l : l \\in S_k)$;
|   **Else**
|      Stop the procedure;
| **end**
| Select a single best among $\\mathcal{M}_0, \\mathcal{M}_1, \\ldots, \\mathcal{M}_p$ using $R_a^2$, $C_p$, $AIC$, or $BIC$;
| **Output**: $\\mathcal{M}_{k^{\\prime}}$ with the best criterion value
|"
fwselstr <- paste0("Algorithm", fwsel, ": Forward Selection")
```

\begin{algorithm}[H] \label{alg:fwsel}
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \KwData{$Y_i$ and every predictor $x_{i1}, \ldots x_{ip}$}
  \Input{F-to-enter $F_{IN}$}
  Initialize subset of variables $S_0 = \varnothing$\;
  Initialize design column space $C_0 = Col(\mathbf{1})$\;
  Initialize null model $\mathcal{M}_0$ by $Y_i = \beta_0 + \epsilon_i$\;
  \For{$k \leftarrow 1$ \KwTo $p$}{
    For remained $(p - k + 1)$ predictors, compute partial $F$-statistic $$F_{k,j} = \frac{SSR(X_j \mid X_l : l \in S_{k - 1})}{SSE(X_l : l \in S_k) / (n - k - 1)}$$\;
    $j_k = \argmax_j F_{k,j}$\;
    \eIf{$F_{k, j_k} \ge F_{IN}$}{
      Update the subset $S_k = S_{k - 1} \cup \{ j_k \}$\;
      Update the column space $C_k = Col(\mathbf{1}, \mathbf{x}_l : l \in S_k)$\;
    }{
      Stop the procedure\;
    }
  }
  \Output{$\mathcal{M}_k$ corresponding to selected variables $S_k$}
  \caption{Forward Selection}
\end{algorithm}

`r if (is_html) fwselstr`

Even if Algorithm `r if (is_latex) {"\\@ref(alg:fwsel)"} else fwsel` goes to the end $p$, the number of models reviewed is less than of all possible regressions.

\begin{equation*}
  \begin{split}
    \underset{\mathcal{M}_0}{\underline{1}} + p + (p - 1) + \cdots + (p - (p - 1) + 1) + (p - p + 1) & = 1 + \sum_{k = 1}^p p \\
    & = 1 + \frac{p (p + 1)}{2} \\
    & \color{red}{\le 2^p}
  \end{split}
\end{equation*}

Thus, it has computational advantage over all possible regressions. Moreover, this method is even possible for high-dimensional setting $n < p$. Even though OLS cannot be computed in full model, forward selection procedure is able to go on until $n - 1$ variable-subset model.

Since this does not look at the whole models but *nested models*, it is not guaranteed to find the best model which comes from all possible regressions.

### Backward elimination

Backward elimination is just the reverse of forward selection. Beginning with the full model, it iteratively removes the least relevant predictor one-at-a-time.

```{r bwtab, echo=FALSE, results='asis'}
tribble(
  ~x1, ~x2, ~x3, ~best,
  #--/----/----
  3, 3, 3, FALSE,
  2, 3, 3, FALSE,
  3, 2, 3, TRUE,
  3, 3, 2, FALSE,
  1, 2, 3, FALSE,
  3, 2, 1, TRUE,
  0, 2, 1, TRUE
) %>% 
  mutate_if(
    is.numeric,
    function(x) {
      kableExtra::cell_spec(
        x,
        format = "latex",
        color = ifelse(.$best, "red", "black")
      )
    }
  ) %>% 
  select(-best) %>%
  knitr::kable(
    format = "latex", 
    escape = FALSE, 
    align = "c",
    longtable = TRUE,
    caption = "Illustration of Backward elimination"
  ) %>% 
  kableExtra::kable_styling(c("striped", "condensed"), full_width = FALSE) %>% 
  kableExtra::pack_rows("p=2", start_row = 2, end_row = 4) %>% 
  kableExtra::pack_rows("p=1", start_row = 5, end_row = 6) %>% 
  kableExtra::pack_rows("p=0", start_row = 7, end_row = 7)
```

Similarly, we use partial correlation $r_{k,j}$ or equivalent partial $F$-statistic $F_{k,j}$. Since we are looking for the least important one, we find the smallest value. We can remove the variable with the smallest changes in $R^2$. Stopping rules are also similar.

1. Stop when $\lvert S_k \rvert$ get to a predetermined size $p^{\ast}$.
2. Partial $F$-statistic $F_{k,j}$ is larger than or equal to predetermined $F_{OUT}$ called *F-to-leave*.

We have stopped if $F_{k,j} < F_{IN}$ in forward selection $\ref{alg:fwsel}$. On the other hand, backward selection stops if $F_{k,j} \ge F_{OUT}$.

```{r, include=FALSE}
alg_num <- alg_num + 1
backsel <- alg_num
backselstr <- "
| **Data**: $Y_i$ and every predictor $x_{i1}, \\ldots x_{ip}$
| **Input**: F-to-leave $F_{OUT}$
| Initialize subset of variables $S_0 = \\varnothing$;
| Initialize design column space $C_0 = Col(\\mathbf{1})$;
| Initialize null model $\\mathcal{M}_0$ by $Y_i = \\beta_0 + \\epsilon_i$;
| **For** $k \\leftarrow 1$ **to** $p$:
|   For non-removed $(p - k + 1)$ predictors, compute partial $F$-statistic $F_{k,j} = \\frac{SSR(X_j \\mid X_l : l \\in S_{k - 1})}{SSE(X_l : l \\in S_k) / (n - k - 1)}$;
|   $j_k = \argmin_j F_{k,j}$;
|   **If** $F_{k, j_k} < F_{OUT}$:
|      Update the subset $S_k = S_{k - 1} - \\{ j_k \\}$;
|      Update the column space $C_k = Col( \\mathbf{1}, \\mathbf{x}_l : l \\in S_k)$;
|   **Else**
|      Stop the procedure;
| **end**
| **Output**: $\\mathcal{M}_k$ corresponding to selected variables $S_k$
|"
backselstr <- paste0("Algorithm", backsel, ": Backward elimination")
```

\begin{algorithm}[H] \label{alg:backsel}
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \KwData{$Y_i$ and every predictor $x_{i1}, \ldots x_{ip}$}
  \Input{F-to-leave $F_{OUT}$}
  Initialize subset of variables $S_0 = \varnothing$\;
  Initialize design column space $C_0 = Col(\mathbf{1})$\;
  Initialize null model $\mathcal{M}_0$ by $Y_i = \beta_0 + \epsilon_i$\;
  \For{$k \leftarrow 1$ \KwTo $p$}{
    For non-removed $(p - k + 1)$ predictors, compute partial $F$-statistic $$F_{k,j} = \frac{SSR(X_j \mid X_l : l \in S_{k - 1})}{SSE(X_l : l \in S_k) / (n - k - 1)}$$\;
    $j_k = \argmin_j F_{k,j}$\;
    \eIf{$F_{k, j_k} < F_{OUT}$}{
      Update the subset $S_k = S_{k - 1} - \{ j_k \}$\;
      Update the column space $C_k = Col(\mathbf{1}, \mathbf{x}_l : l \in S_k)$\;
    }{
      Stop the procedure\;
    }
  }
  \Output{$\mathcal{M}_k$ corresponding to selected variables $S_k$}
  \caption{Backward elimination}
\end{algorithm}

`if (is_html) backselstr`

As forward selection, backward elimination `r if (is_latex) {"\\@ref(alg:backsel)"} else backsel` investigates $1 + \frac{p(p + 1)}{2}$ models. It has computational advantages, but does not guarantee the best model among every model. Since this procedure starts from the full model, it is not available when $n < p$, while forward selection is.

### Stepwise regression

Since each forward and backward method has own problem, a hybrid approach has been made. Beginning with null model, add one predictor as in forward selection, and remove any predictor that is useless in that step after adding one predictor.

```{r, include=FALSE}
alg_num <- alg_num + 1
hybrid <- alg_num
hybridstr <- "
| **Data**: $Y_i$ and every predictor $x_{i1}, \\ldots x_{ip}$
| **Input**: F-to-enter $F_{IN}$ and F-to-leave $F_{OUT}$
| Initialize subset of variables $S_0 = \\varnothing$;
| Initialize design column space $C_0 = Col(\\mathbf{1})$;
| Initialize null model $\\mathcal{M}_0$ by $Y_i = \\beta_0 + \\epsilon_i$;
| **For** $k \\leftarrow 1$ **to** $p$:
|   For remained $(p - k + 1)$ predictor, compute partial $F$-statistic $F_{k,j} = \\frac{SSR(X_j \\mid X_l : l \\in S_{k - 1})}{SSE(X_l : l \\in S_k) / (n - k - 1)}$;
|   $j_k = \\argmax_j F_{k,j}$
|   **If** $F_{k, j_k} \\ge F_{IN}$:
|      Compute partial $F$-statistic including the new variable $F_{k,i}^{\\ast} = \\frac{SSR(X_i \\mid (X_l : l \\in S_{k - 1}, l \\neq i), X_{j_k})}{SSE((X_l : l \\in S_k), X_{jk}) / (n - k - 1)}$;
|      Identify a set of $i \\in S_{k - 1}$ denoted by $D_{k - 1}$ s.t. $F_{k,i}^{\\ast} < F_{OUT}$;
|      Update the subset $S_k = S_{k - 1} \\cup \\{ j_k \\} - D_{k - 1}$;
|      Update the column space $C_k = Col( \\mathbf{1}, \\mathbf{x}_l : l \\in S_k)$;
|   **Else**
|      Stop the procedure;
| **end**
| **Output**: $\\mathcal{M}_k$ corresponding to selected variables $S_k$
|"
hybridstr <- paste0("Algorithm", hybrid, ": Stepwise regression")
```

\begin{algorithm}[H] \label{alg:hybrid}
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \KwData{$Y_i$ and every predictor $x_{i1}, \ldots x_{ip}$}
  \Input{F-to-enter $F_{IN}$ and F-to-leave $F_{OUT}$}
  Initialize subset of variables $S_0 = \varnothing$\;
  Initialize design column space $C_0 = Col(\mathbf{1})$\;
  Initialize null model $\mathcal{M}_0$ by $Y_i = \beta_0 + \epsilon_i$\;
  \For{$k \leftarrow 1$ \KwTo $p$}{
    For each remained predictor, compute partial $F$-statistic $$F_{k,j} = \frac{SSR(X_j \mid X_l : l \in S_{k - 1})}{SSE(X_l : l \in S_k) / (n - k - 1)}$$\;
    $j_k = \argmax_j F_{k,j}$\;
    \eIf{$F_{k, j_k} \ge F_{IN}$}{
      Compute partial $F$-statistic including the new variable $$F_{k,i}^{\ast} = \frac{SSR(X_i \mid (X_l : l \in S_{k - 1}, l \neq i), X_{j_k})}{SSE((X_l : l \in S_k), X_{jk}) / (n - k - 1)}$$\;
      Identify a set of $i \in S_{k - 1}$ denoted by $D_{k - 1}$ s.t. $$F_{k,i}^{\ast} < F_{OUT}$$\;
      Update the subset $S_k = S_{k - 1} \cup \{ j_k \} - D_{k - 1}$\;
      Update the column space $C_k = Col(\mathbf{1}, \mathbf{x}_l : l \in S_k)$\;
    }{
      Stop the procedure\;
    }
  }
  \Output{$\mathcal{M}_k$ corresponding to selected variables $S_k$}
  \caption{Stepwise regression}
\end{algorithm}

`if (is_html) hybridstr`

In this process, adding and removal can happen simultaneously. This makes iteration more complex than forward or backward method.

### Computational Techniques in `R`



