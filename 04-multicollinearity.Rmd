# Multicollinearity {#coll}

## Multicollinearity

See a dataset about equal opportunity in public education.

```{r}
(eeo <- haven::read_sav("data/p228.sav"))
```

In 1965, 70 schools were selected at random. 4 measurements were taken for each school.

- `ACHV`: student achievements, *response variable*
- `FAM`: faculty credentials
- `PEER`: influence of their peer group in the school
- `SCHOOL`: school facilities

[@Chatterjee:2015aa].

```{definition, colldef, name = "Multicollinearity"}
A set of predictors $\{ X_1, X_2, \ldots, X_p \}$ is said to have \textbf{\textit{multicollinearity}} iff there exist linear or near-linear dependencies among predictors.
```

### Effects of multicollinearity

Consider design matrix

$$X = \begin{bmatrix} \mathbf{1} & \mathbf{x}_1 & \cdots & \mathbf{x}_p \end{bmatrix} \in \R^{n \times (p + 1)}$$

When there exists any linear dependency among the predictors, column vectors of $X$ are *lineary dependent*. Or equivalently, centered columns $\{ \mathbf{x}_1 - \overline{x}_1 \mathbf{1}, \ldots, \mathbf{x}_p - \overline{x}_p \mathbf{1} \}$ are lineary dependent. Then

$$rank(X) < p + 1 \le n$$

i.e. rank deficient. From either Theorem \@ref(thm:fullrank) or \@ref(thm:fullrank2), $X^TX$ can be *singular*. In this case, we cannot get the LSE solution. It is not only the problem of computing but also the variance. Consider *total variance*, sum of every coefficient variance. From Proposition \@ref(prp:multbmoment),

\begin{equation*}
  \begin{split}
    \sum_{j = 0}^p Var(\hat\beta_j) & = trace \Big( Var(\hb) \Big) \\
    & = trace \Big( \sigma^2(X^TX)^{-1} \Big) \\
    & = \sigma^2 trace \Big( (X^TX)^{-1} \Big) \\
    & = \sigma^2 \sum_{j = 0}^p \frac{1}{\kappa_j} \quad \kappa_j := \text{eigenvalues of}\: X^TX = \text{singular values}^2
  \end{split}
\end{equation*}

If $X$ is not of full rank, $X^T X$ is not of full rank. In other words,

$$\exists j : \kappa_j = 0$$

By construction, even one $\kappa_j = 0$ results in

$$\sum_{j = 0}^p Var(\hat\beta_j) = \infty$$

It is found that *linear dependency leads to increasing variance of the estimates*. This variance problem occurs when nearly-linear dependeny situation, of course. So we should detect and remedy this.

### Multicollinearity diagnostics

```{lemma, varbr}
Consider regression model $j$-th predictor $X_j$ on the remaing $X_k, \: k \neq j$, i.e.

$$X_{ij} = \alpha_0 + \alpha_1 x_{i1} + \cdots + \alpha_{j - 1} x_{i, j - 1} + \alpha_{j + 1} x_{i, j + 1} + \cdots + \alpha_p x_{ip} + \epsilon_i$$

Let $s_{jj} = \sum\limits_{i = 1}^n (x_{ij} - \overline{x}_j)^2$ be corrected sum of squares and let $R_j^2$ be the coefficient of determination. Then

$$Var(\hat\beta_j) = \frac{1}{1 - R_j^2} \frac{\sigma^2}{s_{jj}}, \quad 1 \le j \le p$$
```

```{proof}
Assume $j = 1$ without loss of generality. Write

$$X = \begin{bmatrix} \mathbf{x}_1 & X_B \end{bmatrix}$$

Then we have

\begin{equation}
  X \B = \mathbf{x}_1 \B_{A} + X_{B} \B_{B}
  (\#eq:xabsplit)
\end{equation}

Orthogonalize $\mathbf{x}_1$ by projecting onto $X_B$ as before.

$$\mathbf{x}_{1, \perp} := \mathbf{x}_1 - \Pi_B \mathbf{x}_1$$

where $\Pi_1 = X_B (X_B^T X_B)^{-1} X_B^T$. It follows that

\begin{equation*}
  \begin{split}
    X \hb & = \mathbf{x}_1 \hb_1 + X_B \hb_{B} \\
    & = \mathbf{x}_{1, \perp} \hb_1 + X_B \Big( \hb_{B} + (X_B^T X_B)^{-1} X_B^T \mathbf{x}_1 \hb_{1} \Big) \\
    & = \Pi(\mathbf{Y} \mid R(\mathbf{x}_{1, \perp})) + \Pi(\mathbf{Y} \mid R(X_B))
  \end{split}
\end{equation*}

Thus,

$$
\begin{cases}
  \hb_{1, \perp} = (\mathbf{x}_1^T \mathbf{x}_1)^{-1} \mathbf{x}_1^T \mathbf{Y} \\
  \hb_B = (X_B^T X_B)^{-1} X_B^T (\mathbf{Y} - \mathbf{x}_1 \hb_{1})
\end{cases}
$$

Note that $\mathbf{x}_{1, \perp}$ is the *residual vector in our previous resgression model* in the statement. By construction,

$$\mathbf{x}_{1, \perp}^T \mathbf{x}_{1, \perp} = SSE$$

Since $SST = s_{11}$,

$$R_{1}^2 = \frac{SST - SSE}{SST} = \frac{s_{11} - \mathbf{x}_{1, \perp}^T \mathbf{x}_{1, \perp}}{s_{11}}$$

Therefore,

\begin{equation}
  \mathbf{x}_{1, \perp}^T \mathbf{x}_{1, \perp} = s_{11} (1 - R_1^2)
  (\#eq:s11r)
\end{equation}

From Equation \@ref(eq:s11r),

$$Var(\hat\beta_1) = \sigma^2(\mathbf{x}_{1, \perp}^T \mathbf{x}_{1, \perp})^{-1} = \frac{\sigma^2}{s_{11}(1 - R_1^2)}$$

One proceeds in a similary way for the other $j$.

\begin{equation}
  \mathbf{x}_{j, \perp}^T \mathbf{x}_{j, \perp} = s_{j} (1 - R_j^2)
  (\#eq:sjjr)
\end{equation}
```

What does Lemma \@ref(lem:varbr) mean? Recall that in simple linear regression setting,

$$Var(\hat\beta_j) = \frac{\sigma^2}{s_{jj}}$$

When multicollinearity occurs, one term is multiplied so that

$$Var(\hat\beta_j) = \frac{1}{1 - R_j^2} \frac{\sigma^2}{s_{jj}}$$

Naturally, we can think of $\frac{1}{1 - R_j^2}$ detecting multicollinearity.

```{definition, vif, name = "Variance inflation factor"}
Let $R_j^2$ be the coefficient of determination that results when $X_j$ is regressed against remaining $X_k, \: k \neq j$. Then the variance inflation factor for $X_j$ is defined by

$$VIF_j := \frac{1}{1 - R_j^2}, \: j = 1, \ldots, p$$
```












