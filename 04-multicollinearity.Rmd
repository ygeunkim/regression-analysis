# Multicollinearity {#coll}

## Multicollinearity

See a dataset about equal opportunity in public education.

```{r}
(eeo <- haven::read_sav("data/p228.sav"))
```

In 1965, 70 schools were selected at random. 4 measurements were taken for each school.

- `ACHV`: student achievements, *response variable*
- `FAM`: faculty credentials
- `PEER`: influence of their peer group in the school
- `SCHOOL`: school facilities

[@Chatterjee:2015aa].

```{r}
eeo_fit <- lm(ACHV ~ ., data = eeo)
```

```{definition, colldef, name = "Multicollinearity"}
A set of predictors $\{ X_1, X_2, \ldots, X_p \}$ is said to have \textbf{\textit{multicollinearity}} iff there exist linear or near-linear dependencies among predictors.
```

### Effects of multicollinearity

Consider design matrix

$$X = \begin{bmatrix} \mathbf{1} & \mathbf{x}_1 & \cdots & \mathbf{x}_p \end{bmatrix} \in \R^{n \times (p + 1)}$$

When there exists any linear dependency among the predictors, column vectors of $X$ are *lineary dependent*. Or equivalently, centered columns $\{ \mathbf{x}_1 - \overline{x}_1 \mathbf{1}, \ldots, \mathbf{x}_p - \overline{x}_p \mathbf{1} \}$ are lineary dependent. Then

$$rank(X) < p + 1 \le n$$

i.e. rank deficient. From either Theorem \@ref(thm:fullrank) or \@ref(thm:fullrank2), $X^TX$ can be *singular*. In this case, we cannot get the LSE solution. It is not only the problem of computing but also the variance. Consider *total variance*, sum of every coefficient variance. From Proposition \@ref(prp:multbmoment),

\begin{equation*}
  \begin{split}
    \sum_{j = 0}^p Var(\hat\beta_j) & = trace \Big( Var(\hb) \Big) \\
    & = trace \Big( \sigma^2(X^TX)^{-1} \Big) \\
    & = \sigma^2 trace \Big( (X^TX)^{-1} \Big) \\
    & = \sigma^2 \sum_{j = 0}^p \frac{1}{\kappa_j} \quad \kappa_j := \text{eigenvalues of}\: X^TX = \text{singular values}^2
  \end{split}
\end{equation*}

If $X$ is not of full rank, $X^T X$ is not of full rank. In other words,

$$\exists j : \kappa_j = 0$$

By construction, even one $\kappa_j = 0$ results in

$$\sum_{j = 0}^p Var(\hat\beta_j) = \infty$$

It is found that *linear dependency leads to increasing variance of the estimates*. This variance problem occurs when nearly-linear dependeny situation, of course. So we should detect and remedy this.

## Multicollinearity diagnostics

### Correlation matrix

Multicollinearity leads to unstable regression coefficients. From Equation \@ref(eq:rsqangle), we know that $\hb$ is related to sample correlation between response and predictors. Pairwise correlation gives information about linear relationship between $X_j$.

```{r}
cor(eeo)
```

### Variance inflation factor

```{lemma, varbr}
Consider regression model $j$-th predictor $X_j$ on the remaing $X_k, \: k \neq j$, i.e.

$$X_{ij} = \alpha_0 + \alpha_1 x_{i1} + \cdots + \alpha_{j - 1} x_{i, j - 1} + \alpha_{j + 1} x_{i, j + 1} + \cdots + \alpha_p x_{ip} + \epsilon_i$$

Let $s_{jj} = \sum\limits_{i = 1}^n (x_{ij} - \overline{x}_j)^2$ be corrected sum of squares and let $R_j^2$ be the coefficient of determination. Then

$$Var(\hat\beta_j) = \frac{1}{1 - R_j^2} \frac{\sigma^2}{s_{jj}}, \quad 1 \le j \le p$$
```

```{proof}
Assume $j = 1$ without loss of generality. Write

$$X = \begin{bmatrix} \mathbf{x}_1 & X_B \end{bmatrix}$$

Then we have

\begin{equation}
  X \B = \mathbf{x}_1 \B_{A} + X_{B} \B_{B}
  (\#eq:xabsplit)
\end{equation}

Orthogonalize $\mathbf{x}_1$ by projecting onto $X_B$ as before.

$$\mathbf{x}_{1, \perp} := \mathbf{x}_1 - \Pi_B \mathbf{x}_1$$

where $\Pi_1 = X_B (X_B^T X_B)^{-1} X_B^T$. It follows that

\begin{equation*}
  \begin{split}
    X \hb & = \mathbf{x}_1 \hb_1 + X_B \hb_{B} \\
    & = \mathbf{x}_{1, \perp} \hb_1 + X_B \Big( \hb_{B} + (X_B^T X_B)^{-1} X_B^T \mathbf{x}_1 \hb_{1} \Big) \\
    & = \Pi(\mathbf{x}_1 \mid R(\mathbf{x}_{1, \perp})) + \Pi(\mathbf{x}_1 \mid R(X_B))
  \end{split}
\end{equation*}

Thus,

$$
\begin{cases}
  \hb_{1, \perp} = (\mathbf{x}_1^T \mathbf{x}_1)^{-1} \mathbf{x}_1^T \mathbf{Y} \\
  \hb_B = (X_B^T X_B)^{-1} X_B^T (\mathbf{Y} - \mathbf{x}_1 \hb_{1})
\end{cases}
$$

Note that $\mathbf{x}_{1, \perp}$ is the *residual vector in our previous resgression model* in the statement. By construction,

$$\mathbf{x}_{1, \perp}^T \mathbf{x}_{1, \perp} = SSE$$

Since $SST = s_{11}$,

$$R_{1}^2 = \frac{SST - SSE}{SST} = \frac{s_{11} - \mathbf{x}_{1, \perp}^T \mathbf{x}_{1, \perp}}{s_{11}}$$

Therefore,

\begin{equation}
  \mathbf{x}_{1, \perp}^T \mathbf{x}_{1, \perp} = s_{11} (1 - R_1^2)
  (\#eq:s11r)
\end{equation}

From Equation \@ref(eq:s11r),

$$Var(\hat\beta_1) = \sigma^2(\mathbf{x}_{1, \perp}^T \mathbf{x}_{1, \perp})^{-1} = \frac{\sigma^2}{s_{11}(1 - R_1^2)}$$

One proceeds in a similary way for the other $j$.

\begin{equation}
  \mathbf{x}_{j, \perp}^T \mathbf{x}_{j, \perp} = s_{j} (1 - R_j^2)
  (\#eq:sjjr)
\end{equation}
```

What does Lemma \@ref(lem:varbr) mean? Recall that in simple linear regression setting,

$$Var(\hat\beta_j) = \frac{\sigma^2}{s_{jj}}$$

When multicollinearity occurs, one term is multiplied so that

\begin{equation}
  Var(\hat\beta_j) = \frac{1}{1 - R_j^2} \frac{\sigma^2}{s_{jj}}
  (\#eq:vifmotif)
\end{equation}


Naturally, we can think of $\frac{1}{1 - R_j^2}$ detecting multicollinearity.

```{definition, vif, name = "Variance inflation factor"}
Let $R_j^2$ be the coefficient of determination that results when $X_j$ is regressed against remaining $X_k, \: k \neq j$. Then the variance inflation factor for $X_j$ is defined by

$$VIF_j := \frac{1}{1 - R_j^2}, \: j = 1, \ldots, p$$
```

From Equation \@ref(eq:vifmotif),

\begin{equation}
  Var(\hat\beta_j) \propto (VIF_j) \sigma^2
  (\#eq:vifprop)
\end{equation}

Variance of $j$-th regression coefficient is proportion to $VIF_j$. The term *variance inflation factor* originated from this fact.

```{corollary, vifcorr}
$VIF_j$ is simple the inflation rate of $Var(\hat\beta_j)$ in comparison with the case where $X_j$ is not correlated with other predictors, i.e.

$$s_{jk} = \sum_{i = 1}^n (x_{ij} - \overline{x}_j)(x_{ik} - \overline{x}_k) = 0 \quad \forall k \neq j$$
```

```{proof}
By definition,

\begin{equation*}
  \begin{split}
    & VIF_j^2 = 1 \\
    & \Leftrightarrow R_j^2 = 0 \\
    & \Leftrightarrow s_{jj} - \mathbf{x}_{j, \perp}^T \mathbf{x}_{j, \perp} = \sum_{i = 1}^n (x_{ij} - \overline{x}_j)^2 - \sum_{i^{\prime} = 1}^n (x_{i^{\prime}j} - \overline{x}_j) (x_{i^{\prime}k} - \overline{x}_k) = 0 \\
    & \Leftrightarrow s_{jk} = \sum_{i^{\prime} = 1}^n (x_{ij} - \overline{x}_j) (x_{ik} - \overline{x}_k) = 0 \quad \forall k \neq j
  \end{split}
\end{equation*}
```

For example, suppose that

$$x_1 \approx c_0 + c_2 x_2 + \cdots + c_p x_p$$

Then

$$R_1^2 \approx 1$$

and so

$$VIF_1 \rightarrow \infty$$

```{remark}
Large $VIF_j$ for one or multiple $j$s indicate multicollinearity.
```

@Rawlings:2006aa and @Chatterjee:2015aa suggest some thresholds with references.

```{conjecture, vifthres1}
If $$VIF_j > 10$$,
then $\beta_j$ would be poorly estimated
```

Note that the *precision of OLS* is measured by its variance. Using the proportionality \@ref(eq:vifprop), let $D^2$ be the *expected squred distance of OLS* estimators [@Chatterjee:2015aa].

$$D^2 = \sigma^2 \sum_{j = 1}^p VIF_j$$

The smaller, the more accurate OLS is.

```{remark}
If predictor variables are orthogonal, then

$$\forall j : VIF_j = 1$$

and so

$$D^2 = p\sigma^2$$
```

Consider the ratio of $D^2$ to orthogonal $D^2$.

$$\frac{\sigma^2 \sum VIF_j}{p \sigma^2} = \frac{1}{p} \sum VIF_j \equiv \overline{VIF}$$

```{definition, vifbar}
Write average of every $VIF_j$ by

$$\overline{VIF} := \frac{1}{p} \sum_{j = 1}^p VIF_j$$
```

$\overline{VIF}$ is not just average. The remark implies that $\overline{VIF}$ estimates the ratio of the true multicollinearity to a model when predictors are uncorrelated. Hence, this can also be used as an criterion of multicollinearity.

```{conjecture, vifthres2}
If $$\overline{VIF} >> 1$$,
then serious multicollinearity might occur.
```

`car` library has a function `vif()`.

```{r}
car::vif(eeo_fit)
```

### Condition number

```{theorem, symeigen}
Let $\Sigma \in \R^{p \times p}$. Then the eigenvalues of $\Sigma$ are all real.
```

```{proof}
Let

$$\Sigma \mathbf{x} = \lambda \mathbf{x}$$

Then

$$\mathbf{x}^T \Sigma \mathbf{x} = \lambda \mathbf{x}^T \mathbf{x} = \lambda \lVert \mathbf{x} \rVert^2$$

Write $\alpha := \mathbf{x}^T \Sigma \mathbf{x}$. 

Claim: $\alpha \in \R$

Since $\mathbf{x} \in \R^p$ and $\Sigma \in \R^{p \times p}$, i.e. real matrix, it is obvious that $\alpha$ is real.

Since $\lVert \mathbf{x} \rVert^2 \neq 0$,

$$\lambda = \frac{\alpha}{\lVert \mathbf{x} \rVert^2}$$

is also a real number.
```

```{theorem, xtxeigen}
Let $X \in \R^{n \times p}$. Then the eigenvalues of $X^T X$ are all non-negative real number.
```

```{proof}
Since $X \in \R^{n \times p}$, $X^T X \in \R^{n \times n}$ and is symmetric. Let

$$(X^T X) \B = \lambda \B, \quad \B \neq \mathbf{0}$$

Since $X^T X$ is symmetric, every $\lambda \in \R$.

\begin{equation*}
  \begin{split}
    \lVert X \B \rVert^2 & = (X \B)^T (X \B) \\
    & = \B^T X^T X \B \\
    & = \lambda \B^T \B \qquad X^T X \B = \lambda \B \\
    & = \lambda \lVert \B \rVert^2
  \end{split}
\end{equation*}

Hence,

\begin{equation}
  \lambda = \frac{\lVert X \B \rVert^2}{\lVert \B \rVert^2} \ge 0
  (\#eq:nonegeigen)
\end{equation}
```

From Theorem \@ref(thm:xtxeigen), let

$$\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_{p + 1} \ge 0$$

be eigenvalues of $X^T X$. Then we have

$$\lambda_{p + 1} > 0$$

guarantees existence of $(X^T X)^{-1}$. Reversely,

$$\lambda_{p + 1} \approx 0$$

results in nearly-non-invertibility.

```{definition, kondition, name = "Condition number"}
Let $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_{p + 1} \ge 0$ be eigenvalues of $X^T X$. Then \textbf{\textit{Condition number}} of $X^T X$ is defined by

$$\kappa := \frac{\lambda_1}{\lambda_{p + 1}} = \frac{(\text{maximal eigenvalue of} \: X^TX)}{(\text{minimal eigenvalue of} \: X^TX)}$$
```

Some textbooks such as @Chatterjee:2015aa define this measure additional square root, i.e. singular values of $X$. Since how large is important, it does not matter much. $\kappa$ measures *how small minimal eigenvalue compared to maximal eigenvalue*. This means spread of the eigenvalue spectrum of $X^T X$.

```{conjecture, kappathres}
The larger $\kappa$, the more serious multicollinearity is. ($\lambda_{p + 1} \approx 0 \Rightarrow \kappa \rightarrow \infty$)

\begin{enumerate}
  \item Weak dependence $\kappa \approx 100$
  \item Moderate to strong $100 \le \kappa \le 1000$
  \item Severe $\kappa \ge 1000$
\end{enumerate}
```

There is a base function called `kappa()` calculating the condition number of a `matrix`. Since this function has a `S3` method for `lm`, we can just provide `lm` object to get $\kappa$ of our model.

```{r}
kappa(eeo_fit)
```


## Principal Component Analysis

Multicollinearity violates the assumption of OLS model. This makes the estimator unstable - large variance. Sometimes we cannot even get the solution. In a linear regression frame, there exist some alternative to OLS dealing with large variance. Here we present two methods.

1. **Principal component regression**: By construction, principal components are uncorrelated.
2. Shrinkage methods: **Ridge regression** enables least squres get the solution and shrinks its variance.

Before looking at principal component analysis (PCA), we see some preliminary matrix algebra theorems: spectral decomposition and singular value decomposition.

### Spectral decomposition

```{theorem, specdecomp, name = "Spectral Decomposition"}
If $A \in \R^{p \times p}$ is a real symmetric matrix, then $A$ is diagonalizable as

$$A = P \Lambda P^T$$

where

$$
P = \begin{bmatrix}
  \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_p
\end{bmatrix} = \begin{bmatrix}
  v_{11} & \cdots & v_{1p} \\
  \vdots & \cdots & \vdots \\
  v_{p1} & \cdots & v_{pp}
\end{bmatrix} \: \text{orthogonal}
$$

and

$$\Lambda = diag(\lambda_1, \ldots, \lambda_p)$$

with eigenvalue and corresponding orthonormal eigenvector of $A$ $(\lambda_j, \mathbf{v}_j), \: j = 1, \ldots, p$.
```

The theorem satisfies when a matrix is *symmetric*. For example, covariance matrix or correlation matrix. The matrix decomposition can also be expressed as following corollary.

```{corollary, specdecomp2}
If $A \in \R^{p \times p}$ is a real symmetric matrix, then

$$A = \lambda_1 \mathbf{v}_1 \mathbf{v}_1^T + \cdots + \lambda_p \mathbf{v}_p \mathbf{v}_p^T$$

where each $(\lambda_j, \mathbf{v}_j)$ is defined as in Theorem \@ref(thm:specdecomp).
```

Spectral decomposition $A = P \Lambda P^T$ gives some useful facts to $A$.

```{proposition, specprop, name = "Properties of spectral decomposition"}
Let $A \in \R^{p \times p}$ be a real symmetric matrix. Then $A = P \Lambda P^T$.

\begin{itemize}
  \item $PP^T = P^T P = I$ so that $P^{-1} = P^T$
  \item $A^{-1} = P \Lambda^{-1} P^T$ with $\Lambda = diag \Big(\frac{1}{\lambda_1}, \ldots, \frac{1}{lambda_p} \Big)$
  \item $A^k = P \Lambda^k P^T$ with $\Lambda = diag(\lambda_1^k, \ldots, \lambda_p^k)$
\end{itemize}
```

```{proof}
Since eigenvectors are orthonormalized, $P$ is orthogonal.

$$A^{-1} = (P^T)^{-1} \Lambda^{-1} P^{-1} = P \Lambda^{-1} P^T$$

$$A^k = (P \Lambda \underset{= I}{\underline{P^T) (P}} \Lambda P^T) \cdots (P \Lambda \underset{= I}{\underline{P^T) (P}} \Lambda P^T) = P \Lambda^k P^T$$

Recall that $\Lambda$ is diagonal.
```

Collaborating with quadrating form, spectral decomposition can give a geometric meaning.

```{theorem, pat, name = "Principal Axes Theorem"}
If $A \in \R^{p \times p}$ is a real symmetric matrix, then

$$\exists \: \text{change of variables} \: \mathbf{u} = P^T \mathbf{X} \quad\text{such that} \quad \mathbf{X}^T A \mathbf{X} = \mathbf{u}^T \Lambda \mathbf{u}$$

where $\Lambda$ is a diagonal matrix.
```

```{proof}
Spectral decomposition \@ref(thm:specdecomp) implies that

$$P^T AP = \Lambda$$

Then

$$\mathbf{X}^T A \mathbf{X} = \mathbf{u}^T P^T A P \mathbf{u} = \mathbf{u}^T \Lambda \mathbf{u}$$
```

```{corollary, pat2}
If $A \in \R^{p \times p}$ is a real symmetric matrix, then there exists a change of variables $\mathbf{u} = P^T \mathbf{X}$ s.t.

$$\mathbf{X}^T A \mathbf{X} = \lambda_1 \mathbf{v}_1^T \mathbf{X} + \cdots + \lambda_p \mathbf{v}_p^T \mathbf{X}$$
```

Above Theorem \@ref(thm:pat) and Corollary \@ref(cor:pat2) is linearly transforming its coordinates of conic section. The directions of coordinate are $\mathbf{v}_j$, orthonormal eigenvectors. Axes are determined by corresponding egenvalues.

### Singular value decomposition















