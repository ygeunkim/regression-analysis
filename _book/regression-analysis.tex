\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={R Lab for Regression Analysis},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{R Lab for Regression Analysis}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Young-geun Kim\\
Department of Statistics, SKKU\\
\href{mailto: dudrms33@g.skku.edu}{dudrms33@g.skku.edu}}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{06 Apr, 2019}

\usepackage{booktabs}
\usepackage{float}
\usepackage{pdfpages}

\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\indep}{\stackrel{indep}{\sim}}
\newcommand{\hsim}{\stackrel{H_0}{\sim}}
\newcommand{\ind}{\perp\!\!\!\perp}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{argmin}

\let\oldmaketitle\maketitle
\AtBeginDocument{\let\maketitle\relax}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

\begin{titlepage}
  \includepdf{cover.pdf}
\end{titlepage}

\let\maketitle\oldmaketitle
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{welcome}{%
\chapter*{Welcome}\label{welcome}}
\addcontentsline{toc}{chapter}{Welcome}

This book aims at covering materials of regression analysis. Also, there will be R programming for regression.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\texttt{tidyverse} package will be used in every chapter, so loading step will be hidden.

\hypertarget{linear-regression-analysis}{%
\section*{Linear Regression Analysis}\label{linear-regression-analysis}}
\addcontentsline{toc}{section}{Linear Regression Analysis}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(BioOxyDemand, }\DataTypeTok{package =} \StringTok{"MPV"}\NormalTok{)}
\NormalTok{(BioOxyDemand <-}
\StringTok{  }\NormalTok{BioOxyDemand }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{tbl_df}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 14 x 2
       x     y
   <int> <int>
 1     3     4
 2     8     7
 3    10     8
 4    11     8
 5    13    10
 6    16    11
 7    27    16
 8    30    26
 9    35    21
10    37     9
11    38    31
12    44    30
13   103    75
14   142    90
\end{verbatim}

\hypertarget{relation}{%
\subsection*{Relation}\label{relation}}
\addcontentsline{toc}{subsection}{Relation}

We wonder how \texttt{x} affects \texttt{y}, especially linearly.

\begin{itemize}
\tightlist
\item
  Functional relation: mathematical equation, \[y = \beta_0 + \beta_1 x\]
\item
  Statistical relation: embeded with noise
\end{itemize}

So we try to estimate

\[y = \beta_0 + \beta_1 x + \epsilon\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{BioOxyDemand }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(x, y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/unnamed-chunk-4-1} \end{center}

Looking just with the eyes, we can see the linear relationship. Regression analysis estimates the relationship statistically.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{BioOxyDemand }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(x, y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/unnamed-chunk-5-1} \end{center}

\hypertarget{simple}{%
\chapter{Simple Linear Regression}\label{simple}}

\hypertarget{model}{%
\section{Model}\label{model}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delv <-}\StringTok{ }\NormalTok{MPV}\OperatorTok{::}\NormalTok{p2}\FloatTok{.9} \OperatorTok{%>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delv }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Number of Cases"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Delivery Time"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/delivery-1} 

}

\caption{The Delivery Time Data}\label{fig:delivery}
\end{figure}

Given data \((x_1, Y_1), \ldots, (x_n, Y_n)\), we try to fit linear model

\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]

Here \(\epsilon_i\) is a error term, which is a random variable.

\[\epsilon \stackrel{iid}{\sim} (0, \sigma^2)\]

It gives the problem of estimating three parameters \((\beta_0, \beta_1, \sigma^2)\). Before estimating these, we set some assumptions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  linear relationship
\item
  \(\epsilon_i\)s are independent
\item
  \(\epsilon_i\)s are identically destributed, i.e.~\emph{constant variance}
\item
  In some setting, \(\epsilon_i \sim N\)
\end{enumerate}

\hypertarget{least-squares-estimation}{%
\section{Least Squares Estimation}\label{least-squares-estimation}}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/lsefig-1} 

}

\caption{Idea of the least square estimation}\label{fig:lsefig}
\end{figure}

We try to find \(\beta_0\) and \(\beta_1\) that minimize the sum of squares of the vertical distances, i.e.

\begin{equation}
  (\beta_0, \beta_1) = \arg\min \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2
  \label{eq:ssq}
\end{equation}

\hypertarget{normal-equations}{%
\subsection{Normal equations}\label{normal-equations}}

Denote that Equation \eqref{eq:ssq} is quadratic. Then we can find its minimum by find the zero point of the first derivative. Set

\[Q(\beta_0, \beta_1) := \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2\]

Then we have

\begin{equation}
  \frac{\partial Q}{\partial \beta_0} = -2 \sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i) = 0
  \label{eq:normbeta0}
\end{equation}

and

\begin{equation}
  \frac{\partial Q}{\partial \beta_1} = -2 \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)x_i = 0
  \label{eq:normbeta1}
\end{equation}

From Equation \eqref{eq:normbeta0},

\[\sum_{i = 1}^n Y_i - n \hat\beta_0 - \hat\beta_1 \sum_{i = 1}^n x_i = 0\]

Thus,

\[\hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x}\]

Equation \eqref{eq:normbeta1} gives

\[\sum_{i = 1}^n x_i (Y_i - \overline{Y} + \hat\beta_1\overline{x} - \hat\beta_1 x_i) = \sum_{i = 1}^n x_i(Y_i - \overline{Y}) - \hat\beta_1\sum_{i = 1}^n x_i (x_i - \overline{x}) = 0\]

Thus,

\[\hat\beta_1 = \frac{\sum\limits_{i = 1}^nx_i(Y_i - \overline{Y})}{\sum\limits_{i = 1}^n x_i (x_i - \overline{x})}\]

\BeginKnitrBlock{remark}
\iffalse{} {Remark. } \fi{}\[\hat\beta_1 = \frac{S_{XY}}{S_{XX}}\]

where \(S_{XX} := \sum\limits_{i = 1}^n (x_i - \overline{x})^2\) and \(S_{XY} := \sum\limits_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y})\)
\EndKnitrBlock{remark}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Note that \(\overline{x}^2 = \frac{1}{n^2}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2\). Then we have

\begin{equation}
  \begin{split}
    S_{XX} & = \sum_{i = 1}^n (x_i - \overline{x})^2 \\
    & = \sum_{i = 1}^n x_i^2 - 2\sum_{i = 1}^n x_i \overline{x} + \sum_{i = 1}^n\overline{x}^2 \\
    & = \sum_{i = 1}^n x_i^2 - \frac{2}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 + \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 \\
    & = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2
  \end{split}
  \label{eq:sxx}
\end{equation}

It follows that

\begin{equation*}
  \begin{split}
    \hat\beta_1 & = \frac{\sum x_i(Y_i - \overline{Y})}{\sum x_i (x_i - \overline{x})} \\
    & = \frac{\sum x_i (Y_i - \overline{Y}) - \overline{x}\sum (Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \qquad \because \sum (Y_i - \overline{Y}) = 0 \\
    & = \frac{\sum (x_i - \overline{x})(Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \\
    & = \frac{S_{XY}}{S_{XX}}
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{data =}\NormalTok{ delv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x, data = delv)

Coefficients:
(Intercept)            x  
       3.32         2.18  
\end{verbatim}

\hypertarget{prediction-and-mean-response}{%
\subsection{Prediction and Mean response}\label{prediction-and-mean-response}}

\begin{quote}
``Essentially, all models are wrong, but some are useful.''

---George Box
\end{quote}

Recall that we have assumed the \textbf{linear assumption} between the predictor and the response variables, i.e.~the true model. Estimating \(\beta_0\) and \(\beta_1\) is same as estimating the \emph{assumed true model}.

\BeginKnitrBlock{definition}[Mean response]
\protect\hypertarget{def:eyx}{}{\label{def:eyx} \iffalse (Mean response) \fi{} }\[E(Y \mid X = x) = \beta_0 + \beta_1 x\]
\EndKnitrBlock{definition}

We can estimate this mean resonse by

\begin{equation}
  \widehat{E(Y \mid x)} = \hat\beta_0 + \hat\beta_1 x
  \label{eq:meanres}
\end{equation}

However, in practice, the model might not be true, which is included in \(\epsilon\) term.

\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]

Our real problem is predicting individual \(Y\), not the mean. The \emph{prediction} of response can be done by

\begin{equation}
  \hat{Y_i}  = \hat\beta_0 + \hat\beta_1 x_i
  \label{eq:indpred}
\end{equation}

Observe that the values of Equations \eqref{eq:meanres} and \eqref{eq:indpred} are same. However, due to the \textbf{error term in the prediction}, it has larger standard error.

\hypertarget{lseprop}{%
\subsection{Properties of LSE}\label{lseprop}}

Parameters \(\beta_0\) and \(\beta_1\) have some properties related to the expectation and variance. We can notice that these lse's are \textbf{unbiased linear estimator}. In fact, these are the \emph{best unbiased linear estimator}. This will be covered in the Gauss-Markov theorem.

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:sxy}{}{\label{lem:sxy} }\[S_{XX} = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 = \sum_{i = 1}^n x_i(x_i - \overline{x})\]

\[S_{XY} = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg) = \sum_{i = 1}^n Y_i(x_i - \overline{x})\]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}We already proven the first part of \(S_{XX}\). See the Equation \eqref{eq:sxx}. The second part is tivial. Since \(\sum (x_i - \overline{x}) = 0\),

\[S_{XX} = \sum_{i = 1}^n (x_i - \overline{x})^2 = \sum_{i = 1}^n (x_i - \overline{x})x_i\]

For the first part of \(S_{XY}\),

\begin{equation*}
  \begin{split}
    S_{XY} & = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    & = \sum_{i = 1}^n x_i Y_i - \overline{x} \sum_{i = 1}^n Y_i - \overline{Y} \sum_{i = 1}^n x_i + n \overline{x} \overline{Y} \\
    & = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg)
  \end{split}
\end{equation*}

Second part of \(S_{XY}\) also can be proven from the definition.

\begin{equation*}
  \begin{split}
    S_{XY} & = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    & = \sum_{i = 1}^n Y_i (x_i - \overline{x}) - \overline{Y} \sum_{i = 1}^n (x_i - \overline{x}) \\
    & = \sum_{i = 1}^n Y_i (x_i - \overline{x}) \qquad \because \sum_{i = 1}^n (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\BeginKnitrBlock{lemma}[Linearity]
\protect\hypertarget{lem:linbet}{}{\label{lem:linbet} \iffalse (Linearity) \fi{} }Each coefficient is a linear estimator.

\[\hat\beta_1 = \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i\]

\[\hat\beta_0 = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})}{S_{XX}} \bigg) Y_i\]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}From lemma \ref{lem:sxy},

\begin{equation*}
  \begin{split}
    \hat\beta_1 & = \frac{S_{XY}}{S_{XX}} \\
    & = \frac{1}{S_{XX}}\sum_{i = 1}^n (x_i - \overline{x}) Y_i
  \end{split}
\end{equation*}

It gives that

\begin{equation*}
  \begin{split}
    \hat\beta_0 & = \overline{Y} - \hat\beta_1 \overline{x} \\
    & = \frac{1}{n}\sum_{i = 1}^n Y_i - \overline{x} \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i \\
    & = \sum_{i = 1}^n\bigg(\frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)Y_i
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\BeginKnitrBlock{proposition}[Unbiasedness]
\protect\hypertarget{prp:ue}{}{\label{prp:ue} \iffalse (Unbiasedness) \fi{} }Both coefficients are unbiased.

\(\text{(a)}\: E\hat\beta_1 = \beta_1\)

\(\text{(b)}\: E\hat\beta_0 = \beta_0\)
\EndKnitrBlock{proposition}

From the model, \(Y_1, \ldots, Y_n \stackrel{indep}{\sim} (\beta_0 + \beta_1 x_i, \sigma^2)\).

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}From lemma \ref{lem:sxy},

\begin{equation*}
  \begin{split}
    E\hat\beta_1 & = \sum_{i = 1}^n \bigg[ \frac{(x_i - \overline{x})}{S_{XX}} E(Y_i) \bigg] \\
    & = \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}(\beta_0 + \beta_1 x_i) \\
    & = \frac{\beta_1 \sum (x_i - \overline{x})x_i}{\sum (x_i - \overline{x})x_i} \qquad \because \sum (x_i - \overline{x}) = 0 \\
    & = \beta_1
  \end{split}
\end{equation*}

It follows that

\begin{equation*}
  \begin{split}
    E\hat\beta_0 & = E(\overline{Y} - \hat\beta_1 \overline{x}) \\
    & = E(\overline{Y}) - \overline{x}E(\hat\beta_1) \\
    & = E(\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) - \beta_1 \overline{x} \\
    & = \beta_0 + \beta_1 \overline{x} - \beta_1 \overline{x} \\
    & = \beta_0
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\BeginKnitrBlock{proposition}[Variances]
\protect\hypertarget{prp:vb}{}{\label{prp:vb} \iffalse (Variances) \fi{} }Variances and covariance of coefficients

\(\text{(a)}\: Var\hat\beta_1 = \frac{\sigma^2}{S_{XX}}\)

\(\text{(b)}\: Var\hat\beta_0 = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2\)

\(\text{(c)}\: Cov(\hat\beta_0, \hat\beta_1) = - \frac{\overline{x}}{S_{XX}} \sigma^2\)
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Proving is just arithmetic.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
\end{enumerate}

\begin{equation*}
  \begin{split}
    Var\hat\beta_1 & = \frac{1}{S_{XX}^2}\sum_{i = 1}^n \bigg[ (x_i - \overline{x})^2 Var(Y_i) \bigg] + \frac{1}{S_{XX}^2} \sum_{j \neq k}^n \bigg[ (x_j - \overline{x})(x_k - \overline{x}) Cov(Y_j, Y_k) \bigg] \\
    & = \frac{\sigma^2}{S_{XX}} \qquad \because Cov(Y_j, Y_k) = 0 \: \text{if} \: j \neq k
  \end{split}
\end{equation*}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\item
\end{enumerate}

\begin{equation*}
  \begin{split}
    Var\hat\beta_0 & = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)^2Var(Y_i) + \sum_{j \neq k} \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\bigg( \frac{1}{n} - \frac{(x_k - \overline{x})\overline{x}}{S_{XX}} \bigg) Cov(Y_j, Y_k) \\
    & = \frac{\sigma^2}{n} - 2 \sigma^2 \frac{\overline{x}}{S_{XX}} \sum_{i = 1}^n (x_i - \overline{x}) + \frac{\sigma^2 \overline{x}^2 \sum (x_i - \overline{x})^2}{S_{XX}^2} \\
    & = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \sigma^2 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\item
\end{enumerate}

\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, \hat\beta_1) & = Cov(\overline{Y} - \hat\beta_1 \overline{x}, \hat\beta_1) \\
    & = - \overline{x} Var\hat\beta_1 \\
    & = - \frac{\overline{x}}{S_{XX}} \sigma^2
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\hypertarget{gauss-markov-theorem}{%
\subsection{Gauss-Markov Theorem}\label{gauss-markov-theorem}}

Chapter \ref{lseprop} shows that the \(\beta_0^{LSE}\) and \(\beta_1^{LSE}\) are the \textbf{linear unbiased estimators}. Are these good? Good compared to \emph{what estimators}? Here we consider \emph{linear unbiased estimator}. If variances in the proposition \ref{prp:vb} are lower than any parameters in this parameter family, \(\beta_0^{LSE}\) and \(\beta_1^{LSE}\) are the \textbf{best linear unbiased estimators}.

\BeginKnitrBlock{theorem}[Gauss Markov Theorem]
\protect\hypertarget{thm:gmt}{}{\label{thm:gmt} \iffalse (Gauss Markov Theorem) \fi{} }\(\hat\beta_0\) and \(\hat\beta_1\) are BLUE, i.e.~the best linear unbiased estimator.

\[Var(\hat\beta_0) \le Var\Big( \sum_{i = 1}^n a_i Y_i \Big) \: \forall a_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n a_i Y_i \Big) = \beta_0\]

\[Var(\hat\beta_1) \le Var\Big( \sum_{i = 1}^n b_i Y_i \Big) \: \forall b_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n b_i Y_i \Big) = \beta_1\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{proof}[Bestness of $\beta_1$]
\iffalse{} {Proof (Bestness of \(\beta_1\)). } \fi{}Consider \(\Theta := \bigg\{ \sum\limits_{i = 1}^n b_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n b_i Y_i \Big) = \beta_1 \bigg\}\).

Claim: \(Var( \sum b_i Y_i) - Var(\hat\beta_1) \ge 0\)

Let \(\sum b_i Y_i \in \Theta\). Then \(E(\sum b_i Y_i) = \beta_1\).

Since \(E(Y_i) = \beta_0 + \beta_1 x_i\),

\[\beta_0 \sum b_i + \beta_1 \sum b_i x_i = \beta_1\]

It gives

\begin{equation} \label{eq:ule}
  \begin{cases}
    \sum b_i = 0 \\
    \sum b_i x_i = 1
  \end{cases}
\end{equation}

Then

\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum b_i Y_i - \hat\beta_1\Big) & = Var\Big( \sum b_i Y_i - \sum \frac{(x_i - \bar{x})}{S_{XX}} Y_i \Big) \\
    & \stackrel{indep}{=} \sum \bigg( b_i - \frac{(x_i - \bar{x})}{S_{XX}} \bigg)^2 \sigma^2 \\
    & = \sum \bigg( b_i^2 - \frac{2b_i (x_i - \bar{x})}{S_{XX}} + \frac{(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \sigma^2 \\
    & = \sum b_i^2 \sigma^2 - \frac{2 \sigma^2}{S_{XX}} \sum b_i x_i + \frac{2 \bar{x} \sigma^2}{S_{XX}} \sum b_i + \sigma^2 \frac{\sum (x_i - \bar{x})^2}{S_{XX}^2} \\
    & = \sum b_i^2 \sigma^2 - \frac{\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule} \:\text{and}\: S_{XX} = \sum (x_i - \bar{x})^2 \\
    & = Var(\sum b_i Y_i) - Var(\hat\beta_1)
  \end{split}
\end{equation*}

Hence,

\[Var(\sum b_i Y_i) \ge Var(\hat\beta_1)\]
\EndKnitrBlock{proof}

\BeginKnitrBlock{proof}[Bestness of $\beta_0$]
\iffalse{} {Proof (Bestness of \(\beta_0\)). } \fi{}Consider \(\Theta := \bigg\{ \sum\limits_{i = 1}^n a_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n a_i Y_i \Big) = \beta_0 \bigg\}\).

Claim: \(Var( \sum a_i Y_i) - Var(\hat\beta_0) \ge 0\)

Let \(\sum a_i Y_i \in \Theta\). Then \(E(\sum a_i Y_i) = \beta_0\).

Since \(E(Y_i) = \beta_0 + \beta_1 x_i\),

\[\beta_0 \sum a_i + \beta_1 \sum a_i x_i = \beta_0\]

It gives

\begin{equation} \label{eq:ule0}
  \begin{cases}
    \sum a_i = 1 \\
    \sum a_i x_i = 0
  \end{cases}
\end{equation}

Then

\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum a_iY_i - \hat\beta_0 \Big) & = Var\bigg[\sum a_iY_i - \sum\Big( \frac{1}{n} - \frac{(x_k - \bar{x})\bar{x}}{S_{XX}} \Big) Y_k \bigg] \\
    & = \sum \bigg(a_i - \frac{1}{n} +  \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \bigg)^2\sigma^2 \\
    & = \sum \bigg[ a_i^2 - 2a_i\Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big) + \Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big)^2 \bigg]\sigma^2 \\
        & = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n}\sum a_i + \frac{2\bar{x}\sigma^2\sum a_ix_i}{S_{XX}} - \frac{2\bar{x}^2\sigma^2\sum a_i}{S_{XX}} \\
        & \qquad + \sigma^2\bigg( \frac{1}{n} - \frac{2\bar{x}}{nS_{XX}} \sum(x_i - \bar{x}) + \frac{\bar{x}^2\sum(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \\
        & = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n} - \frac{2\bar{x}^2\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule0} \\
        & \qquad + \bigg(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \qquad \because \sum(x_i - \bar{x}) = 0 \: \text{and} \: S_{XX} := \sum (x_i - \bar{x})^2 \\
        & = \sum a_i^2\sigma^2 - \bigg( \frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \\
        & = Var\Big( \sum a_i Y_i \Big) - Var\hat\beta_0
  \end{split}
\end{equation*}

Hence,

\[Var(\sum a_i Y_i) \ge Var(\hat\beta_0)\]
\EndKnitrBlock{proof}

\BeginKnitrBlock{example}
\protect\hypertarget{exm:usingnormal}{}{\label{exm:usingnormal} }Show that \(\sum (Y_i - \hat{Y_i}) = 0\), \(\sum x_i (Y_i - \hat{Y_i}) = 0\), and \(\sum \hat{Y_i} (Y_i - \hat{Y_i}) = 0\).
\EndKnitrBlock{example}

\BeginKnitrBlock{solution}
\iffalse{} {Solution. } \fi{}Consider the two normal equations \eqref{eq:normbeta0} and \eqref{eq:normbeta1}. Note that \(\hat{Y_i} = \hat\beta_0 + \hat\beta_1 x_i\).

From the Equation \eqref{eq:normbeta0}, we have \(\sum (Y_i - \hat{Y_i}) = 0\).

From the Equation \eqref{eq:normbeta1}, we have \(\sum x_i (Y_i - \hat{Y_i}) = 0\).

It follows that

\begin{equation*}
  \begin{split}
    \sum \hat{Y_i} (Y_i - \hat{Y_i}) & = \sum (\hat\beta_0 + \hat\beta_1 x_i) (Y_i - \hat{Y_i}) \\
    & = \hat\beta_0 \sum (Y_i - \hat{Y_i}) + \hat\beta_1 \sum x_i (Y_i - \hat{Y_i}) \\
    & = 0
  \end{split}
\end{equation*}
\EndKnitrBlock{solution}

\hypertarget{estimation-of-sigma2}{%
\subsection{\texorpdfstring{Estimation of \(\sigma^2\)}{Estimation of \textbackslash sigma\^{}2}}\label{estimation-of-sigma2}}

There is the last parameter, \(\sigma^2 = Var(Y_i)\). In the \emph{least squares estimation literary}, we estimate \(\sigma^2\) by

\begin{equation} \label{eq:siglse}
  \hat\sigma^2 = \frac{1}{n - 2} \sum_{i = 1}^n (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2
\end{equation}

Why \(n - 2\)? This makes the estimator unbiased.

\BeginKnitrBlock{proposition}[Unbiasedness]
\protect\hypertarget{prp:sigex}{}{\label{prp:sigex} \iffalse (Unbiasedness) \fi{} }\[E(\hat\sigma^2) = \sigma^2\]
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Note that

\[(Y_i - \hat\beta_0 - \hat\beta_1 x_i) = (Y_i - \overline{Y}) - \hat\beta_1(x_i - \overline{x})\]

Then

\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) & = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2 \bigg] \\
    & = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \overline{Y})^2 + \hat\beta_1^2 \sum (x_i - \overline{x})^2 -2\hat\beta_1 \sum (Y_i - \overline{Y})(x_i - \overline{x}) \bigg] \\
    & = \frac{1}{n - 2} E ( S_{YY} + \hat\beta_1^2 S_{XX} - 2 \hat\beta_1 S_{XY}) \\
    & = \frac{1}{n - 2} E ( S_{YY} - \hat\beta_1^2 S_{XX}) \qquad \because S_{XY} = \hat\beta_1 S_{XX} \\
    & = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{XX} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big)
  \end{split}
\end{equation*}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
\end{enumerate}

\begin{equation*}
  \begin{split}
    ES_{YY} & = E\Big[ \sum (Y_i - \overline{Y})^2 \Big] \\
    & = E \Big[ \sum \Big( (\beta_0 + \beta_1 x_i + \epsilon_i) - (\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) \Big)^2 \Big] \\
    & = E \Big[ \sum \Big( \beta_1 (x_i - \overline{x}) + (\epsilon_i - \overline{\epsilon}) \Big)^2 \Big] \\
    & = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) + 2\beta_1 \sum (x_i - \overline{x}) E(\epsilon_i - \overline{\epsilon}) \\
    & = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big)
  \end{split}
\end{equation*}

Since \(E(\bar\epsilon) = 0\) and \(Var(\bar\epsilon) = \frac{\sigma^2}{n}\),

\begin{equation*}
  \begin{split}
    E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) & = E \Big( \sum (\epsilon_i^2 + \bar\epsilon^2 - 2\epsilon_i \bar\epsilon) \Big) \\
    & = \sum E(\epsilon_i^2) - n E(\bar\epsilon^2) \qquad \because \sum \epsilon = n \bar\epsilon \\
    & = \sum (Var(\epsilon_i) + E(\epsilon_i)^2) - n(Var(\bar\epsilon) + E(\bar\epsilon)^2) \\
    & = n\sigma^2 - \sigma^2 \\
    & = (n - 1)\sigma^2
  \end{split}
\end{equation*}

Thus,

\[ES_{YY} = \beta_1^2 S_{XX} + (n - 1)\sigma^2\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\item
\end{enumerate}

\begin{equation*}
  \begin{split}
    E\hat\beta_1^2 & = Var\hat\beta_1 + E(\hat\beta_1)^2 \\
    & = \frac{\sigma^2}{S_{XX}} + \beta_1^2
  \end{split}
\end{equation*}

It follows that

\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) & = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{YY} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big) \\
    & = \frac{1}{n - 2} \bigg( \Big(\beta_1^2 S_{XX} + (n - 1)\sigma^2 \Big) - S_{XX}\Big(\frac{\sigma^2}{S_{XX}} + \beta_1^2 \Big) \bigg) \\
    & = \frac{1}{n - 2}((n - 2)\sigma^2) \\
    & = \sigma^2
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\hypertarget{maximum-likelihood-estimation}{%
\section{Maximum Likelihood Estimation}\label{maximum-likelihood-estimation}}

In this section, we add an assumption to an random errors \(\epsilon_i\).

\[\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\]

\BeginKnitrBlock{example}[Gaussian Likelihood]
\protect\hypertarget{exm:gmle}{}{\label{exm:gmle} \iffalse (Gaussian Likelihood) \fi{} }Note that \(Y_i \stackrel{indep}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)\). Then the likelihood function is

\[L(\beta_0, \beta_1, \sigma^2) = \prod_{i = 1}^n\bigg( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \bigg(- \frac{(Y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2} \bigg) \bigg)\]

and so the log-likelihood function can be computed as

\[l(\beta_0, \beta_1, \sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2\]
\EndKnitrBlock{example}

\hypertarget{likelihood-equations}{%
\subsection{Likelihood equations}\label{likelihood-equations}}

\BeginKnitrBlock{definition}[Maximum Likelihood Estimator]
\protect\hypertarget{def:mledef}{}{\label{def:mledef} \iffalse (Maximum Likelihood Estimator) \fi{} }\[(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) := \arg\sup L(\beta_0, \beta_1, \sigma^2)\]
\EndKnitrBlock{definition}

Since \(l(\cdot) = \ln L(\cdot)\) is monotone,

\BeginKnitrBlock{remark}
\iffalse{} {Remark. } \fi{}\[(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) = \arg\sup l(\beta_0, \beta_1, \sigma^2)\]
\EndKnitrBlock{remark}

We can find the maximum of this \emph{quadratic} function by making first derivative.

\begin{equation}
  \frac{\partial l}{\partial \beta_0} = \frac{1}{\sigma^2} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i) = 0
  \label{eq:mlbeta0}
\end{equation}

\begin{equation}
  \frac{\partial l}{\partial \beta_1} = \frac{1}{\sigma^2} \sum_{i = 1}^n x_i (Y_i - \beta_0 - \beta_1 x_i) = 0
  \label{eq:mlbeta1}
\end{equation}

\begin{equation}
  \frac{\partial l}{\partial \sigma^2} = - \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = 0
  \label{eq:mlsig}
\end{equation}

Denote that Equations \eqref{eq:mlbeta0} and \eqref{eq:mlbeta1} given \(\hat\sigma^2\) are equivalent to the normal equations. Thus,

\[\hat\beta_0^{MLE} = \hat\beta_0^{LSE}, \quad \hat\beta_1^{MLE} = \hat\beta_1^{LSE}\]

From Equation \eqref{eq:mlsig},

\[\hat\sigma^{2MLE} = \frac{1}{n}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2 = \frac{n - 2}{n} \hat\sigma^{2LSE}\]

While \(\hat\sigma^{2LSE}\) is an unbiased, above \emph{MLE is not an unbiased estimator}. Since \(\hat\sigma^{2MLE} \approx \hat\sigma^{2LSE}\) for large \(n\), howerver, it is \emph{asymptotically unbiased}.

\BeginKnitrBlock{theorem}[Rao-Cramer Lower Bound, univariate case]
\protect\hypertarget{thm:rclb}{}{\label{thm:rclb} \iffalse (Rao-Cramer Lower Bound, univariate case) \fi{} }Let \(X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta)\). If \(\hat\theta\) is an unbiased estimator of \(\theta\),

\[Var(\hat\theta) \ge \frac{1}{I_n(\theta)}\]

where \(I_n(\theta) = -E\bigg(\frac{\partial^2 l(\theta)}{\partial \theta^2} \bigg)\)
\EndKnitrBlock{theorem}

To apply this theorem \ref{thm:rclb} in the simple linear regression setting, i.e.~\((\beta_0, \beta_1)\), we need to look at the \emph{bivariate case}.

\BeginKnitrBlock{theorem}[Rao-Cramer Lower Bound, bivariate case]
\protect\hypertarget{thm:rclb2}{}{\label{thm:rclb2} \iffalse (Rao-Cramer Lower Bound, bivariate case) \fi{} }Let \(X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta1, \theta_2)\) and let \(\boldsymbol{\theta} = (\theta_1, \theta_2)^T\). If each \(\hat\theta_1\), \(\hat\theta_2\) is an unbiased estimator of \(\theta_1\) and \(\theta_2\), then

\[
Var(\boldsymbol{\theta}) := \begin{bmatrix}
Var(\hat\theta_1) & Cov(\hat\theta_1, \hat\theta_2) \\
Cov(\hat\theta_1, \hat\theta_2) & Var(\hat\theta_2)
\end{bmatrix} \ge I_n^{-1}(\theta_1, \theta_2)
\]

where

\[
I_n(\theta_1, \theta_2) = - \begin{bmatrix}
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1^2} \bigg) & E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) \\
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) & E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_2^2} \bigg)
\end{bmatrix}
\]
\EndKnitrBlock{theorem}

Assume that \(\sigma^2\) is \textbf{known}. From the Equations \eqref{eq:mlbeta0} and \eqref{eq:mlbeta1},

\[
\begin{cases}
  \frac{\partial^2 l}{\partial \beta_0^2} = - \frac{n}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_1^2} = - \frac{\sum x_i^2}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_0 \partial \beta_1} = - \frac{\sum x_i}{\sigma^2}
\end{cases}
\]

Thus,

\[
I_n(\beta_0, \beta_1) = \begin{bmatrix}
  \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} \\
  \frac{\sum x_i}{\sigma^2} & \frac{\sum x_i^2}{\sigma^2}
\end{bmatrix}
\]

Applying gaussian elimination,

\begin{equation*}
  \begin{split}
    \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} & 1 & 0 \\
      \frac{\sum x_i}{\sigma^2} & \frac{\sum x_i^2}{\sigma^2} & 0 & 1
    \end{array}
    \right] & \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} & 1 & 0 \\
      \frac{\sum x_i}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) & \frac{\sum x_i^2}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) & 0 & \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    & \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} & 1 & 0 \\
      0 & \frac{\sum x_i^2 - \overline{x}\sum x_i}{\sigma^2\overline{x}} = \frac{S_{XX}}{\sigma^2\overline{x}} & -1 & \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    & \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 & \overline{x} & \frac{\sigma^2}{n} & 0 \\
      0 & 1 & -\frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right] \\
    & \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 & 0 & \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 & -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
      0 & 1 & -\frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right]
  \end{split}
\end{equation*}

Hence,

\[
I_n^{-1}(\beta_0, \beta_1) = \begin{bmatrix}
  \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 & -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
  -\frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
\end{bmatrix} = \begin{bmatrix}
  Var(\hat\beta_0) & Cov(\hat\beta_0, \hat\beta_1) \\
  Cov(\hat\beta_0, \hat\beta_1) & Var(\hat\beta_1)
\end{bmatrix}
\]

Since \(Var(\boldsymbol{\hat\beta}) - I^{-1} = 0\) is non-negative definite, each \(Var(\hat\beta_0) = \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2\) and \(Var(\hat\beta_1) = \frac{\sigma^2}{S_{XX}}\) is a theoretical bound.

\BeginKnitrBlock{remark}
\iffalse{} {Remark. } \fi{}This says that \(\hat\beta_0^{LSE} = \hat\beta_0^{MLE}\) and \(\hat\beta_1^{LSE} = \hat\beta_1^{MLE}\) have the smallest variance among all unbiased estimator.
\EndKnitrBlock{remark}

This result is \emph{stronger than Gauss-Markov theorem} \ref{thm:gmt}, where the LSE has the smalleset variance among all \emph{linear unbiased} estimators. It can be simply obtained from the \emph{Lehmann-Scheffe Theorem}: If some unbiased estimator is a function of complete sufficient statistic, then this estimator is the unique MVUE \citep{Hogg:2018aa}.

\BeginKnitrBlock{remark}[Lehmann and Scheffe for regression coefficients]
\iffalse{} {Remark (Lehmann and Scheffe for regression coefficients). } \fi{}\(u\Big(\sum Y_i, S_{XY} \Big)\) is CSS in this regression problem, i.e.~known \(\sigma^2\).
\EndKnitrBlock{remark}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}From the example \ref{exm:gmle},

\begin{equation*}
  \begin{split}
    L(\beta_0, \beta_1) & = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum(Y_i - \beta_0 - \beta_1 x_i)^2 \bigg] \\
    & = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum \Big(Y_i^2 - (\beta_0 + \beta_1 x_i)Y_i + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg] \\
    & = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \Big( -\beta_0 \sum Y_i - \beta_1 \sum x_i Y_i \Big) \bigg] \exp\bigg[-\frac{1}{2\sigma^2} \Big( \sum Y_i^2 + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg]
  \end{split}
\end{equation*}

By the Factorization theorem, both \(\sum Y_i\) and \(\sum x_i Y_i\) are sufficient statistics. Since \(S_{XY}\) is one-to-one function of \(\sum x_i Y_i\), it is also a sufficient statistic.

Denote that the normal distribution is in exponential family.

Hence, \((\sum Y_i, S_{XY})\) are CSS.
\EndKnitrBlock{proof}

\hypertarget{residuals}{%
\section{Residuals}\label{residuals}}

\BeginKnitrBlock{definition}[Residuals]
\protect\hypertarget{def:res}{}{\label{def:res} \iffalse (Residuals) \fi{} }\[e_i := Y_i - \hat{Y_i}\]
\EndKnitrBlock{definition}

\hypertarget{prediction-error}{%
\subsection{Prediction error}\label{prediction-error}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delv }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{yhat =} \KeywordTok{predict}\NormalTok{(}\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x))) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_linerange}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin =}\NormalTok{ y, }\DataTypeTok{ymax =}\NormalTok{ yhat), }\DataTypeTok{col =} \KeywordTok{I}\NormalTok{(}\StringTok{"red"}\NormalTok{), }\DataTypeTok{alpha =} \FloatTok{.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Number of Cases"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Delivery Time"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/regplot-1} 

}

\caption{Fit and residuals}\label{fig:regplot}
\end{figure}

See Figure \ref{fig:regplot}. Each red line is \(e_i\). As we can see, \(e_i\) represents the difference between \emph{observed} response and \emph{predicted} response. A large \(\lvert e_i \rvert\) indicates a large prediction error. You can call this \(e_i\) for each \(Y_i\) by \texttt{lm()\$residuals} or \texttt{residuals()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delv_fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(y }\OperatorTok{~}\StringTok{ }\NormalTok{x, }\DataTypeTok{data =}\NormalTok{ delv)}
\NormalTok{delv_fit}\OperatorTok{$}\NormalTok{residuals}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     1      2      3      4      5      6      7      8      9     10 
-1.874  1.651  2.181  2.855 -2.628 -0.444  0.327 -0.724 10.634  7.298 
    11     12     13     14     15     16     17     18     19     20 
 2.191 -4.082  1.475  3.372  1.094  3.918 -1.028  0.446 -0.349 -5.216 
    21     22     23     24     25 
-7.182 -7.581 -4.156 -0.900 -1.275 
\end{verbatim}

\(\sum e_i^2\), which has been minimized in the procedure of LSE, can be used to see \emph{overall size of prediction errors}.

\BeginKnitrBlock{definition}[Residual Sum of Squares]
\protect\hypertarget{def:sse}{}{\label{def:sse} \iffalse (Residual Sum of Squares) \fi{} }\[SSE := \sum_{i = 1}^n e_i^2\]
\EndKnitrBlock{definition}

\hypertarget{residuals-and-the-variance}{%
\subsection{Residuals and the variance}\label{residuals-and-the-variance}}

\(e_i\) is a random quantity, which contains the information for \(\epsilon_i\). \(\sum e_i^2\) can give information about \(\sigma^2 = Var(\epsilon_i)\). For this, it is expected that \(e_i\) and \(\epsilon_i\) have similar feature.

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:yandbet}{}{\label{lem:yandbet} }Covriance between Y and each coefficient

\(\text{(a)}\: Cov(\hat\beta_0, Y_i) = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2\)

\(\text{(b)}\: Cov(\hat\beta_1, Y_i) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2\)
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}(a)

\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, Y_i) & = Cov(\sum a_i Y_i, Y_i) \\
    & = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2
  \end{split}
\end{equation*}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\item
\end{enumerate}

\begin{equation*}
  \begin{split}
    Cov(\hat\beta_1, Y_i) & = Cov(\sum b_i Y_i, Y_i) \\
    & = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\BeginKnitrBlock{proposition}[Properties of residuals]
\protect\hypertarget{prp:resprop}{}{\label{prp:resprop} \iffalse (Properties of residuals) \fi{} }Mean and variance of the residual

\(\text{(a)}\: E(e_i) = 0\)

\(\text{(b)}\: Var(e_i) \neq \sigma^2\)

\(\text{(c)}\: \forall i \neq j : Cov(e_i, e_j) \neq 0\)
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}(a) Recall that this is the assumption of the regression model.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Lemma \ref{lem:yandbet} implies that
\end{enumerate}

\begin{equation*}
  \begin{split}
    Cov(\overline{Y}, \hat\beta_1) & = Cov(\frac{1}{n}\sum Y_i, \hat\beta_1) \\
    & = \frac{1}{n} \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    & = 0 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}

Then

\begin{equation}
  \begin{split}
    Var(\hat{Y_i}) & = Var(\hat\beta_0 + \hat\beta_1 x_i) \\
    & = Var \bigg[ \overline{Y} + (x_i - \overline{x}) \hat\beta_1 \bigg] \qquad \because \hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x} \\
    & = Var(\overline{Y}) + (x_i - \overline{x})^2 Var(\hat\beta_1) + 2(x_i - \overline{x}) Cov(\overline{Y}, \hat\beta_1) \\
    & = \frac{\sigma^2}{n} + (x_i - \overline{x})^2\frac{\sigma^2}{S_{XX}} + 0 \\
    & = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
  \label{eq:predvar}
\end{equation}

From the same lemma \ref{lem:yandbet},

\begin{equation}
  \begin{split}
    Cov(Y_i, \hat{Y_i}) & = Cov(Y_i, \overline{Y} + (x_i - \overline{x}) \hat\beta_1) \\
    & = Cov(Y_i, \overline{Y}) + (x_i - \overline{x}) Cov(Y_i, \hat\beta_1) \\
    & = \frac{\sigma^2}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}}\sigma^2 \qquad \because Cov(Y_i, \hat\beta_1) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    & = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
  \label{eq:yyhat}
\end{equation}

These Equations \eqref{eq:predvar} and \eqref{eq:yyhat} give that

\begin{equation}
  \begin{split}
    Var(e_i) & = Var(Y_i) + Var(\hat{Y_i}) -2Cov(Y_i, \hat{Y_i}) \\
    & = \sigma^2 + \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 - 2 \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    & = \bigg(1 - \frac{1}{n} - \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    & \neq \sigma^2
  \end{split}
  \label{eq:residvar}
\end{equation}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Let \(i \neq j\). Then
\end{enumerate}

\begin{equation*}
  \begin{split}
    Cov(e_i, e_j) & = Cov\Big( Y_i - (\hat\beta_0 + \hat\beta_1 x_i), Y_j - (\hat\beta_0 + \hat\beta_1 x_j) \Big) \\
    & = Cov(Y_i, Y_j) - Cov\Big(Y_i, (\hat\beta_0 + \hat\beta_1 x_j) \Big) - Cov((\hat\beta_0 + \hat\beta_1 x_i), Y_j) + Cov((\hat\beta_0 + \hat\beta_1 x_i), (\hat\beta_0 + \hat\beta_1 x_j)) \\
    & = 0 - \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_j}{S_{XX}}\sigma^2 \\
    & \qquad - \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_i}{S_{XX}}\sigma^2 \\
    & \qquad + \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    & = - \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    & = - \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})(x_j - \overline{x})}{S_{XX}} \bigg)\sigma^2 \\
    & \neq 0
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\hypertarget{decomposition-of-total-variability}{%
\section{Decomposition of Total Variability}\label{decomposition-of-total-variability}}

\hypertarget{total-sum-of-squares}{%
\subsection{Total sum of squares}\label{total-sum-of-squares}}

\BeginKnitrBlock{definition}[Uncorrected Total Sum of Squares]
\protect\hypertarget{def:unsst}{}{\label{def:unsst} \iffalse (Uncorrected Total Sum of Squares) \fi{} }\[SST_{uncor} := \sum_{i = 1}^n Y_i^2\]
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Corrected Total Sum of Squares]
\protect\hypertarget{def:sst}{}{\label{def:sst} \iffalse (Corrected Total Sum of Squares) \fi{} }\[SST := \sum_{i = 1}^n (Y_i - \overline{Y})^2\]
\EndKnitrBlock{definition}

What does this total sum of squares mean? To know this, we should know \(\overline{Y}\) first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delv }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{formula =}\NormalTok{ y }\OperatorTok{~}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Number of Cases"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Delivery Time"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/ybarpred-1} 

}

\caption{Regression without predictor}\label{fig:ybarpred}
\end{figure}

See Figure \ref{fig:ybarpred}. The line represents the closest line when we use only intercept term for the regression model. In other words, \emph{if we use no information for the response}, i.e.~no predictor variables, we will get just average of the response variable. Consider

\[Y_i = \beta_0 + \epsilon_i\]

Then we can get only one normal equation

\[\sum (Y_i - \hat\beta_0) = 0\]

Hence,

\[\hat\beta_0 = \frac{1}{n} \sum_{i = 1}^n Y_i \equiv \overline{Y}\]

From this fact, \(SST\) implies \textbf{total variance}.

\hypertarget{regression-sum-of-squares}{%
\subsection{Regression sum of squares}\label{regression-sum-of-squares}}

\BeginKnitrBlock{definition}[Regression Sum of Squares]
\protect\hypertarget{def:ssr}{}{\label{def:ssr} \iffalse (Regression Sum of Squares) \fi{} }\[SSR := \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2\]
\EndKnitrBlock{definition}

This \(SSR\) compares \(\hat{Y_i}\) versus \(\overline{Y}\), computing the sum of squares for difference between predicted values from \emph{regression model} and \emph{model not using predictors}.

\hypertarget{residual-sum-of-squares}{%
\subsection{Residual sum of squares}\label{residual-sum-of-squares}}

Now consider the \emph{residual sum of squares} \(SSE\) in the definition \ref{def:sse}. As mentioned, this is related to the \emph{prediction errors}, which the regression model could not explain the data.

\hypertarget{decompsst}{%
\subsection{Decomposition of total sum of squares}\label{decompsst}}

\(SST\) can be decomposed by construction of sum of squares.

\BeginKnitrBlock{proposition}[Decomposition of SST]
\protect\hypertarget{prp:decom}{}{\label{prp:decom} \iffalse (Decomposition of SST) \fi{} }\[SST = SSR + SSE\]

where \(SST = \sum (Y_i - \overline{Y})^2\), \(SSR = \sum (\hat{Y_i} - \overline{Y})^2\), and \(SSE = \sum (Y_i - \hat{Y_i})\)
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}From the Example \ref{exm:usingnormal},

\begin{equation*}
  \begin{split}
    \sum_{i = 1}^n (Y_i - \overline{Y})^2 & = \sum_{i = 1}^n (Y_i - \hat{Y_i} + \hat{Y_i} - \overline{Y})^2 \\
    & = \sum_{i = 1}^n (Y_i - \hat{Y_i})^2 + 2 \sum_{i = 1}^n (Y_i - \hat{Y_i})(\hat{Y_i} - \overline{Y}) + \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2 \\
    & = \sum_{i = 1}^n (Y_i - \hat{Y_i})^2 + \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2 \qquad \because \sum (Y_i - \hat{Y_i}) = 0 \: \text{and} \: \sum (Y_i - \hat{Y_i})\hat{Y_i} = 0
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

This represents each \(SSR\) and \(SSE\) divides total variability as following.

\[\overset{SST}{\text{total variability}} = \overset{SSR}{\text{explained by regression}} + \overset{SSE}{\text{left unexplained by regression}}\]

Denote that the total variability \(SST\) is \emph{constant given data set}. If our model is good, \(SSR\) grows and \(SSE\) flattens. Thus the larger \(SSR\) is, the better. The lower \(SSE\) is, the better.

\hypertarget{coefficient-of-determination}{%
\subsection{Coefficient of determination}\label{coefficient-of-determination}}

We have discussed in the previous section \ref{decompsst} that \(SSR\) and \(SSE\) splits the total variability into \emph{explained part and not-explained part by our regression model}. Our first interest is whether the model works well for the data well, so we can think about the \emph{proportion of explained part to the total variance}. The following measure \(R^2\) computes this kind of value.

\BeginKnitrBlock{definition}[Coefficient of Determination]
\protect\hypertarget{def:rsq}{}{\label{def:rsq} \iffalse (Coefficient of Determination) \fi{} }\[R^2 := \frac{SSR}{SST} = 1 - \frac{1 - SSE}{SST}\]
\EndKnitrBlock{definition}

By construction,

\[0 \le R^2 \le 1\]

As \(R^2\) goes to \(0\), the model goes wrong. As \(R^2\) is close to \(1\), large proportion of variability has been explained. So we prefer large values rather than small.

\BeginKnitrBlock{proposition}
\protect\hypertarget{prp:rsqlin}{}{\label{prp:rsqlin} }\(R^2\) shows the strength of linear relation between two variables \(x\) and \(Y\) in the simple linear regression.

\[R^2 = \hat\rho_{XY}\]

where \(\hat\rho_{XY} := \frac{\sum (X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum (X_i - \overline{X})^2} \sqrt{\sum (Y_i - \overline{Y})^2}}\) is the sample correlation coefficients
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Note that \(\hat{Y_i} - \overline{Y} = \hat\beta_1 (x_i - \overline{x}) = \frac{S_{XY}}{S_{XX}} (x_i - \overline{x})\). Then

\begin{equation*}
  \begin{split}
    \sum (\hat{Y_i} - \overline{Y})^2 & = \frac{S_{XY}^2}{S_{XX}^2} \sum (x_i - \overline{x})^2 \\
    & = \frac{S_{XY}^2}{S_{XX}}
  \end{split}
\end{equation*}

It follows that

\begin{equation*}
  \begin{split}
    R^2 & = \frac{\sum (\hat{Y_i} - \overline{Y})^2}{\sum (Y_i - \overline{Y})^2} \\
    & = \frac{S_{XY}^2}{S_{XX}S_{YY}} \\
    & =: \hat\rho_{XY}^2
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

In this relation, we can know that \(R^2\) statistic performs as a measure of the linear relationship in the simple linear regression setting.

\hypertarget{matnot}{%
\section{Geometric Interpretations}\label{matnot}}

\hypertarget{fundamental-subspaces}{%
\subsection{Fundamental subspaces}\label{fundamental-subspaces}}

These linear algebra concepts might be more useful for \emph{multiple linear regression}, but let's briefly recap \citep{Leon:2014aa}.

\BeginKnitrBlock{definition}[Fundamental Subspaces]
\protect\hypertarget{def:subspace}{}{\label{def:subspace} \iffalse (Fundamental Subspaces) \fi{} }Let \(X \in \mathbb{R}^{n \times (p + 1)}\).

Then the Null space is defined by

\[N(X) := \{ \mathbf{b} \in \mathbb{R}^n \mid X\mathbf{b} = \mathbf{0} \}\]

The Row space is defined by

\[Row(X) := sp(\{\mathbf{r}_1, \ldots, \mathbf{r}_{p + 1} \}) \quad \text{where} \: X^T = [\mathbf{r}_1^T, \ldots, \mathbf{r}_{n}^T]\]

The Column space is defined by

\[Col(X) := sp(\{\mathbf{c}_1, \ldots, \mathbf{c}_{n} \}) \quad \text{where} \: X = [\mathbf{c}_1, \ldots, \mathbf{c}_{p + 1}]\]

The Range of \(X\) is defined by

\[R(X) := \{ \mathbf{y} \in \mathbb{R}^n \mid \mathbf{y} = X\mathbf{b} \quad \text{for some} \: \mathbf{b} \in \mathbb{R}^{p + 1} \}\]
\EndKnitrBlock{definition}

These spaces have some constructional relationship.

\BeginKnitrBlock{theorem}[Fundamental Subspaces Theorem]
\protect\hypertarget{thm:fundsub}{}{\label{thm:fundsub} \iffalse (Fundamental Subspaces Theorem) \fi{} }Let \(X \in \mathbb{R}^{n \times (p + 1)}\). Then

\[N(X) = R(X^T)^{\perp} = Col(X^T)^{\perp} = Row(X)^{\perp}\]

Transposed matrix also satisfy this.

\[N(X^T) = R(X)^{\perp} = Col(X)^{\perp}\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Let \(\mathbf{a} \in N(X)\). Then \(X\mathbf{a} = \mathbf{0}\).

Let \(\mathbf{y} \in R(X^T)\). Then \(X^T \mathbf{b} = \mathbf{y}\) for some \(\mathbf{b} \in \mathbb{R}^{p + 1}\).

Choose \(\mathbf{b} \in \mathbb{R}^{p + 1}\) such that \(X^T \mathbf{b} = \mathbf{y}\). Then

\begin{equation*}
  \begin{split}
    \mathbf{0} & = X\mathbf{a} \\
    & = \mathbf{b}^T X\mathbf{a} \\
    & = \mathbf{y}^T \mathbf{a}
  \end{split}
\end{equation*}

Hence,

\[N(X) \perp R(X^T)\]

Since

\[X^T \mathbf{b} = \mathbf{c}_1 \mathbf{b} + \cdots + \mathbf{c}_{p + 1} \mathbf{b}\]

it is trivial that \(R(X) = Col(X)\) and \(R(X^T) = Col(X^T)\).

If \(\mathbf{a} \in N(X)\), then

\[
X\mathbf{a} = \begin{bmatrix}
  \mathbf{r}_1 \\
  \mathbf{r}_2 \\
  \cdots \\
  \mathbf{r}_n
\end{bmatrix} \begin{bmatrix}
  a_1 \\
  \cdots \\
  a_{p + 1}
\end{bmatrix} = \begin{bmatrix}
  0 \\
  0 \\
  \cdots \\
  0
\end{bmatrix}
\]

Thus,

\[\forall i :  \mathbf{a}^T \mathbf{r}_i = 0\]

and so

\[N(X) \subseteq Row(X)^{\perp}\]

Conversely, if \(\mathbf{a} \in Row(X)^{\perp}\), then \(\forall i : \mathbf{a}^T \mathbf{r}_i = 0\). This implies that \(X\mathbf{a} = \mathbf{0}\). Thus,

\[Row(X)^{\perp} \subseteq N(X)\]

and so

\[N(X) = Row(X)^{\perp}\]
\EndKnitrBlock{proof}

\(N(X^T) = R(X)^{\perp}\) part in Theorem \ref{thm:fundsub} will give the geometric insight to \emph{least squares solution}.

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:perpbasis}{}{\label{thm:perpbasis} }Let \(S\) be a subspace of \(\mathbb{R}^n\). Then

\[dim S + dim S^{\perp} = n\]

If \(\{ \mathbf{x}_1, \ldots, \mathbf{x}_r \}\) is a basis for \(S\) and \(\{ \mathbf{x}_{r + 1}, \ldots, \mathbf{x}_n \}\) is a basis for \(S^{\perp}\), then \(\{ \mathbf{x}_1, \ldots, \mathbf{x}_r, \mathbf{x}_{r + 1}, \ldots, \mathbf{x}_n \}\) is a basis for \(\mathbb{R}^n\).
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:dsum}{}{\label{thm:dsum} }Let \(S\) be a subspace of \(\R^n\). Then

\[\R^n = S \oplus S^{\perp}\]
\EndKnitrBlock{theorem}

\hypertarget{simple-linear-regression}{%
\subsection{Simple linear regression}\label{simple-linear-regression}}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:projection}{}{\label{thm:projection} }Let \(S\) be a subspace of \(\mathbb{R}^n\). For each \(\mathbf{y} \in \mathbf{R}^n\), there exists a unique \(\mathbf{p} \in S\) that is closest to \(\mathbf{y}\), i.e.

\[\Vert \mathbf{y} - \mathbf{p}  \Vert > \Vert \mathbf{y} - \mathbf{\hat{y}} \Vert\]

for any \(\mathbf{p} \neq \mathbf{\hat{y}}\). Furthermore, a given vector \(\mathbf{p} \in S\) will be the closest to a given vector \(\mathbf{y} \in \mathbb{R}^n\) if and only if

\[\mathbf{y} - \mathbf{\hat{y}} \in S^{\perp}\]
\EndKnitrBlock{theorem}

Least square estimator \((\hat\beta_0, \hat\beta_1)^T\) minimizes

\begin{equation}
  \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = \Vert \mathbf{Y} - (\beta_0 \mathbf{1} + \beta_1 \mathbf{x}) \Vert^2
  \label{eq:qmatrix}
\end{equation}

with respect to \((\hat\beta_0, \hat\beta_1)^T \in \mathbb{R}^2\) (where \(\mathbf{1} := (1, 1)^T\)). Recall that the normal equation gives

\[\sum_{i = 1}^n(Y_i - \hat\beta_0 - \hat\beta_1 x_i) = \Big( \mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \Big)^T \mathbf{1} = 0\]

and

\[\sum_{i = 1}^n (Y_i - \hat\beta_0 - \hat\beta_1 x_i)x_i = \Big( \mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \Big)^T \mathbf{x} = 0\]

These two relation give

\[\mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \perp sp(\{ \mathbf{1}, \mathbf{x} \})^{\perp}\]

i.e.~\(\mathbf{\hat{Y}} = \hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}\) is the projection of \(\mathbf{Y}\).

Theorem \ref{thm:projection} can give the same result.

\[\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x} \in R([\mathbf{1}, \mathbf{x}])^{\perp} = sp(\{ \mathbf{1}, \mathbf{x} \})^{\perp}\]

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/simpledraw-1} 

}

\caption{Geometric Illustration of Simple Linear Regression}\label{fig:simpledraw}
\end{figure}

We can see the details from Figure \ref{fig:simpledraw}. In fact, decomposition of \(SST\) and \(R^2\) are also in here.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/simpledraw2-1} 

}

\caption{Geometric Illustration of Decomposing SST}\label{fig:simpledraw2}
\end{figure}

See Figure \ref{fig:simpledraw2}.

\[
\begin{cases}
  SST = \lVert \mathbf{Y} - \overline{Y} \mathbf{1} \rVert^2 \\
  SSR = \lVert \mathbf{\hat{Y}} - \overline{Y} \mathbf{1} \rVert^2 \\
  SSE = \lVert \mathbf{Y} - \mathbf{\hat{Y}} \rVert^2
\end{cases}
\]

Pythagorean law implies that

\[SST = SSR + SSE\]

Also,

\[R^2 = \frac{SSR}{SST} = cos^2\theta = \hat\rho_{XY}^2\]

\hypertarget{solproj}{%
\subsection{Projection mapping}\label{solproj}}

Look again Figure \ref{fig:simpledraw}. Let \(X \equiv [\mathbf{1}, \mathbf{x}] \in \R^{n \times 2}\) and let \(\boldsymbol\beta \equiv (\beta_0, \beta_1)^T\). By the fundamental subspaces theorem \ref{thm:fundsub},

\[\mathbf{Y} - X\boldsymbol{\hat\beta} \in Col(X)^{\perp} = N(X^T)\]

Thus,

\begin{equation}
  X^T(\mathbf{Y} - X\boldsymbol{\hat\beta}) = \mathbf{0}
  \label{eq:projeq}
\end{equation}

This is the another representation of normal equation. Then we now have

\begin{equation*}
  \begin{split}
    & X^T\mathbf{Y} - X^TX\boldsymbol{\hat\beta} = \mathbf{0} \\
    & \Leftrightarrow X^T\mathbf{Y} = X^TX\boldsymbol{\hat\beta}
  \end{split}
\end{equation*}

If \(X^TX\) is nonsingular,

\[\boldsymbol{\hat\beta} = (X^TX)^{-1}X^T \mathbf{Y}\]

It follows that

\[\mathbf{\hat{Y}} = X\boldsymbol{\hat\beta} = X(X^TX)^{-1}X^T \mathbf{Y}\]

Combining this equation and our figure, we can know that \(X(X^TX)^{-1}X^T\) projects \(\mathbf{Y}\) from \(\R^n\) onto \(Col(X) = R(X)\). This is called projection operator/mapping.

\BeginKnitrBlock{definition}[Projection matrix]
\protect\hypertarget{def:projop}{}{\label{def:projop} \iffalse (Projection matrix) \fi{} }Projection operator or mapping from \(\R^n\) to \(W\) is written by

\[\Pi(\cdot \mid W) := X(X^TX)^{-1}X^T\]
\EndKnitrBlock{definition}

As mentioned, \(X^TX\) should be invertible to get the LSE solution.

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:fullrank}{}{\label{thm:fullrank} }Let \(\mathbf{Y} = X\boldsymbol\beta\) inconsistent and let \(X \in \R^{n \times (p + 1)}\) with \(n > p + 1\).

If \(rank(X) = p + 1\), i.e.~full rank, then \(X^T X\) is invertible.
\EndKnitrBlock{theorem}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Suppose that \((X^TX)\mathbf{b} = \mathbf{0}\). Then

\[X^T (X\mathbf{b}) = \mathbf{0}\]

By the fundamental subspaces theorem \ref{thm:fundsub},

\[X\mathbf{b} \in N(X^T) = Col(X)^{\perp}\]

By construction,

\[X\mathbf{b} \in Col(X) = N(X^T)^{\perp}\]

Then

\[X\mathbf{b} \in N(X^T) \cap N(X^T)^{\perp} = \{ \mathbf{0} \}\]

It follows that

\[X\mathbf{b} = \mathbf{0}\]

If \(rank(X) = n\), then the linear equation system has trivial solution \(\mathbf{b} = \mathbf{0}\) and so does \(X^T (X\mathbf{b}) = \mathbf{0}\). Hence, \(X^T X\) is invertible.
\EndKnitrBlock{proof}

Using projection matrix \(\Pi_W\), we can re-express each sum of squares. Recall that when we only use \(y_i\) for regression fitting, the result becomes its average. It is because \(\mathbf{Y}\) vector has been projected onto \(sp(\{ \mathbf{1} \})\) line.

\BeginKnitrBlock{remark}
\iffalse{} {Remark. } \fi{}\[\overline{Y}\mathbf{1} = \mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T\mathbf{Y} = \Pi_{\mathbf{1}}\mathbf{Y}\]

\[\mathbf{\hat{Y}} = X(X^TX)^{-1}X^T \mathbf{Y} = \Pi_X \mathbf{Y}\]
\EndKnitrBlock{remark}

Intuitively, every projection matrix is idempotent and symmetric. Once projected, the result is same when projecting it again.

\BeginKnitrBlock{corollary}[Sum of squares]
\protect\hypertarget{cor:projss}{}{\label{cor:projss} \iffalse (Sum of squares) \fi{} }\(\Pi_{\mathbf{1}}\) and \(\Pi_X\) can express each \(SS\) as following.

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
\end{enumerate}

\[SST = \mathbf{Y}^T (I - \Pi_{\mathbf{1}}) \mathbf{Y}\]

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{1}
\item
\end{enumerate}

\[SSR = \mathbf{Y}^T (\Pi_X - \Pi_{\mathbf{1}}) \mathbf{Y}\]

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{2}
\item
\end{enumerate}

\[SSE = \mathbf{Y}^T (I - \Pi_X) \mathbf{Y}\]
\EndKnitrBlock{corollary}

\hypertarget{simpledist}{%
\section{Distributions}\label{simpledist}}

\hypertarget{mean-response-and-response}{%
\subsection{Mean response and response}\label{mean-response-and-response}}

We have already look at predicting each mean response and response from equation \eqref{eq:meanres} and \eqref{eq:indpred}.

\BeginKnitrBlock{theorem}[Estimation of the mean response]
\protect\hypertarget{thm:mux}{}{\label{thm:mux} \iffalse (Estimation of the mean response) \fi{} }\[\hat\mu_x \equiv \widehat{E(Y \mid x)} = \hat\beta_0 + \hat\beta_1 x\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}[(out of sample) Prediction of a response]
\protect\hypertarget{thm:yhatx}{}{\label{thm:yhatx} \iffalse ((out of sample) Prediction of a response) \fi{} }\[\hat{Y_x}  = \hat\beta_0 + \hat\beta_1 x\]
\EndKnitrBlock{theorem}

Recall that predicting \ref{thm:mux} targets at

\[\mu_x \equiv E(Y \mid x) = \beta_0 + \beta_1 x\]

which have been assumed to be true model. On the other hand, predicting \ref{thm:yhatx} targets at

\[Y = \beta_0 + \beta_1 + \epsilon_x\]

The linearity is not true in reality. So the errors caused by modeling linear model are included in \(\epsilon_x\). This error term makes difference between properties of \ref{thm:mux} and \ref{thm:yhatx}.

To derive their distribution and see the difference, we additionaly assume Normality, i.e.

\[\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\]

\hypertarget{simplebdist}{%
\subsection{Regression coefficients}\label{simplebdist}}

Under Normality, we have

\[Y_i \stackrel{indep}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)\]

Then

\[
\mathbf{Y} = \begin{bmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
\end{bmatrix} \sim MVN_n\Bigg( \boldsymbol\mu \equiv \begin{bmatrix}
  \beta_0 + \beta_1 x_1 \\
  \beta_0 + \beta_1 x_2 \\
  \vdots \\
  \beta_0 + \beta_1 x_n
\end{bmatrix}, \Sigma \equiv \sigma^2 I = \begin{bmatrix}
  \sigma^2 & 0 & \cdots & 0 \\
  0 & \sigma^2 & \cdots & 0 \\
  \vdots & \vdots & \vdots & \vdots \\
  0 & 0 & 0 & \sigma^2
\end{bmatrix} \Bigg)
\]

Write \(\boldsymbol{\hat\beta} = (\hat\beta_0, \hat\beta_1)^T\). From Lemma \ref{lem:linbet},

\[\hat\beta_0 = \mathbf{a}^T\mathbf{Y}\]

where \(\mathbf{a} = (a_1, \ldots, a_n)^T \in \mathbb{R}^n\) with \(a_i = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\)

and

\[\hat\beta_1 = \mathbf{b}^T\mathbf{Y}\]

where \(\mathbf{b} = (b_1, \ldots, b_n)^T \in \mathbb{R}^n\) with \(b_i = \frac{(x_i - \overline{x})}{S_{XX}}\).

Let

\[A^T = [ \mathbf{a}^T, \mathbf{b}^T ]\]

Then

\[
\boldsymbol{\hat\beta} = A\mathbf{Y}
\]

Linearity of the multivariate normal distribution, Proposition \ref{prp:ue} and \ref{prp:vb} imply that

\begin{equation}
  \boldsymbol{\hat\beta} = \begin{bmatrix}
    \hat\beta_0 \\ \hline
    \hat\beta_1
  \end{bmatrix} \sim MVN \bigg( A\boldsymbol\mu = \begin{bmatrix}
    \beta_0 \\ \hline
    \beta_1
  \end{bmatrix},
  A\Sigma A^T = \sigma^2 AA^T = \begin{bmatrix}
    \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 & - \frac{\overline{x}}{S_{XX}}\sigma^2 \\ \hline
    - \frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
  \end{bmatrix} \bigg)
  \label{eq:b01mvn}
\end{equation}

Since the joint random vector follows multivariate normal distribution, each \emph{partitioned subset follow normal}. For this theorem, see \citet{Johnson:2013aa}. Hence, we finally get the following result.

\BeginKnitrBlock{theorem}[Distributions of regression coefficients]
\protect\hypertarget{thm:b01dist}{}{\label{thm:b01dist} \iffalse (Distributions of regression coefficients) \fi{} }Each regression coefficient follows Normal distribution.

\[\hat\beta_0 \sim N \bigg( \beta_0, \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 \bigg)\]

\[\hat\beta_1 \sim N \bigg( \beta_1, \frac{\sigma^2}{S_{XX}} \bigg)\]
\EndKnitrBlock{theorem}

\hypertarget{mean-response}{%
\subsection{Mean response}\label{mean-response}}

In simple linear regression setting, we assume \(\mu_x = E(Y \mid x) = \beta_0 + \beta_1 x\) is true.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delv }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.7}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}
    \DataTypeTok{x =} \StringTok{"Number of Cases"}\NormalTok{,}
    \DataTypeTok{y =} \StringTok{"Delivery Time"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/smoothline-1} 

}

\caption{Mean response and its standard deviation}\label{fig:smoothline}
\end{figure}

For example, in the Figure \ref{fig:smoothline}, the blue line indicates \(E(Y \mid X = x)\) for each point \(x\). Without fitting using \texttt{lm()}, \texttt{geom\_smooth(method\ =\ "lm")} let us visualize the fitted line. Since the default method is not the linear regression, the \texttt{method} option should be specified.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{delv }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{eyx =} \KeywordTok{predict}\NormalTok{(delv_fit, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 25 x 3
       y     x   eyx
   <dbl> <dbl> <dbl>
 1  16.7     7 18.6 
 2  11.5     3  9.85
 3  12.0     3  9.85
 4  14.9     4 12.0 
 5  13.8     6 16.4 
 6  18.1     7 18.6 
 7   8       2  7.67
 8  17.8     7 18.6 
 9  79.2    30 68.6 
10  21.5     5 14.2 
# ... with 15 more rows
\end{verbatim}

We have already seen in section \ref{simplebdist} that the estimators \(\hat\beta_0\) and \(\hat\beta_1\) are random variables. So \(\hat\mu_x\) is. In fact, the ribbon of the line in Figure \ref{fig:smoothline} represents upper and lower confidence limits on mean response. In the later section, we get to know that it is \(+ t(n - 2)\widehat{SE}(\hat\mu_x)\) and \(- t(n - 2) \widehat{SE}(\hat\mu_x)\). It can be drawn by default with the option of the \texttt{geom\_smooth(se\ =\ TRUE)}.

\BeginKnitrBlock{theorem}[Distribution of mean response estimator]
\protect\hypertarget{thm:mrdist}{}{\label{thm:mrdist} \iffalse (Distribution of mean response estimator) \fi{} }\(\hat\mu_x\) is also Normally distributed.

\[\hat\mu_x \sim N\bigg( \mu_x, \sigma^2\bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Since \(\hat\mu_x = \hat\beta_0 + \hat\beta_1 x\) is the linear combination of \((\hat\beta_0, \hat\beta_1)^T\),

\[\hat\mu_x \sim N\Big(E(\hat\mu_x), Var(\hat\mu_x)\Big)\]

From Theorem \ref{thm:b01dist},

\[E(\hat\mu_x) = E(\hat\beta_0) + E(\hat\beta_1)x = \beta_0 + \beta_1x \equiv \mu_x\]

and from Proposition \ref{prp:vb}

\begin{equation*}
  \begin{split}
    Var(\hat\mu_x) & = Var(\hat\beta_0 + \hat\beta_1 x) \\
    & = Var(\hat\beta_0) + x^2Var(\hat\beta_1) + 2xCov(\hat\beta_0, \hat\beta_1) \\
    & = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 + \frac{x^2\sigma^2}{S_{XX}} - \frac{2\overline{x}x\sigma^2}{S_{XX}} \\
    & = \sigma^2\bigg(\frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\BeginKnitrBlock{corollary}
\protect\hypertarget{cor:mrdiff}{}{\label{cor:mrdiff} }\[\hat\mu_x - \mu_x \sim N\bigg( 0, \sigma^2\bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)\]
\EndKnitrBlock{corollary}

Denote that in both Theorem \ref{thm:mrdist} and Corollary \ref{cor:mrdiff}, \(\sigma^2\) is parameter. So to use \(SE(\hat\mu_x) = \sqrt{Var(\hat\mu_x)}\) in practice we plug in its estimator, usually Equation \eqref{eq:siglse}.

\BeginKnitrBlock{corollary}[Standard error of mean response estimator]
\protect\hypertarget{cor:mrse}{}{\label{cor:mrse} \iffalse (Standard error of mean response estimator) \fi{} }\[\widehat{SE}(\hat\mu_x) = \hat\sigma^2\bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)\]

where \(\hat\sigma^2 = MSE\)
\EndKnitrBlock{corollary}

\hypertarget{response}{%
\subsection{Response}\label{response}}

Our goal is to predict each response at each point, i.e.~\(Y_x = \beta_0 + \beta_1 x + \epsilon_x\). \(\epsilon_x \sim N(0, \sigma^2)\) is independent of the given data (\(\epsilon_1, \ldots, \epsilon_n\)). In this sense, this prediction is called \emph{out of sample prediction}. This setting makes difference between the \emph{residuals, which are correlated to the data}. See Proposition \ref{prp:resprop} for this. This is occurred because each \(\hat\beta_0\) and \(\hat\beta_1\) is linear combination of \(Y_1, \ldots, Y_n\), not \(Y_x\).

While \(Cov(Y_i, \hat{Y_i}) > 0, i = 1, \ldots, n\) (See Equation \eqref{eq:yyhat}), in case of out-of-sample \(Y_x\),

\[Cov(Y_x, \hat{Y_x}) = Cov(Y_x, \hat\beta_0 + \hat\beta_1 x) = 0\]

Hence, arithmetically, this \emph{out of sample prediction becomes to have larger standard error}.

\BeginKnitrBlock{proposition}[Joint distribution of coefficients and error term]
\protect\hypertarget{prp:bepsmvn}{}{\label{prp:bepsmvn} \iffalse (Joint distribution of coefficients and error term) \fi{} }\((\hat\beta_0, \hat\beta_1, \epsilon_x)^T\) is Normally distributed.
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Want 1: \((\hat\beta_0, \hat\beta_1)^T \perp\!\!\!\perp \epsilon_x\)

We have

\begin{equation}
  \begin{split}
    Cov((\hat\beta_0, \hat\beta_1)^T, \epsilon_x) & = \Big[Cov(\hat\beta_i, \epsilon_x) \Big]_{2 \times 1} \\
    & = \bigg[Cov\bigg(\sum_{i = 1}^n k_i Y_i, \epsilon_x \bigg) \bigg]_{2 \times 1} \qquad k_i = \text{each linear coefficient for}\: \hat\beta_0, \hat\beta_1 \\
    & = \mathbf{0}
  \end{split}
  \label{eq:betaepsind}
\end{equation}

From Equation \eqref{eq:b01mvn},

\[(\hat\beta_0, \hat\beta_1)^T \sim MVN\]

and from assumption,

\[\epsilon_x \sim N(0, \sigma^2)\]

It follows from Equation \eqref{eq:betaepsind} that (\citet{Johnson:2013aa})

\[(\hat\beta_0, \hat\beta_1)^T \perp\!\!\!\perp \epsilon_x\]

Want 2: \((\hat\beta_0, \hat\beta_1, \epsilon_x)^T \sim MVN\)

From independency, we have (\citet{Johnson:2013aa})

\[
\begin{bmatrix}
  \hat\beta_0 \\
  \hat\beta_1 \\ \hline
  \epsilon_x
\end{bmatrix} \sim MVN_{2 + 1} \bigg( \begin{bmatrix}
  \beta_0 \\
  \beta_1 \\ \hline
  0
\end{bmatrix}, \left[
  \begin{array}{c|c}
    Cov(\boldsymbol{\hat\beta}) \in \mathbb{R}^{2 \times 2} & \mathbf{0} \in \mathbb{R}^2 \\ \hline
    \mathbf{0}^T \in \mathbb{R}^{2 \times 1} & \sigma^2
  \end{array}
\right] \bigg)
\]
\EndKnitrBlock{proof}

This proposition gives clue to distribution of prediction error.

\BeginKnitrBlock{theorem}[Distribution of out-of-sample prediction error]
\protect\hypertarget{thm:preddist}{}{\label{thm:preddist} \iffalse (Distribution of out-of-sample prediction error) \fi{} }Out of sample prediction error \(\hat{Y_x} - Y_x\) is Normally distributed

\[\hat{Y_x} - Y_x \sim N\bigg( 0, \sigma^2 \bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Note that

\begin{equation*}
  \begin{split}
    \hat{Y_x} - Y_x & = (\hat\beta_0 + \hat\beta_1 x) - (\beta_0 + \beta_1 x + \epsilon_x) \\
    & = [1, x, -1] (\hat\beta_0, \hat\beta_1, \epsilon_x)^T - \beta_0 - \beta_1 x
  \end{split}
\end{equation*}

i.e.~\(\hat{Y_x} - Y_x\) is a linear combination of \((\hat\beta_0, \hat\beta_1, \epsilon_x)^T\). From prosition \ref{prp:bepsmvn},

\begin{equation}
  \begin{split}
    \hat{Y_x} - Y_x & \sim MVN \Bigg( [1, x, -1]\begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    0
    \end{bmatrix} - \beta_0 - \beta_1 x,
    [1, x, -1]
    \left[
      \begin{array}{c|c}
        Cov(\boldsymbol{\hat\beta}) \in \mathbb{R}^{2 \times 2} & \mathbf{0} \in \mathbb{R}^2 \\ \hline
        \mathbf{0}^T \in \mathbb{R}^{2 \times 1} & \sigma^2
      \end{array}
    \right]
    \begin{bmatrix}
      1 \\
      x \\
      -1
    \end{bmatrix}
     \Bigg) \\
    & \stackrel{d}{=} MVN \bigg( 0, \sigma^2\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} - 2 \frac{\overline{x}x}{S_{XX}} + \frac{x^2}{S_{XX}} \bigg) + 1 \bigg) \\
    & \stackrel{d}{=} MVN \bigg( 0, \sigma^2\bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)
  \end{split}
  \label{eq:prederrmvn}
\end{equation}
\EndKnitrBlock{proof}

Now we know the standard error of this out-of-sample prediction error.

\[SE(\hat{Y_x} - Y_x) = \sigma^2\bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)\]

We can see this standard error is \emph{always larger than of mean response estimator} due to \(1\) in the bracket, i.e.~\(\sigma^2\). As mentioned, this is due to \(\epsilon\) term. When we estimate or predict the mean response the model have been assumed to be true. In this out-of-sample prediction setting, however, the model can be wrong. This assumption error is also included in \(\epsilon\) term and it is called \emph{irreducible error}, which cannot be reduced anymore.

\BeginKnitrBlock{remark}
\iffalse{} {Remark. } \fi{}\[SE(\hat\mu_x - \mu_x) < SE(\hat{Y_x} - Y_x)\]
\EndKnitrBlock{remark}

It might be more clear if we see the inequality in the above remark. We know the fact that \(\hat{Y_x}\) and \(Y_x\) are uncorrelated in this out-of-sample setting. \(Y_x\) is random variable, while \(\mu_x\) is constant. Then we can re-express the inequality as

\[SE(\hat\mu_x) < SE(\hat{Y_x}) + SE(Y_x)\]

Actually, both \(\hat\mu_x\) and \(\hat{Y_x}\) are estimated as \(\hat\beta_0 + \hat\beta_1 x\). Thus, \(SE(Y_x) = \sigma^2\) makes out-of-sample more noisy.

To use standard error practically, we use \(\hat\sigma^2\) as in corollary \ref{cor:mrse}.

\BeginKnitrBlock{corollary}[Standard error of out-of-sample prediction error]
\protect\hypertarget{cor:predse}{}{\label{cor:predse} \iffalse (Standard error of out-of-sample prediction error) \fi{} }\[\widehat{SE}(\hat{Y_x} - Y_x) = \hat\sigma^2\bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)\]

where \(\hat\sigma^2 = MSE\)
\EndKnitrBlock{corollary}

\hypertarget{statistical-inference}{%
\section{Statistical Inference}\label{statistical-inference}}

Based on each distribution of estimator in section \ref{simpledist}, we can construct various inferece for each

\begin{itemize}
\tightlist
\item
  \(\beta_0\)
\item
  \(\beta_1\)
\item
  \(\mu_x\)
\item
  \(Y_x\)
\item
  \(\sigma^2\)
\end{itemize}

We can get the standard error for each coefficient through \texttt{summary()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(delv_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x, data = delv)

Residuals:
   Min     1Q Median     3Q    Max 
-7.581 -1.874 -0.349  2.181 10.634 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)    3.321      1.371    2.42    0.024 *  
x              2.176      0.124   17.55  8.2e-15 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.18 on 23 degrees of freedom
Multiple R-squared:  0.93,  Adjusted R-squared:  0.927 
F-statistic:  308 on 1 and 23 DF,  p-value: 8.22e-15
\end{verbatim}

Or more state-or-art way, \texttt{broom:tidy()} function has a method for each model object to make tidy data: \texttt{tibble}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(delv_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 5
  term        estimate std.error statistic  p.value
  <chr>          <dbl>     <dbl>     <dbl>    <dbl>
1 (Intercept)     3.32     1.37       2.42 2.37e- 2
2 x               2.18     0.124     17.5  8.22e-15
\end{verbatim}

\hypertarget{confidence-interval}{%
\subsection{Confidence interval}\label{confidence-interval}}

Consider standardization.

\[\frac{\hat\theta - \theta}{\sqrt{SE(\hat\theta)}}\]

Each \(SE\) includes \(\sigma^2\) as we have already seen. First think about \textbf{known} \(\sigma^2\) setting. All three estimators follow Normal distribution, and \(SE\) is constant by our the setting. Then we can construct each confidence interval as

\[\hat\theta \pm z_{\frac{\alpha}{2}} \sqrt{SE(\hat\theta)}\]

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/estci-1} 

}

\caption{Confidence Interval when $\sigma^2$ is known}\label{fig:estci}
\end{figure}

Now just plug in the results of section \ref{simpledist}. For each regression coefficient,

\BeginKnitrBlock{proposition}[Confidence intervals on $\beta$]
\protect\hypertarget{prp:betaci}{}{\label{prp:betaci} \iffalse (Confidence intervals on \(\beta\)) \fi{} }With known \(\sigma^2\), \((1 - \alpha)100 \%\) confidence intervals on \(\beta_0\) and \(\beta_1\) are given as

\[\beta_0 : \quad \hat\beta_0 \pm z_{\frac{\alpha}{2}} \sqrt{\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \sigma^2}\]

\[\beta_1 : \quad \hat\beta_1 \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma^2}{S_{XX}}}\]
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proposition}[Confidence interval on $\hat\mu_x$]
\protect\hypertarget{prp:mrci}{}{\label{prp:mrci} \iffalse (Confidence interval on \(\hat\mu_x\)) \fi{} }With known \(\sigma^2\), \((1 - \alpha)100 \%\) confidence interval on \(\hat\mu_x\) is given as

\[\mu_x : \quad \hat\mu_x \pm z_{\frac{\alpha}{2}} \sqrt{\sigma^2 \bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}\]
\EndKnitrBlock{proposition}

In practice, however, we do not know \(\sigma^2\). In this case, we replace \(\sigma^2\) with \(\hat\sigma^2 = \frac{1}{n - 2}\sum\limits_{i = 1}^n (\hat{Y_i} - Y_i)^2 = MSE\). Then

\[\frac{\hat\theta - \theta}{\sqrt{\widehat{SE}}} = \frac{\frac{\hat\theta - \theta}{\sqrt{SE = \sigma^2(\cdot)}}}{\sqrt{\frac{\frac{SSE}{\sigma^2}}{n - 2}\bigg( \cdot \bigg)}} =  \frac{\frac{\hat\theta - \theta}{\sqrt{SE = \sigma^2}} \sim N(0, 1)}{\sqrt{\frac{\frac{SSE}{\sigma^2} \sim \chi^2(n - 2)}{n - 2}}} \sim t(n - 2)\]

Thus, we need to replace \(z_{\frac{\alpha}{2}}\) with \(t_{\frac{\alpha}{2}}(n - 2)\).

\BeginKnitrBlock{proposition}[Confidence intervals on $\beta$ when unknown $\sigma^2$]
\protect\hypertarget{prp:betaci2}{}{\label{prp:betaci2} \iffalse (Confidence intervals on \(\beta\) when unknown \(\sigma^2\)) \fi{} }With unknown \(\sigma^2\), \((1 - \alpha)100 \%\) confidence intervals on \(\beta_0\) and \(\beta_1\) are given as

\[\beta_0 : \quad \hat\beta_0 \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \hat\sigma^2}\]

\[\beta_1 : \quad \hat\beta_1 \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\frac{\hat\sigma^2}{S_{XX}}}\]

where \(\hat\sigma^2 = MSE\)
\EndKnitrBlock{proposition}

Here we can estimate the intervals. Basically, \texttt{confint()} function gives this interval.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(delv_fit, }\DataTypeTok{level =} \FloatTok{.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            2.5 % 97.5 %
(Intercept) 0.484   6.16
x           1.920   2.43
\end{verbatim}

\BeginKnitrBlock{proposition}[Confidence interval on $\hat\mu_x$ when unknown $\sigma^2$]
\protect\hypertarget{prp:mrci2}{}{\label{prp:mrci2} \iffalse (Confidence interval on \(\hat\mu_x\) when unknown \(\sigma^2\)) \fi{} }With unknown \(\sigma^2\), \((1 - \alpha)100 \%\) confidence interval on \(\hat\mu_x\) is given as

\[\mu_x : \quad \hat\mu_x \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\hat\sigma^2 \bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}\]

where \(\hat\sigma^2 = MSE\)
\EndKnitrBlock{proposition}

\texttt{predict()} provides options for this confidence interval. Specify \texttt{interval\ =\ "confidence"}. This argument has three option.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{"none"}: just compute fitted value, by default.
\item
  \texttt{"confidence"}: confidence interval of mean response
\item
  \texttt{"prediction"}: prediction interval of out-of-sample prediction
\end{enumerate}

Default \texttt{level} is \texttt{0.95}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(delv_fit, }\DataTypeTok{interval =} \StringTok{"confidence"}\NormalTok{, }\DataTypeTok{level =} \FloatTok{.95}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 25 x 3
     fit   lwr   upr
   <dbl> <dbl> <dbl>
 1 18.6  16.8   20.3
 2  9.85  7.57  12.1
 3  9.85  7.57  12.1
 4 12.0   9.91  14.1
 5 16.4  14.5   18.2
 6 18.6  16.8   20.3
 7  7.67  5.22  10.1
 8 18.6  16.8   20.3
 9 68.6  62.9   74.3
10 14.2  12.2   16.2
# ... with 15 more rows
\end{verbatim}

\hypertarget{prediction-interval}{%
\subsection{Prediction interval}\label{prediction-interval}}

One proceeds in a similar way for out-of-sample \(Y_x\).

\BeginKnitrBlock{proposition}[Prediction interval on $\hat{Y_x}$]
\protect\hypertarget{prp:predci}{}{\label{prp:predci} \iffalse (Prediction interval on \(\hat{Y_x}\)) \fi{} }With known \(\sigma^2\), \((1 - \alpha)100 \%\) confidence interval on \(\hat\mu_x\) is given as

\[Y_x : \quad \hat{Y_x} \pm z_{\frac{\alpha}{2}} \sqrt{\sigma^2 \bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}\]
\EndKnitrBlock{proposition}

Also, with unknown \(\sigma^2\),

\BeginKnitrBlock{proposition}[Prediction interval on $\hat{Y_x}$ when unknown $\sigma^2$]
\protect\hypertarget{prp:predci2}{}{\label{prp:predci2} \iffalse (Prediction interval on \(\hat{Y_x}\) when unknown \(\sigma^2\)) \fi{} }With unknown \(\sigma^2\), \((1 - \alpha)100 \%\) confidence interval on \(\hat\mu_x\) is given as

\[Y_x : \quad \hat{Y_x} \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\hat\sigma^2 \bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}\]

where \(\hat\sigma^2 = MSE\)
\EndKnitrBlock{proposition}

Since this is out-of-sample setting, we should also give \texttt{newdata} option. Otherwise, we will get warning message. Denote that this argument only receive \texttt{data.frame} object with same element names.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(delv_fit, }\DataTypeTok{newdata =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =} \DecValTok{31}\OperatorTok{:}\DecValTok{35}\NormalTok{), }\DataTypeTok{interval =} \StringTok{"prediction"}\NormalTok{, }\DataTypeTok{level =} \FloatTok{.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   fit  lwr  upr
1 70.8 60.3 81.3
2 73.0 62.3 83.6
3 75.1 64.3 85.9
4 77.3 66.4 88.3
5 79.5 68.4 90.6
\end{verbatim}

\hypertarget{hypothesis-testing}{%
\subsection{Hypothesis testing}\label{hypothesis-testing}}

Look again the output of \texttt{summary.lm()} and \texttt{broom::tidy.lm()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(delv_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x, data = delv)

Residuals:
   Min     1Q Median     3Q    Max 
-7.581 -1.874 -0.349  2.181 10.634 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)    3.321      1.371    2.42    0.024 *  
x              2.176      0.124   17.55  8.2e-15 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.18 on 23 degrees of freedom
Multiple R-squared:  0.93,  Adjusted R-squared:  0.927 
F-statistic:  308 on 1 and 23 DF,  p-value: 8.22e-15
\end{verbatim}

We can see \texttt{t\ value} and \texttt{Pr(\textgreater{}\textbar{}t\textbar{})}. At the same time, \texttt{statistic} and \texttt{p.value}. What are these values? These are the results of the following tests.

\[H_0 : \beta_0 = \alpha_0 \qquad \text{vs} \qquad H_1 : \beta_0 \neq \alpha_0\]

\begin{equation}
  T = \frac{\hat\beta_0 - \alpha_0}{\hat\sigma \sqrt{\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}} \bigg)}} \stackrel{H_0}{\sim} t(n - 2)
  \label{eq:b0test}
\end{equation}

For this test statistic \eqref{eq:b0test},

\[\text{reject}\: H_0 \quad \text{if} \: \lvert T \rvert > t_{\frac{\alpha}{2}}(n - 2)\]

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/b0rjt-1} 

}

\caption{Rejection region for $\beta_0$}\label{fig:b0rjt}
\end{figure}

More importantly, we test \(\beta_1\) which means slope

\[H_0 : \beta_1 = \alpha_1 \qquad \text{vs} \qquad H_1 : \beta_1 \neq \alpha_1\]

\begin{equation}
  T = \frac{\hat\beta_0 - \alpha_0}{\hat\sigma \sqrt{\frac{1}{S_{xx}}}} \stackrel{H_0}{\sim} t(n - 2)
  \label{eq:b1test}
\end{equation}

For this test statistic \eqref{eq:b1test},

\[\text{reject}\: H_0 \quad \text{if} \: \lvert T \rvert > t_{\frac{\alpha}{2}}(n - 2)\]

Looking at these two statistics, we can intuitively know the meaning. As \(\lvert \hat\beta_1 - \alpha_1 \rvert\) becomes larger, the data support \(H_1\).

\hypertarget{analysis-of-variance}{%
\section{Analysis of Variance}\label{analysis-of-variance}}

\hypertarget{useful-distributions}{%
\subsection{Useful distributions}\label{useful-distributions}}

In linear regression setting, we usually assume \(\epsilon_i \iid N(0, \sigma^2)\). There are some useful distributions around Normal.

\BeginKnitrBlock{proposition}[$\chi^2$-distribution]
\protect\hypertarget{prp:chisq}{}{\label{prp:chisq} \iffalse (\(\chi^2\)-distribution) \fi{} }Square of standard normal follows \(\chi^2\)-distribution.

If \(Z \sim N(0, 1)\), then \(Z^2 \sim \chi^2(1)\)

If \(Z_i \indep N(0, 1)\), then \(Z_1^2 + \cdots + Z_n^2 \sim \chi^2(n)\)
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proposition}[t-distribution]
\protect\hypertarget{prp:tdist}{}{\label{prp:tdist} \iffalse (t-distribution) \fi{} }Let \(Z \sim N(0, 1) \ind V \sim \chi^2(m)\). Then

\[T = \frac{Z}{\sqrt{V / m}} \sim t(m)\]
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proposition}[F-distribution]
\protect\hypertarget{prp:fdist}{}{\label{prp:fdist} \iffalse (F-distribution) \fi{} }Let \(V \sim \chi^2(m) \ind W \sim \chi^2(n)\). Then

\[F = \frac{V / m}{W / n} \sim F(m, n)\]
\EndKnitrBlock{proposition}

Also, there is \emph{non-central analogue} of these three distributions, i.e.~starting from \(Z \sim N(\mu, 1)\).

\BeginKnitrBlock{proposition}[Noncentral $\chi^2$-distribution]
\protect\hypertarget{prp:nonchi}{}{\label{prp:nonchi} \iffalse (Noncentral \(\chi^2\)-distribution) \fi{} }Square of scaled normal follows non-central \(\chi^2\)-distribution.

If \(Z_i \indep N(\mu_i, 1)\), then \(Z_1^2 + \cdots + Z_n^2 \sim \chi^2(n, \sum\limits_{i = 1}^n \mu_i^2)\)

\(\sum\limits_{i = 1}^n \mu_i^2\) is called a non-central parameter.
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proposition}[Noncentral t-distribution]
\protect\hypertarget{prp:nontdist}{}{\label{prp:nontdist} \iffalse (Noncentral t-distribution) \fi{} }Let \(X \sim N(\mu, 1) \ind V \sim \chi^2(m)\). Then

\[T = \frac{Z}{\sqrt{V / m}} \sim t(m, \mu)\]

\(\mu\) is called a non-central parameter.
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proposition}[Noncentral F-distribution]
\protect\hypertarget{prp:nonfdist}{}{\label{prp:nonfdist} \iffalse (Noncentral F-distribution) \fi{} }Let \(V \sim \chi^2(m, \delta) \ind W \sim \chi^2(n)\). Then

\[F = \frac{V / m}{W / n} \sim F(m, n, \delta)\]

\(\delta\) is called a non-central parameter.
\EndKnitrBlock{proposition}

\hypertarget{quadratic-form}{%
\subsection{Quadratic form}\label{quadratic-form}}

Now we can determine the distributions of various quadratic forms. The reason we are taking care of this is ANOVA deals with sum of squares, i.e.~quadratic form.

\BeginKnitrBlock{theorem}[Idempotent and symmetric]
\protect\hypertarget{thm:idem}{}{\label{thm:idem} \iffalse (Idempotent and symmetric) \fi{} }Let \(A \in \R^{k \times k}\) be idempotent and symmetric. Then

\(\text{(a)}\: A^n\) is also idempotent

\(\text{(b)}\: I - A\) is also idempotent

\(\text{(c)}\:\) Every eigenvalue of \(A\) is either \(0\) or \(1\) so that \(tr(A) = rank(A)\)
\EndKnitrBlock{theorem}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}(a) and (b) are trivial.

\[(A^n)^2 = (A^2)^n = A^n\]

\[(I - A)^2 = I - 2A + A^2 = I - A\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\item
\end{enumerate}

Fix \(\lambda\) an eigenvalue of \(A\). Let \(\mathbf{v} \neq \mathbf{0}\) be the corresponding eigenvector.

By definition,

\[A\mathbf{v} = \lambda \mathbf{v}\]

Then

\[A^2\mathbf{v} = \lambda(A\mathbf{v}) = \lambda^2\mathbf{v}\]

and so \(\lambda^2\) is eigenvalue of \(A^2\).

Since \(A^2 = A\),

\[\lambda = \lambda^2\]

Hence,

\[\lambda = 0 \:\text{or}\: 1\]

Note that for every matrix and its eigenvalues \(\lambda_j\)

\[tr(X) = \sum_{j = 1}^p \lambda_j, \quad rank(X) = \text{the number of non-zero}\: \lambda_j\]

Since \(\lambda = 0, 1\) of A,

\[tr(A) = rank(A)\]
\EndKnitrBlock{proof}

\BeginKnitrBlock{proposition}[Independence]
\protect\hypertarget{prp:quadmvn}{}{\label{prp:quadmvn} \iffalse (Independence) \fi{} }Assume \(\mathbf{Y} \sim MVN(\mathbf\mu, \Sigma)\). Then

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\tightlist
\item
  If \(A\) and \(B\) are symmetric,
\end{enumerate}

\[Y^T AY \ind Y^T BY \Leftrightarrow A\Sigma B = 0\]

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  If \(A\) is symmetric,
\end{enumerate}

\[Y^T AY \ind BY \Leftrightarrow B\Sigma A = 0\]
\EndKnitrBlock{proposition}

\BeginKnitrBlock{theorem}[Distribution of quadratic form]
\protect\hypertarget{thm:quaddist}{}{\label{thm:quaddist} \iffalse (Distribution of quadratic form) \fi{} }Assume that \(\mathbf{Y} \sim MVN(\mathbf\mu, \Sigma)\) and that \(A\) is symmetric and idempotent. Then

\[Y^T AT \sim \chi^2(K, \delta)\]

where \(K = rank(A)\) and \(\delta = \boldsymbol\mu^T A \boldsymbol\mu\). Furthermore,

\[
\begin{cases}
  E(Y^T AT) = K + \delta \\
  Var(Y^T AT) = 2(K + 2\delta)
\end{cases}
\]
\EndKnitrBlock{theorem}

\BeginKnitrBlock{corollary}[Inner product of standard normal vector]
\protect\hypertarget{cor:mvnchi}{}{\label{cor:mvnchi} \iffalse (Inner product of standard normal vector) \fi{} }Let \(\mathbf{Z} = (Z_1, \ldots, Z_n)^T \sim MVN(\mathbf{0}, I_n)\). Then

\[\mathbf{Z}^T\mathbf{Z} = \sum_{i = 1}^n Z_i^2 \sim \chi^2(n)\]
\EndKnitrBlock{corollary}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}From Theorem \ref{thm:quaddist} point of view,

\[\mathbf{Z}^T\mathbf{Z} = \mathbf{Z}^T I_n \mathbf{Z}\]

Thus,

\[K = rank(I_n) = n\]

\[\delta = \mathbf{0}\]
\EndKnitrBlock{proof}

Using the above facts, we can now show distributions of sums of squares. First recall that

\[\mathbf{Y} \sim MVN(X\boldsymbol\beta, \sigma^2 I)\]

\BeginKnitrBlock{proposition}[Distribution of SSE]
\protect\hypertarget{prp:ssedist}{}{\label{prp:ssedist} \iffalse (Distribution of SSE) \fi{} }\[\frac{SSE}{\sigma^2} \sim \chi^2(n - 2, 0)\]
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}From Corollary \ref{cor:projss}, write

\[\frac{SSE}{\sigma^2} = \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)^T (I - \Pi_X) \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)\]

Note that

\[\frac{\mathbf{Y}}{\sigma} \sim MVN(\frac{1}{\sigma}X\boldsymbol\beta, I)\]

Since \(I - \Pi_X\) is idempotent and symmetric,

\[K = rank(I - \Pi_X) = tr(I - \Pi_X) = n - rank(\Pi_X) = n - 2\]

\begin{equation} \label{eq:delta1}
  \begin{split}
    \delta & = \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg)^T (I - \Pi_X) \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg) \\
    & = \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} - \frac{(\boldsymbol\beta^TX^T)X(X^TX)^{-1}X^T(X\boldsymbol\beta)}{\sigma^2} \\
    & = \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} - \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} \\
    & = 0
  \end{split}
\end{equation}

Hence,

\[\frac{SSE}{\sigma^2} \sim \chi^2(n - 2)\]
\EndKnitrBlock{proof}

\BeginKnitrBlock{proposition}[Distribution of SSR]
\protect\hypertarget{prp:ssrdist}{}{\label{prp:ssrdist} \iffalse (Distribution of SSR) \fi{} }\[\frac{SSR}{\sigma^2} \sim \chi^2(1, \delta)\]

where \(\delta = \sum\limits_{i = 1}^n (x_i - \overline{x})^2 \beta_1^2 = \frac{S_{xx}\beta_1^2}{\sigma^2}\)
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}From Corollary \ref{cor:projss}, write

\[\frac{SSR}{\sigma^2} = \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)^T (\Pi_X - \Pi_{\mathbf{1}}) \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)\]

Note that \(\Pi_X - \Pi_{\mathbf{1}}\) is symmetric idempotent. One proceeds in a similar way.

\[K = rank(\Pi_X - \Pi_{\mathbf{1}}) = tr(\Pi_X - \Pi_{\mathbf{1}}) = rank(\Pi_X) - rank(\Pi_{\mathbf{1}}) = 2 - 1 = 1\]

\begin{equation*}
  \begin{split}
    \delta & = \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg)^T (\Pi_X - \Pi_{\mathbf{1}}) \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg) \qquad \because \frac{\mathbf{Y}}{\sigma} \sim MVN(\frac{1}{\sigma}X\boldsymbol\beta, I) \\
    & = \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} - \frac{\boldsymbol\beta^TX^T\Pi_{\mathbf{1}}X\boldsymbol\beta}{\sigma^2} \\
    & = \frac{\boldsymbol\beta^T(X^TX - X^T \Pi_{\mathbf{1}}X)\boldsymbol\beta}{\sigma^2} \\
    & = \frac{\boldsymbol\beta^T \Big\{ X^T(I - \Pi_{\mathbf{1}})X \Big\} \boldsymbol\beta }{\sigma^2}
  \end{split}
\end{equation*}

Since \(\mathbf{1} \in sp(\{ \mathbf{1} \})\),

\[\Pi_{\mathbf{1}} \mathbf{1} = \mathbf{1}\]

It gives that

\[\mathbf{1}^T(I - \Pi_{\mathbf{1}})\mathbf{1} = 0\]

If \(\mathbf{x} \neq \mathbf{1}\), then we have

\[\mathbf{x}^T(I - \Pi_{\mathbf{1}})\mathbf{x} = \sum_{i = 1}^n (x_i - \overline{x})^2 = S_{xx}\]

Recall that

\[\overline{x}\mathbf{1} = \mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T\mathbf{x} = \Pi_{\mathbf{1}}\mathbf{x}\]

Then we have

\[\mathbf{1}^T(I - \Pi_{\mathbf{1}})\mathbf{x} = \sum x_i - n \overline{x} = 0\]

Similarly,

\[\mathbf{x}^T(I - \Pi_{\mathbf{1}})\mathbf{1} = n \overline{x} - \sum x_i = 0\]

Hence by partitioning \(X = [\mathbf{1} \mid \mathbf{x}]\),

\begin{equation} \label{eq:delta2}
  \begin{split}
    \delta & = \frac{\boldsymbol\beta^T \Big\{ [\mathbf{1} \mid \mathbf{x}]^T(I - \Pi_{\mathbf{1}})[\mathbf{1} \mid \mathbf{x}] \Big\} \boldsymbol\beta }{\sigma^2} \\
    & = \frac{\boldsymbol\beta^T \begin{bmatrix} 0 & 0 \\ 0 & S_{xx} \end{bmatrix} \boldsymbol\beta}{\sigma^2} \\
    & = \frac{S_{xx}\beta_1^2}{\sigma^2}
  \end{split}
\end{equation}
\EndKnitrBlock{proof}

\BeginKnitrBlock{proposition}[Independence]
\protect\hypertarget{prp:ssind}{}{\label{prp:ssind} \iffalse (Independence) \fi{} }SSE and SSR are independent, i.e.

\[SSE \ind SSR\]
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Note that both \(SSE\) and \(SSR\) are quadratic forms of \(\mathbf{Y} \sim MVN(X\boldsymbol\beta, \sigma^2 I)\) and that each \(I - \Pi_X\) and \(\Pi_X - \Pi_{\mathbf{1}}\) is symmetric. Then from Proposition \ref{prp:quadmvn},

Claim: \((I - \Pi_X)(\sigma^2I)(\Pi_X - \Pi_{\mathbf{1}}) = 0\), i.e.~\((I - \Pi_X)(\Pi_X - \Pi_{\mathbf{1}}) = 0\)

It is obvious that

\[\Pi_X\Pi_{\mathbf{1}} = \Pi_{\mathbf{1}}\]

Then

\begin{equation*}
  \begin{split}
    (I - \Pi_X)(\Pi_X - \Pi_{\mathbf{1}}) & = \Pi_X - \Pi_{\mathbf{1}} - \Pi_X^2 + \Pi_X\Pi_{\mathbf{1}} \\
    & = \Pi_X - \Pi_{\mathbf{1}} - \Pi_X + \Pi_{\mathbf{1}} \qquad \because \text{idempotent} \\
    & = 0
  \end{split}
\end{equation*}

This completes the proof.
\EndKnitrBlock{proof}

\BeginKnitrBlock{proposition}[Independence]
\protect\hypertarget{prp:ssbind}{}{\label{prp:ssbind} \iffalse (Independence) \fi{} }SSE and \((\hat\beta_0, \hat\beta_1)\) are independent, i.e.

\[SSE \ind (\hat\beta_0, \hat\beta_1)^T\]
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Note that

\[\boldsymbol{\hat\beta}= (\hat\beta_0, \hat\beta_1)^T = (X^TX)^{-1}X^T\mathbf{Y}\]

Since \(I - \Pi_X\) of \(SSE\) is symmetric, from Proposition \ref{prp:quadmvn},

Claim: \(((X^TX)^{-1}X^T)(\sigma^2I)(I - \Pi_X) = 0\), i.e.~\(((X^TX)^{-1}X^T)(I - \Pi_X) = 0\)

Since \(\Pi_X = X(X^TX)^{-1}X^T\),

\begin{equation*}
  \begin{split}
    ((X^TX)^{-1}X^T)(I - \Pi_X) & = (X^TX)^{-1}X^T - (X^TX)^{-1}X^TX(X^TX)^{-1}X^T \\
    & = (X^TX)^{-1}X^T - (X^TX)^{-1}X^T \\
    & = 0
  \end{split}
\end{equation*}

This completes the proof.
\EndKnitrBlock{proof}

\BeginKnitrBlock{proposition}[Distribution of SST]
\protect\hypertarget{prp:sstdist}{}{\label{prp:sstdist} \iffalse (Distribution of SST) \fi{} }\[\frac{SST}{\sigma^2} \sim \chi^2(n - 1, \delta)\]

where \(\delta = \sum\limits_{i = 1}^n (x_i - \overline{x})^2 \beta_1^2 = \frac{S_{xx}\beta_1^2}{\sigma^2}\)
\EndKnitrBlock{proposition}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}It proceedes in a similary way from Corollary \ref{cor:projss}

\[\frac{SST}{\sigma^2} = \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)^T (I - \Pi_{\mathbf{1}}) \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)\]

Since \(I - \Pi_{\mathbf{1}}\) is symmetric idempotent,

\[K = rank(I - \Pi_{\mathbf{1}}) = tr(I - \Pi_{\mathbf{1}}) = n - rank(\Pi_{\mathbf{1}}) = n - 1\]

\begin{equation*}
  \begin{split}
    \delta & = \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg)^T (I - \Pi_{\mathbf{1}}) \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg) \\
    & = \frac{S_{xx}\beta_1^2}{\sigma^2} \qquad \because \eqref{eq:delta1} \:\text{and}\: \eqref{eq:delta2} 
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\hypertarget{anova-for-testing-significance-of-regression}{%
\subsection{ANOVA for testing significance of regression}\label{anova-for-testing-significance-of-regression}}

Recall that

\[SST = SSR + SSE\]

\begin{itemize}
\tightlist
\item
  \(SST\): the variation of a response itself
\item
  \(SSR\): the variation of a response \emph{explained by the model}
\item
  \(SSE\): the variation of a response that \emph{cannot be explained by the model}
\end{itemize}

As mentioned in section \ref{decompsst}, whether the model is useful or not can depend on the proportion of \(SSR\) versus \(SSE\) in constant \(SST\). When \(SSR\) is large compared to \(SSE\), we can say that the model is good. On the other hand, when \(SSR\) is not large, the model might be poor. This is what \(R^2\) measures intuitively.

However, this direct comparison somtimes does not work in many times. Both \(SSR\) and \(SSE\) comes from different distribution, which have different degrees of freedom. So we \emph{compare standardized versions}, i.e.~divided by the degrees of freedom.

\BeginKnitrBlock{definition}[Degrees of freedom]
\protect\hypertarget{def:dof}{}{\label{def:dof} \iffalse (Degrees of freedom) \fi{} }Degrees of freedom of each sum of squares is

\[df = \text{the number of deviation} - \text{the number of linear constraints}\]
\EndKnitrBlock{definition}

\BeginKnitrBlock{corollary}[df of SS]
\protect\hypertarget{cor:dfss}{}{\label{cor:dfss} \iffalse (df of SS) \fi{} }\(df\) of each sum of square is computed as

\(\text{(a)}\: df(SST) = n - 1\)

\(\text{(b)}\: df(SSR) = 1\)

\(\text{(c)}\: df(SSE) = n - 2\)
\EndKnitrBlock{corollary}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}(a)

Since \(\sum (Y_i - \overline{Y}) = 0\), we have \(1\) linear constraints. Thus,

\[df(SST) = n - 1\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\item
\end{enumerate}

Note that \(\hat{Y_i} - \overline{Y} = \hat\beta_1(x_i - \overline{x})\)

where \(\sum (x_i - \overline{x}) = 0\).

Thus,

\[df(SSR) = n - (n - 1) = 1\]

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\item
\end{enumerate}

From Example \ref{exm:usingnormal}, \(\sum (Y_i - \hat{Y_i}) = 0\) and \(\sum x_i (Y_i - \hat{Y_i}) = 0\).

Thus,

\[df(SSE) = n - 2\]
\EndKnitrBlock{proof}

Dividing sum of squares in \(df\), we can standardize it.

\BeginKnitrBlock{definition}[Mean square]
\protect\hypertarget{def:ms}{}{\label{def:ms} \iffalse (Mean square) \fi{} }Mean square is a sum of square \(SS\) divided by its degree of freedom \(df\)

\[MS := \frac{SS}{df}\]
\EndKnitrBlock{definition}

Using the values of corollary \ref{cor:dfss} we can define each mean square for \(SSR\) and \(SSE\).

\BeginKnitrBlock{definition}[Regression mean square]
\protect\hypertarget{def:msr}{}{\label{def:msr} \iffalse (Regression mean square) \fi{} }\[MSR := \frac{SSR}{1} = SSR\]
\EndKnitrBlock{definition}

From Proposition \ref{prp:ssrdist}, the following corollary can be drawn.

\BeginKnitrBlock{corollary}[Distribution of MSR]
\protect\hypertarget{cor:msrdist}{}{\label{cor:msrdist} \iffalse (Distribution of MSR) \fi{} }Under \(H_0: \beta_1 = 0\),

\[\frac{SSR}{\sigma^2} \stackrel{H_0}{\sim} \chi^2(1)\]
\EndKnitrBlock{corollary}

Now standardize residual sum of square.

\BeginKnitrBlock{definition}[Residual mean square]
\protect\hypertarget{def:mse}{}{\label{def:mse} \iffalse (Residual mean square) \fi{} }\[MSE := \frac{SSE}{n - 2}\]
\EndKnitrBlock{definition}

From Proposition \ref{prp:ssrdist}, we can construct same statistic. In fact, \(\frac{SSE}{\sigma^2}\) follows \(\chi^2(n - 2)\) whether or not \(\beta_1\) is zero. Its \(\delta = 0\).

\BeginKnitrBlock{corollary}[Distribution of MSE]
\protect\hypertarget{cor:msedist}{}{\label{cor:msedist} \iffalse (Distribution of MSE) \fi{} }\[\frac{SSE}{\sigma^2} \sim \chi^2(n - 2)\]
\EndKnitrBlock{corollary}

Finally, we can now use Proposition \ref{prp:fdist} so that

\[
F \equiv \frac{MSR}{MSE} = \frac{\frac{SSE / \sigma^2 \sim \chi^2(1)}{1}}{\frac{SSR / \sigma^2 \hsim \chi^2(n - 2)}{n - 2}} \hsim F(1, n - 2)
\]

By construction, this test statistic is used for

\[H_0: \beta_1 = 0\]

which means that the predictor does not explain the response anything. In other words, we are testing that

\begin{equation}
  H_0: \text{Model is not useful at all} \qquad \text{vs} \qquad H_1: \text{Model can explain data}
  \label{eq:goodfit}
\end{equation}

\BeginKnitrBlock{remark}[F statistic on testing significance]
\iffalse{} {Remark (F statistic on testing significance). } \fi{}Null hypothesis \eqref{eq:goodfit} can be tested with \(F\)-statistic.

\[F_0 = \frac{MSR}{MSE} = \frac{SSR / df(SSR)}{SSE / df(SSE)} \hsim F(df(SSR), df(SSE))\]
\EndKnitrBlock{remark}

Then we reject \(H_0\) if

\[F_0 > F_\alpha\bigg( df(SSR), df(SSE) \bigg)\]

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{regression-analysis_files/figure-latex/goodfitfig-1} 

}

\caption{Rejection region for significance testing}\label{fig:goodfitfig}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(delv_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = y ~ x, data = delv)

Residuals:
   Min     1Q Median     3Q    Max 
-7.581 -1.874 -0.349  2.181 10.634 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)    3.321      1.371    2.42    0.024 *  
x              2.176      0.124   17.55  8.2e-15 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.18 on 23 degrees of freedom
Multiple R-squared:  0.93,  Adjusted R-squared:  0.927 
F-statistic:  308 on 1 and 23 DF,  p-value: 8.22e-15
\end{verbatim}

This statistic is \texttt{F-statistic} included in \texttt{summary.lm()} output. This is saved as \texttt{\$fstatistic}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(delv_fit)}\OperatorTok{$}\NormalTok{fstatistic}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
value numdf dendf 
  308     1    23 
\end{verbatim}

We usually summarize these statistic in table form, so called \emph{ANOVA table}.

\begin{longtable}[]{@{}cccccc@{}}
\toprule
Source & SS & df & MS & F & p-value\tabularnewline
\midrule
\endhead
Model & \(SSR\) & \(1\) & \(MSR\) & \(F_0\) & p-value\tabularnewline
Error & \(SSE\) & \(n - 2\) & \(MSE\) & &\tabularnewline
Total & \(SST\) & \(n - 1\) & & &\tabularnewline
\bottomrule
\end{longtable}

To get this table, just use \texttt{anova()} for \texttt{lm} object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anova}\NormalTok{(delv_fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value  Pr(>F)    
x          1   5382    5382     308 8.2e-15 ***
Residuals 23    402      17                    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Since the last \texttt{Total} row is just sum of the model and error, the function does not give it. To use this table as \texttt{data.frame} more easily, just implement \texttt{broom::tidy} as before.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anova}\NormalTok{(delv_fit) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 6
  term         df sumsq meansq statistic   p.value
  <chr>     <int> <dbl>  <dbl>     <dbl>     <dbl>
1 x             1 5382. 5382.       308.  8.22e-15
2 Residuals    23  402.   17.5       NA  NA       
\end{verbatim}

Denote that here \emph{simple linear regression setting} \(F\)-statistic and \(t\)-statistic of Equation \eqref{eq:b1test} perform exactly same thing, \(H_0 : \beta_1 = 0\). In fact, we know that

\[F(1, k) \stackrel{d}{=} T_k^2\]

\BeginKnitrBlock{remark}
\iffalse{} {Remark. } \fi{}In the simple linear regression setting, \(F\)-test for significance and \(t\)-test for no slope are equivalent, i.e.~under \(H_0 : \beta_1 = 0\)

\[F_0 = \frac{\hat\beta_1 S_{xx}}{\sigma^2} = \bigg( \frac{\hat\beta_1}{\sigma / \sqrt{S_xx}} \bigg) = T_0^2\]
\EndKnitrBlock{remark}

\hypertarget{multiple}{%
\chapter{Multiple Linear Regression}\label{multiple}}

\hypertarget{model-1}{%
\section{Model}\label{model-1}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(cem <-}\StringTok{ }\NormalTok{MPV}\OperatorTok{::}\NormalTok{cement }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 13 x 5
       y    x1    x2    x3    x4
   <dbl> <dbl> <dbl> <dbl> <dbl>
 1  78.5     7    26     6    60
 2  74.3     1    29    15    52
 3 104.     11    56     8    20
 4  87.6    11    31     8    47
 5  95.9     7    52     6    33
 6 109.     11    55     9    22
 7 103.      3    71    17     6
 8  72.5     1    31    22    44
 9  93.1     2    54    18    22
10 116.     21    47     4    26
11  83.8     1    40    23    34
12 113.     11    66     9    12
13 109.     10    68     8    12
\end{verbatim}

Above is a data set about cement and concerning four ingredients from the \citet{Montgomery:2015aa} textbook.

\begin{itemize}
\tightlist
\item
  \texttt{y}: heat evolved in calories per gram of cement
\item
  \texttt{x1}: tricalcium aluminate
\item
  \texttt{x2}: tricalcium silicate
\item
  \texttt{x3}: tetracalcium alumino ferrite
\item
  \texttt{x4}: dicalcium silicate
\end{itemize}

Given data \((x_{11}, x_{12}, \ldots, x_{1p}, Y_1), \ldots, (x_{n1}, x_{n2}, \ldots, x_{np}, Y_n)\) (\(p = 4\)), we try to fit linear regression model

\[Y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \epsilon_{i}\]

with

\[\epsilon_i \iid (0, \sigma^2)\]

Compared to simple linear regression problem \ref{simple}, we have more parameters for coefficients

\[(\beta_0, \beta_1, \ldots, \beta_p, \sigma^2)\]

Each \(\beta_j\) is a change of \(Y\) when each predictor variable \(x_j\) increases in 1 unit while the others fixed. In this part, we use \emph{matrix notation}. Extending our former matrix work \ref{matnot},

\[
\underset{\substack{\\ \\ \\ \\ \huge \mathbf{Y}}}{\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
\end{bmatrix}} = \underset{\substack{\\ \\ \\ \\ \huge X}}{\begin{bmatrix}
  1 & x_{11} & \cdots & x_{1p} \\
  1 & x_{21} & \cdots & x_{2p} \\
  \vdots & \vdots & \vdots & \vdots \\
  1 & x_{n1} & \cdots & x_{np}
\end{bmatrix}} \underset{\substack{\\ \\ \\ \\ \huge \boldsymbol\beta}}{\begin{bmatrix}
  \beta_0 \\
  \vdots \\
  \beta_p
\end{bmatrix}} + \underset{\substack{\\ \\ \\ \\ \huge \boldsymbol\epsilon}}{\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \vdots \\
  \epsilon_n
\end{bmatrix}}
\]

where \(\epsilon_i\) are i.i.d., and

\[E\boldsymbol\epsilon = \mathbf{0}\]

\[Var\boldsymbol\epsilon = \sigma^2 I\]

\hypertarget{least-square-estimation}{%
\section{Least Square Estimation}\label{least-square-estimation}}

Write \(\boldsymbol\beta \equiv (\beta_1, \ldots, \beta_p)^T \in \R^{p + 1}\). Extend Equation \eqref{eq:qmatrix}.

\begin{equation}
  \begin{split}
    \boldsymbol{\hat\beta} & = \argmin_{\boldsymbol\beta \in \R^{p + 1}} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_{i1} - \cdots - \beta_p x_{ip})^2 \\
    & = \argmin_{\boldsymbol\beta \in \R^{p + 1}} \lVert \mathbf{Y} - \beta_0 \mathbf{1} - \beta_1 \mathbf{x}_1 - \cdots - \beta_p \mathbf{x}_p \rVert^2 \\
    & = \argmin_{\boldsymbol\beta \in \R^{p + 1}} \lVert \mathbf{Y} - X\boldsymbol\beta \rVert^2
  \end{split}
  \label{eq:qmultiple}
\end{equation}

As discussed, the solution \(\boldsymbol{\hat\beta}\) is related to the projection. \(X\boldsymbol{\hat\beta}\) is a projection of \(\mathbf{Y}\) onto \(Col(X)\).

\hypertarget{normal-equation}{%
\subsection{Normal equation}\label{normal-equation}}

Now recap the section \ref{solproj}. Fundamental subspaces theorem \ref{thm:fundsub} implies that

\[\mathbf{Y} - X\boldsymbol{\hat\beta} \in Col(X)^{\perp} = N(X^T)\]

From the second part of subset, i.e.~\(N(X^T)\), we now have \emph{Normal equation}

\begin{equation}
  X^T(\mathbf{Y} - X\boldsymbol{\hat\beta}) = \mathbf{0}
  \label{eq:multeq}
\end{equation}

This is equivalent to

\[X^T\mathbf{Y} = X^TX\boldsymbol{\hat\beta}\]

Hence, if \(X^T X\) is invertible, the equation gives unique solution

\[\boldsymbol{\hat\beta} = (X^TX)^{-1}X^T \mathbf{Y}\]

Our first question is when \(X^T X\) is invertible, and Theorem \ref{thm:fullrank} have said that it is when the model matrix \(X\) is full rank.

\BeginKnitrBlock{lemma}
\protect\hypertarget{lem:modelnnd}{}{\label{lem:modelnnd} }Let \(X \in \R^{n \times (p + 1)}\) be any model matrix. Then \(X^T X\) is always non-negative definite.

\[\forall \mathbf{v} \in \R^{p + 1} : \mathbf{v}^T(X^T X)\mathbf{v} \ge 0\]
\EndKnitrBlock{lemma}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Let \(\mathbf{v} \in \R^{p + 1}\). Then

\[\mathbf{v}^T(X^T X)\mathbf{v} = (X\mathbf{v})^T (X\mathbf{v}) = \lVert X\mathbf{v} \rVert^2 \ge 0\]
\EndKnitrBlock{proof}

This lemma can also prove our Theorem \ref{thm:fullrank}.

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:fullrank2}{}{\label{thm:fullrank2} }Let \(\mathbf{Y} = X\boldsymbol\beta\) inconsistent and let \(X \in \R^{n \times (p + 1)}\) with \(n > p + 1\).

If \(rank(X) = p + 1\), i.e.~full rank, then \(X^T X\) is invertible.
\EndKnitrBlock{theorem}

\BeginKnitrBlock{proof}
\iffalse{} {Proof. } \fi{}Let \(\mathbf{c} \in \R^{(p + 1)}\)

Suppose that \(X^T X\) is positive definite.

\begin{equation*}
  \begin{split}
    & \Leftrightarrow \mathbf{c}^TX^T X \mathbf{c} = 0 \quad \text{implies} \quad \mathbf{c} = \mathbf{0} \\
    & \Leftrightarrow X\mathbf{c} = \mathbf{0} \quad \text{implies} \quad \mathbf{c} = \mathbf{0} \\
    & \Leftrightarrow \text{columns of}\: X \:\text{linearly independent} \\
    & \Leftrightarrow rank(X) = p + 1
  \end{split}
\end{equation*}
\EndKnitrBlock{proof}

\hypertarget{orthogonal-decomposition}{%
\subsection{Orthogonal decomposition}\label{orthogonal-decomposition}}

Let's briefly look at orthogonalization process.

\BeginKnitrBlock{theorem}[Gram-Schmidt Process]
\protect\hypertarget{thm:gs}{}{\label{thm:gs} \iffalse (Gram-Schmidt Process) \fi{} }Let \(\{ \mathbf{a}_1, \ldots, \mathbf{a}_m \}\) be a basis for the inner product space \(V\). Let

\[\mathbf{u}_1 = \bigg( \frac{1}{\lVert \mathbf{a}_1 \rVert} \bigg) \mathbf{a}_1\]

and define next \(\mathbf{u}_2, \ldots, \mathbf{u}_m\) recursively by

\[\mathbf{u}_{k + 1} = \frac{1}{\lVert \mathbf{a}_{k + 1} - \mathbf{p}_k \rVert}(\mathbf{a}_{k + 1} - \mathbf{p}_k)\]

for \(k = 1, \ldots, m - 1\), where

\[\mathbf{p}_k = <\mathbf{a}_{k + 1}, \mathbf{u}_1> \mathbf{u}_1 + <\mathbf{a}_{k + 1}, \mathbf{u}_2 >\mathbf{u}_2 + \cdots + < \mathbf{a}_{k + 1}, \mathbf{u} > \mathbf{u}_k\]

is the projection of \(\mathbf{a}_{k + 1}\) onto \(sp(\{ \mathbf{u}_1, \ldots, \mathbf{u}_k \})\).

Hence, we get \(\{ \mathbf{u}_1, \ldots, \mathbf{u}_m \}\) is an orthonormal basis for \(V\).
\EndKnitrBlock{theorem}

This \emph{orthonormal basis} gives some useful facts with least squares problem \citep{Leon:2014aa}.

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:orthonormal}{}{\label{thm:orthonormal} }Let \(Col(X)\) be a subspace of \(\R^n\), let \(\mathbf{Y} \in \R^n\), and let \(\{ \mathbf{u}_0, \ldots, \mathbf{u}_{p} \}\) be an orthonormal basis for \(Col(X)\). If

\[\mathbf{\hat{Y}} = \sum_{j = 0}^p \hat\beta_j \mathbf{u}_j\]

where

\[\hat\beta_j = \Pi(\mathbf{Y} \mid R(\mathbf{u}_j)) \quad \text{for each} \: i\]

then \(\mathbf{\hat{Y}} - \mathbf{Y} \in Col(X)^{\perp}\).
\EndKnitrBlock{theorem}

\BeginKnitrBlock{theorem}
\protect\hypertarget{thm:orthonormalproj}{}{\label{thm:orthonormalproj} }Under the hypothesis of Theorem \ref{thm:orthonormal}, \(\mathbf{\hat{Y}} \in Col(X)\) is the closest to \(\mathbf{Y}\) amongst its any element \(\mathbf{p}\), i.e.

\[\Vert \mathbf{p} - \mathbf{Y} \Vert > \Vert \mathbf{\hat{Y}} - \mathbf{Y} \Vert\]

for any \(\mathbf{p} \neq \mathbf{\hat{Y}}\) in \(Col(X)\)
\EndKnitrBlock{theorem}

In other words, projection of \(\mathbf{Y}\) onto \(Col(X)\), \(\mathbf{\hat{Y}}\) can be \emph{represented as sum of projections of} \(\mathbf{Y}\) \emph{onto each (orthogonal) individual variable}. Before looking at individual basis, consider two-block space.

Write

\[
X = \left[\begin{array}{c|ccc}
  1 & x_{11} & \cdots & x_{1p} \\
  1 & x_{21} & \cdots & x_{2p} \\
  \vdots & \vdots & \vdots & \vdots \\
  1 & x_{n1} & \cdots & x_{np}
\end{array}\right] = [\mathbf{1}, \mathbb{X}_A]
\]

Consider \(R(X)\), \(R(\mathbf{1})\), and \(R(\mathbb{X}_A)\).

To decompose subspace \(R(X)\), we try to orthogonalize \(\mathbf{1}\) and \(\mathbb{X}_A\) applying G-S process \ref{thm:gs}.

\[\mathbf{1} \perp \mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A\]

Theorem \ref{thm:dsum} implies that

\[R(X) = R(\mathbf{1}) \oplus R(\mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A)\]

\BeginKnitrBlock{theorem}[Orthogonal decomposition]
\protect\hypertarget{thm:orthdecomp}{}{\label{thm:orthdecomp} \iffalse (Orthogonal decomposition) \fi{} }Let \(X = [\mathbf{1}, \mathbb{X}_A]\). Then

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
\end{enumerate}

\[R(X) = R(\mathbf{1}) \oplus R(\mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A)\]

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\setcounter{enumi}{1}
\item
\end{enumerate}

\[\Pi(\cdot \mid R(X)) = \Pi(\cdot \mid R(\mathbf{1})) + \Pi(\cdot \mid R(\mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A))\]
\EndKnitrBlock{theorem}

Write

\[\mathbb{X}_{A, \perp} := \mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A\]

Note that

\[\Pi_{\mathbf{1}} = \mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T = \frac{1}{n}\mathbf{1}\mathbf{1}^T\]

Then

\begin{equation}
  \begin{split}
    X\boldsymbol{\hat\beta} & = \hat\beta_0\mathbf{1} + \mathbb{X}_A\boldsymbol{\hat\beta}_A \\
    & = \hat\beta_0\mathbf{1} + (\mathbb{X}_{A, \perp} + \Pi_{\mathbf{1}}\mathbb{X}_A)\boldsymbol{\hat\beta}_A \\
    & = \Big(\hat\beta_0 + \frac{1}{n}\mathbf{1}^T\mathbb{X}_A\boldsymbol{\hat\beta}_A \Big)\mathbf{1} + \mathbb{X}_{A,\perp}\boldsymbol{\hat\beta}_A \qquad \because \hat\beta_0 + \frac{1}{n}\mathbf{1}^T\mathbb{X}_A\boldsymbol{\hat\beta}_A \in \R
  \end{split}
  \label{eq:blockfit}
\end{equation}

From (ii) of Theorem \ref{thm:orthdecomp},

\begin{equation}
  \begin{split}
    \Pi(\mathbf{Y} \mid R(X)) & = \Pi(\mathbf{Y} \mid R(\mathbf{1})) + \Pi(\mathbf{Y} \mid R(\mathbb{X}_{A,\perp})) \\
    & = \overline{Y}\mathbf{1} + \mathbb{X}_{A,\perp}(\mathbb{X}_{A,\perp}^T\mathbb{X}_{A,\perp})^{-1}\mathbb{X}_{A,\perp}^T\mathbf{Y}
  \end{split}
  \label{eq:decompfit}
\end{equation}

Equations \eqref{eq:blockfit} and \eqref{eq:decompfit} give that

\[
\begin{cases}
  \hat\beta_0 = \overline{Y} - \frac{1}{n}\mathbf{1}\mathbb{X}_A\boldsymbol{\hat\beta}_A \\
  \boldsymbol{\hat\beta}_A = (\mathbb{X}_{A,\perp}^T\mathbb{X}_{A,\perp})^{-1}\mathbb{X}_{A,\perp}^T\mathbf{Y}
\end{cases}
\]

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/multiple-orthogonal} 

}

\caption{Orthogonal decomposition of the column space and LSE}\label{fig:illdecomp}
\end{figure}

See Figure \ref{fig:illdecomp}. Two are orthogonal, so sum of projections onto them become LSE. In fact, \emph{each projection indicate each regression coefficient}. When we do not have orthogonal basis, however, each projection is nothing.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/multiple-nonorth} 

}

\caption{Non-orthongality}\label{fig:illdecomp2}
\end{figure}

In this situation, we have to do orthogonalization.

\[\tilde{\mathbb{X}}_A = \Pi_{\mathbf{1}}\mathbb{X}_A + (\mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A)\]

\hypertarget{gram-schmidt-qr-factorization}{%
\subsection{Gram-Schmidt QR factorization}\label{gram-schmidt-qr-factorization}}

\bibliography{book.bib,packages.bib}


\end{document}
