[
["index.html", "R Lab for Regression Analysis Welcome", " R Lab for Regression Analysis Young-geun Kim Department of Statistics, SKKU 02 Apr, 2019 Welcome This book aims at covering materials of regression analysis. Also, there will be R programming for regression. "],
["linear-regression-analysis.html", "Linear Regression Analysis Relation", " Linear Regression Analysis data(BioOxyDemand, package = &quot;MPV&quot;) (BioOxyDemand &lt;- BioOxyDemand %&gt;% tbl_df()) # A tibble: 14 x 2 x y &lt;int&gt; &lt;int&gt; 1 3 4 2 8 7 3 10 8 4 11 8 5 13 10 6 16 11 7 27 16 8 30 26 9 35 21 10 37 9 11 38 31 12 44 30 13 103 75 14 142 90 Relation We wonder how x affects y, especially linearly. Functional relation: mathematical equation, \\[y = \\beta_0 + \\beta_1 x\\] Statistical relation: embeded with noise So we try to estimate \\[y = \\beta_0 + \\beta_1 x + \\epsilon\\] BioOxyDemand %&gt;% ggplot(aes(x, y)) + geom_point() Looking just with the eyes, we can see the linear relationship. Regression analysis estimates the relationship statistically. BioOxyDemand %&gt;% ggplot(aes(x, y)) + geom_smooth(method = &quot;lm&quot;) + geom_point() "],
["simple.html", "Chapter 1 Simple Linear Regression 1.1 Model 1.2 Least Squares Estimation 1.3 Maximum Likelihood Estimation 1.4 Residuals 1.5 Decomposition of Total Variability 1.6 Geometric Interpretations 1.7 Distributions", " Chapter 1 Simple Linear Regression 1.1 Model delv &lt;- MPV::p2.9 %&gt;% tbl_df() delv %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + labs( x = &quot;Number of Cases&quot;, y = &quot;Delivery Time&quot; ) Figure 1.1: The Delivery Time Data Given data \\((x_1, Y_1), \\ldots, (x_n, Y_n)\\), we try to fit linear model \\[Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] Here \\(\\epsilon_i\\) is a error term, which is a random variable. \\[\\epsilon \\stackrel{iid}{\\sim} (0, \\sigma^2)\\] It gives the problem of estimating three parameters \\((\\beta_0, \\beta_1, \\sigma^2)\\). Before estimating these, we set some assumptions. linear relationship \\(\\epsilon_i\\)s are independent \\(\\epsilon_i\\)s are identically destributed, i.e. constant variance In some setting, \\(\\epsilon_i \\sim N\\) 1.2 Least Squares Estimation Figure 1.2: Idea of the least square estimation We try to find \\(\\beta_0\\) and \\(\\beta_1\\) that minimize the sum of squares of the vertical distances, i.e. \\[\\begin{equation} (\\beta_0, \\beta_1) = \\arg\\min \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 x_i)^2 \\tag{1.1} \\end{equation}\\] 1.2.1 Normal equations Denote that Equation (1.1) is quadratic. Then we can find its minimum by find the zero point of the first derivative. Set \\[Q(\\beta_0, \\beta_1) := \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 x_i)^2\\] Then we have \\[\\begin{equation} \\frac{\\partial Q}{\\partial \\beta_0} = -2 \\sum_{i = 1}^n(Y_i - \\beta_0 - \\beta_1 x_i) = 0 \\tag{1.2} \\end{equation}\\] and \\[\\begin{equation} \\frac{\\partial Q}{\\partial \\beta_1} = -2 \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 x_i)x_i = 0 \\tag{1.3} \\end{equation}\\] From Equation (1.2), \\[\\sum_{i = 1}^n Y_i - n \\hat\\beta_0 - \\hat\\beta_1 \\sum_{i = 1}^n x_i = 0\\] Thus, \\[\\hat\\beta_0 = \\overline{Y} - \\hat\\beta_1 \\overline{x}\\] Equation (1.3) gives \\[\\sum_{i = 1}^n x_i (Y_i - \\overline{Y} + \\hat\\beta_1\\overline{x} - \\hat\\beta_1 x_i) = \\sum_{i = 1}^n x_i(Y_i - \\overline{Y}) - \\hat\\beta_1\\sum_{i = 1}^n x_i (x_i - \\overline{x}) = 0\\] Thus, \\[\\hat\\beta_1 = \\frac{\\sum\\limits_{i = 1}^nx_i(Y_i - \\overline{Y})}{\\sum\\limits_{i = 1}^n x_i (x_i - \\overline{x})}\\] Remark. \\[\\hat\\beta_1 = \\frac{S_{XY}}{S_{XX}}\\] where \\(S_{XX} := \\sum\\limits_{i = 1}^n (x_i - \\overline{x})^2\\) and \\(S_{XY} := \\sum\\limits_{i = 1}^n (x_i - \\overline{x})(Y_i - \\overline{Y})\\) Proof. Note that \\(\\overline{x}^2 = \\frac{1}{n^2}\\bigg(\\sum\\limits_{i = 1}^n x_i\\bigg)^2\\). Then we have \\[\\begin{equation} \\begin{split} S_{XX} &amp; = \\sum_{i = 1}^n (x_i - \\overline{x})^2 \\\\ &amp; = \\sum_{i = 1}^n x_i^2 - 2\\sum_{i = 1}^n x_i \\overline{x} + \\sum_{i = 1}^n\\overline{x}^2 \\\\ &amp; = \\sum_{i = 1}^n x_i^2 - \\frac{2}{n}\\bigg(\\sum\\limits_{i = 1}^n x_i\\bigg)^2 + \\frac{1}{n}\\bigg(\\sum\\limits_{i = 1}^n x_i\\bigg)^2 \\\\ &amp; = \\sum_{i = 1}^n x_i^2 - \\frac{1}{n}\\bigg(\\sum\\limits_{i = 1}^n x_i\\bigg)^2 \\end{split} \\tag{1.4} \\end{equation}\\] It follows that \\[\\begin{equation*} \\begin{split} \\hat\\beta_1 &amp; = \\frac{\\sum x_i(Y_i - \\overline{Y})}{\\sum x_i (x_i - \\overline{x})} \\\\ &amp; = \\frac{\\sum x_i (Y_i - \\overline{Y}) - \\overline{x}\\sum (Y_i - \\overline{Y})}{\\sum x_i^2 - \\frac{1}{n} (\\sum x_i)^2} \\qquad \\because \\sum (Y_i - \\overline{Y}) = 0 \\\\ &amp; = \\frac{\\sum (x_i - \\overline{x})(Y_i - \\overline{Y})}{\\sum x_i^2 - \\frac{1}{n} (\\sum x_i)^2} \\\\ &amp; = \\frac{S_{XY}}{S_{XX}} \\end{split} \\end{equation*}\\] lm(y ~ x, data = delv) Call: lm(formula = y ~ x, data = delv) Coefficients: (Intercept) x 3.32 2.18 1.2.2 Prediction and Mean response “Essentially, all models are wrong, but some are useful.” —George Box Recall that we have assumed the linear assumption between the predictor and the response variables, i.e. the true model. Estimating \\(\\beta_0\\) and \\(\\beta_1\\) is same as estimating the assumed true model. Definition 1.1 (Mean response) \\[E(Y \\mid X = x) = \\beta_0 + \\beta_1 x\\] We can estimate this mean resonse by \\[\\begin{equation} \\widehat{E(Y \\mid x)} = \\hat\\beta_0 + \\hat\\beta_1 x \\tag{1.5} \\end{equation}\\] However, in practice, the model might not be true, which is included in \\(\\epsilon\\) term. \\[Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] Our real problem is predicting individual \\(Y\\), not the mean. The prediction of response can be done by \\[\\begin{equation} \\hat{Y_i} = \\hat\\beta_0 + \\hat\\beta_1 x_i \\tag{1.6} \\end{equation}\\] Observe that the values of Equations (1.5) and (1.6) are same. However, due to the error term in the prediction, it has larger standard error. 1.2.3 Properties of LSE Parameters \\(\\beta_0\\) and \\(\\beta_1\\) have some properties related to the expectation and variance. We can notice that these lse’s are unbiased linear estimator. In fact, these are the best unbiased linear estimator. This will be covered in the Gauss-Markov theorem. Lemma 1.1 \\[S_{XX} = \\sum_{i = 1}^n x_i^2 - \\frac{1}{n}\\bigg(\\sum\\limits_{i = 1}^n x_i\\bigg)^2 = \\sum_{i = 1}^n x_i(x_i - \\overline{x})\\] \\[S_{XY} = \\sum_{i = 1}^n x_i Y_i - \\frac{1}{n}\\bigg(\\sum_{i = 1}^n x_i\\bigg)\\bigg(\\sum_{i = 1}^n Y_i\\bigg) = \\sum_{i = 1}^n Y_i(x_i - \\overline{x})\\] Proof. We already proven the first part of \\(S_{XX}\\). See the Equation (1.4). The second part is tivial. Since \\(\\sum (x_i - \\overline{x}) = 0\\), \\[S_{XX} = \\sum_{i = 1}^n (x_i - \\overline{x})^2 = \\sum_{i = 1}^n (x_i - \\overline{x})x_i\\] For the first part of \\(S_{XY}\\), \\[\\begin{equation*} \\begin{split} S_{XY} &amp; = \\sum_{i = 1}^n (x_i - \\overline{x})(Y_i - \\overline{Y}) \\\\ &amp; = \\sum_{i = 1}^n x_i Y_i - \\overline{x} \\sum_{i = 1}^n Y_i - \\overline{Y} \\sum_{i = 1}^n x_i + n \\overline{x} \\overline{Y} \\\\ &amp; = \\sum_{i = 1}^n x_i Y_i - \\frac{1}{n}\\bigg(\\sum_{i = 1}^n x_i\\bigg)\\bigg(\\sum_{i = 1}^n Y_i\\bigg) \\end{split} \\end{equation*}\\] Second part of \\(S_{XY}\\) also can be proven from the definition. \\[\\begin{equation*} \\begin{split} S_{XY} &amp; = \\sum_{i = 1}^n (x_i - \\overline{x})(Y_i - \\overline{Y}) \\\\ &amp; = \\sum_{i = 1}^n Y_i (x_i - \\overline{x}) - \\overline{Y} \\sum_{i = 1}^n (x_i - \\overline{x}) \\\\ &amp; = \\sum_{i = 1}^n Y_i (x_i - \\overline{x}) \\qquad \\because \\sum_{i = 1}^n (x_i - \\overline{x}) = 0 \\end{split} \\end{equation*}\\] Lemma 1.2 (Linearity) Each coefficient is a linear estimator. \\[\\hat\\beta_1 = \\sum_{i = 1}^n\\frac{(x_i - \\overline{x})}{S_{XX}}Y_i\\] \\[\\hat\\beta_0 = \\sum_{i = 1}^n \\bigg( \\frac{1}{n} - \\frac{(x_i - \\overline{x})}{S_{XX}} \\bigg) Y_i\\] Proof. From lemma 1.1, \\[\\begin{equation*} \\begin{split} \\hat\\beta_1 &amp; = \\frac{S_{XY}}{S_{XX}} \\\\ &amp; = \\frac{1}{S_{XX}}\\sum_{i = 1}^n (x_i - \\overline{x}) Y_i \\end{split} \\end{equation*}\\] It gives that \\[\\begin{equation*} \\begin{split} \\hat\\beta_0 &amp; = \\overline{Y} - \\hat\\beta_1 \\overline{x} \\\\ &amp; = \\frac{1}{n}\\sum_{i = 1}^n Y_i - \\overline{x} \\sum_{i = 1}^n\\frac{(x_i - \\overline{x})}{S_{XX}}Y_i \\\\ &amp; = \\sum_{i = 1}^n\\bigg(\\frac{1}{n} - \\frac{(x_i - \\overline{x})\\overline{x}}{S_{XX}} \\bigg)Y_i \\end{split} \\end{equation*}\\] Proposition 1.1 (Unbiasedness) Both coefficients are unbiased. \\(\\text{(a)}\\: E\\hat\\beta_1 = \\beta_1\\) \\(\\text{(b)}\\: E\\hat\\beta_0 = \\beta_0\\) From the model, \\(Y_1, \\ldots, Y_n \\stackrel{indep}{\\sim} (\\beta_0 + \\beta_1 x_i, \\sigma^2)\\). Proof. From lemma 1.1, \\[\\begin{equation*} \\begin{split} E\\hat\\beta_1 &amp; = \\sum_{i = 1}^n \\bigg[ \\frac{(x_i - \\overline{x})}{S_{XX}} E(Y_i) \\bigg] \\\\ &amp; = \\sum_{i = 1}^n \\frac{(x_i - \\overline{x})}{S_{XX}}(\\beta_0 + \\beta_1 x_i) \\\\ &amp; = \\frac{\\beta_1 \\sum (x_i - \\overline{x})x_i}{\\sum (x_i - \\overline{x})x_i} \\qquad \\because \\sum (x_i - \\overline{x}) = 0 \\\\ &amp; = \\beta_1 \\end{split} \\end{equation*}\\] It follows that \\[\\begin{equation*} \\begin{split} E\\hat\\beta_0 &amp; = E(\\overline{Y} - \\hat\\beta_1 \\overline{x}) \\\\ &amp; = E(\\overline{Y}) - \\overline{x}E(\\hat\\beta_1) \\\\ &amp; = E(\\beta_0 + \\beta_1 \\overline{x} + \\overline{\\epsilon}) - \\beta_1 \\overline{x} \\\\ &amp; = \\beta_0 + \\beta_1 \\overline{x} - \\beta_1 \\overline{x} \\\\ &amp; = \\beta_0 \\end{split} \\end{equation*}\\] Proposition 1.2 (Variances) Variances and covariance of coefficients \\(\\text{(a)}\\: Var\\hat\\beta_1 = \\frac{\\sigma^2}{S_{XX}}\\) \\(\\text{(b)}\\: Var\\hat\\beta_0 = \\bigg( \\frac{1}{n} + \\frac{\\overline{x}^2}{S_{XX}} \\bigg)\\sigma^2\\) \\(\\text{(c)}\\: Cov(\\hat\\beta_0, \\hat\\beta_1) = - \\frac{\\overline{x}}{S_{XX}} \\sigma^2\\) Proof. Proving is just arithmetic. \\[\\begin{equation*} \\begin{split} Var\\hat\\beta_1 &amp; = \\frac{1}{S_{XX}^2}\\sum_{i = 1}^n \\bigg[ (x_i - \\overline{x})^2 Var(Y_i) \\bigg] + \\frac{1}{S_{XX}^2} \\sum_{j \\neq k}^n \\bigg[ (x_j - \\overline{x})(x_k - \\overline{x}) Cov(Y_j, Y_k) \\bigg] \\\\ &amp; = \\frac{\\sigma^2}{S_{XX}} \\qquad \\because Cov(Y_j, Y_k) = 0 \\: \\text{if} \\: j \\neq k \\end{split} \\end{equation*}\\] \\[\\begin{equation*} \\begin{split} Var\\hat\\beta_0 &amp; = \\sum_{i = 1}^n \\bigg( \\frac{1}{n} - \\frac{(x_i - \\overline{x})\\overline{x}}{S_{XX}} \\bigg)^2Var(Y_i) + \\sum_{j \\neq k} \\bigg( \\frac{1}{n} - \\frac{(x_j - \\overline{x})\\overline{x}}{S_{XX}} \\bigg)\\bigg( \\frac{1}{n} - \\frac{(x_k - \\overline{x})\\overline{x}}{S_{XX}} \\bigg) Cov(Y_j, Y_k) \\\\ &amp; = \\frac{\\sigma^2}{n} - 2 \\sigma^2 \\frac{\\overline{x}}{S_{XX}} \\sum_{i = 1}^n (x_i - \\overline{x}) + \\frac{\\sigma^2 \\overline{x}^2 \\sum (x_i - \\overline{x})^2}{S_{XX}^2} \\\\ &amp; = \\bigg( \\frac{1}{n} + \\frac{\\overline{x}^2}{S_{XX}} \\bigg) \\sigma^2 \\qquad \\because \\sum (x_i - \\overline{x}) = 0 \\end{split} \\end{equation*}\\] \\[\\begin{equation*} \\begin{split} Cov(\\hat\\beta_0, \\hat\\beta_1) &amp; = Cov(\\overline{Y} - \\hat\\beta_1 \\overline{x}, \\hat\\beta_1) \\\\ &amp; = - \\overline{x} Var\\hat\\beta_1 \\\\ &amp; = - \\frac{\\overline{x}}{S_{XX}} \\sigma^2 \\end{split} \\end{equation*}\\] 1.2.4 Gauss-Markov Theorem Chapter 1.2.3 shows that the \\(\\beta_0^{LSE}\\) and \\(\\beta_1^{LSE}\\) are the linear unbiased estimators. Are these good? Good compared to what estimators? Here we consider linear unbiased estimator. If variances in the proposition 1.2 are lower than any parameters in this parameter family, \\(\\beta_0^{LSE}\\) and \\(\\beta_1^{LSE}\\) are the best linear unbiased estimators. Theorem 1.1 (Gauss Markov Theorem) \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are BLUE, i.e. the best linear unbiased estimator. \\[Var(\\hat\\beta_0) \\le Var\\Big( \\sum_{i = 1}^n a_i Y_i \\Big) \\: \\forall a_i \\in \\mathbb{R} \\: \\text{s.t.} \\: E\\Big( \\sum_{i = 1}^n a_i Y_i \\Big) = \\beta_0\\] \\[Var(\\hat\\beta_1) \\le Var\\Big( \\sum_{i = 1}^n b_i Y_i \\Big) \\: \\forall b_i \\in \\mathbb{R} \\: \\text{s.t.} \\: E\\Big( \\sum_{i = 1}^n b_i Y_i \\Big) = \\beta_1\\] Proof (Bestness of beta1). Consider \\(\\Theta := \\bigg\\{ \\sum\\limits_{i = 1}^n b_i Y_i \\in \\mathbb{R} : E\\Big( \\sum\\limits_{i = 1}^n b_i Y_i \\Big) = \\beta_1 \\bigg\\}\\). Claim: \\(Var( \\sum b_i Y_i) - Var(\\hat\\beta_1) \\ge 0\\) Let \\(\\sum b_i Y_i \\in \\Theta\\). Then \\(E(\\sum b_i Y_i) = \\beta_1\\). Since \\(E(Y_i) = \\beta_0 + \\beta_1 x_i\\), \\[\\beta_0 \\sum b_i + \\beta_1 \\sum b_i x_i = \\beta_1\\] It gives \\[\\begin{equation} \\label{eq:ule} \\begin{cases} \\sum b_i = 0 \\\\ \\sum b_i x_i = 1 \\end{cases} \\end{equation}\\] Then \\[\\begin{equation*} \\begin{split} 0 \\le Var\\Big(\\sum b_i Y_i - \\hat\\beta_1\\Big) &amp; = Var\\Big( \\sum b_i Y_i - \\sum \\frac{(x_i - \\bar{x})}{S_{XX}} Y_i \\Big) \\\\ &amp; \\stackrel{indep}{=} \\sum \\bigg( b_i - \\frac{(x_i - \\bar{x})}{S_{XX}} \\bigg)^2 \\sigma^2 \\\\ &amp; = \\sum \\bigg( b_i^2 - \\frac{2b_i (x_i - \\bar{x})}{S_{XX}} + \\frac{(x_i - \\bar{x})^2}{S_{XX}^2} \\bigg) \\sigma^2 \\\\ &amp; = \\sum b_i^2 \\sigma^2 - \\frac{2 \\sigma^2}{S_{XX}} \\sum b_i x_i + \\frac{2 \\bar{x} \\sigma^2}{S_{XX}} \\sum b_i + \\sigma^2 \\frac{\\sum (x_i - \\bar{x})^2}{S_{XX}^2} \\\\ &amp; = \\sum b_i^2 \\sigma^2 - \\frac{\\sigma^2}{S_{XX}} \\qquad \\because \\eqref{eq:ule} \\:\\text{and}\\: S_{XX} = \\sum (x_i - \\bar{x})^2 \\\\ &amp; = Var(\\sum b_i Y_i) - Var(\\hat\\beta_1) \\end{split} \\end{equation*}\\] Hence, \\[Var(\\sum b_i Y_i) \\ge Var(\\hat\\beta_1)\\] Proof (Bestness of beta0). Consider \\(\\Theta := \\bigg\\{ \\sum\\limits_{i = 1}^n a_i Y_i \\in \\mathbb{R} : E\\Big( \\sum\\limits_{i = 1}^n a_i Y_i \\Big) = \\beta_0 \\bigg\\}\\). Claim: \\(Var( \\sum a_i Y_i) - Var(\\hat\\beta_0) \\ge 0\\) Let \\(\\sum a_i Y_i \\in \\Theta\\). Then \\(E(\\sum a_i Y_i) = \\beta_0\\). Since \\(E(Y_i) = \\beta_0 + \\beta_1 x_i\\), \\[\\beta_0 \\sum a_i + \\beta_1 \\sum a_i x_i = \\beta_0\\] It gives \\[\\begin{equation} \\label{eq:ule0} \\begin{cases} \\sum a_i = 1 \\\\ \\sum a_i x_i = 0 \\end{cases} \\end{equation}\\] Then \\[\\begin{equation*} \\begin{split} 0 \\le Var\\Big(\\sum a_iY_i - \\hat\\beta_0 \\Big) &amp; = Var\\bigg[\\sum a_iY_i - \\sum\\Big( \\frac{1}{n} - \\frac{(x_k - \\bar{x})\\bar{x}}{S_{XX}} \\Big) Y_k \\bigg] \\\\ &amp; = \\sum \\bigg(a_i - \\frac{1}{n} + \\frac{(x_i - \\bar{x})\\bar{x}}{S_{XX}} \\bigg)^2\\sigma^2 \\\\ &amp; = \\sum \\bigg[ a_i^2 - 2a_i\\Big( \\frac{1}{n} - \\frac{(x_i - \\bar{x})\\bar{x}}{S_{XX}} \\Big) + \\Big( \\frac{1}{n} - \\frac{(x_i - \\bar{x})\\bar{x}}{S_{XX}} \\Big)^2 \\bigg]\\sigma^2 \\\\ &amp; = \\sum a_i^2\\sigma^2 -\\frac{2\\sigma^2}{n}\\sum a_i + \\frac{2\\bar{x}\\sigma^2\\sum a_ix_i}{S_{XX}} - \\frac{2\\bar{x}^2\\sigma^2\\sum a_i}{S_{XX}} \\\\ &amp; \\qquad + \\sigma^2\\bigg( \\frac{1}{n} - \\frac{2\\bar{x}}{nS_{XX}} \\sum(x_i - \\bar{x}) + \\frac{\\bar{x}^2\\sum(x_i - \\bar{x})^2}{S_{XX}^2} \\bigg) \\\\ &amp; = \\sum a_i^2\\sigma^2 -\\frac{2\\sigma^2}{n} - \\frac{2\\bar{x}^2\\sigma^2}{S_{XX}} \\qquad \\because \\eqref{eq:ule0} \\\\ &amp; \\qquad + \\bigg(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{XX}} \\bigg)\\sigma^2 \\qquad \\because \\sum(x_i - \\bar{x}) = 0 \\: \\text{and} \\: S_{XX} := \\sum (x_i - \\bar{x})^2 \\\\ &amp; = \\sum a_i^2\\sigma^2 - \\bigg( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{XX}} \\bigg)\\sigma^2 \\\\ &amp; = Var\\Big( \\sum a_i Y_i \\Big) - Var\\hat\\beta_0 \\end{split} \\end{equation*}\\] Hence, \\[Var(\\sum a_i Y_i) \\ge Var(\\hat\\beta_0)\\] Example 1.1 Show that \\(\\sum (Y_i - \\hat{Y_i}) = 0\\), \\(\\sum x_i (Y_i - \\hat{Y_i}) = 0\\), and \\(\\sum \\hat{Y_i} (Y_i - \\hat{Y_i}) = 0\\). Solution. Consider the two normal equations (1.2) and (1.3). Note that \\(\\hat{Y_i} = \\hat\\beta_0 + \\hat\\beta_1 x_i\\). From the Equation (1.2), we have \\(\\sum (Y_i - \\hat{Y_i}) = 0\\). From the Equation (1.3), we have \\(\\sum x_i (Y_i - \\hat{Y_i}) = 0\\). It follows that \\[\\begin{equation*} \\begin{split} \\sum \\hat{Y_i} (Y_i - \\hat{Y_i}) &amp; = \\sum (\\hat\\beta_0 + \\hat\\beta_1 x_i) (Y_i - \\hat{Y_i}) \\\\ &amp; = \\hat\\beta_0 \\sum (Y_i - \\hat{Y_i}) + \\hat\\beta_1 \\sum x_i (Y_i - \\hat{Y_i}) \\\\ &amp; = 0 \\end{split} \\end{equation*}\\] 1.2.5 Estimation of \\(\\sigma^2\\) There is the last parameter, \\(\\sigma^2 = Var(Y_i)\\). In the least squares estimation literary, we estimate \\(\\sigma^2\\) by \\[\\begin{equation} \\label{eq:siglse} \\hat\\sigma^2 = \\frac{1}{n - 2} \\sum_{i = 1}^n (Y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i)^2 \\end{equation}\\] Why \\(n - 2\\)? This makes the estimator unbiased. Proposition 1.3 (Unbiasedness) \\[E(\\hat\\sigma^2) = \\sigma^2\\] Proof. Note that \\[(Y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i) = (Y_i - \\overline{Y}) - \\hat\\beta_1(x_i - \\overline{x})\\] Then \\[\\begin{equation*} \\begin{split} E(\\hat\\sigma^2) &amp; = \\frac{1}{n - 2} E \\bigg[ \\sum (Y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i)^2 \\bigg] \\\\ &amp; = \\frac{1}{n - 2} E \\bigg[ \\sum (Y_i - \\overline{Y})^2 + \\hat\\beta_1^2 \\sum (x_i - \\overline{x})^2 -2\\hat\\beta_1 \\sum (Y_i - \\overline{Y})(x_i - \\overline{x}) \\bigg] \\\\ &amp; = \\frac{1}{n - 2} E ( S_{YY} + \\hat\\beta_1^2 S_{XX} - 2 \\hat\\beta_1 S_{XY}) \\\\ &amp; = \\frac{1}{n - 2} E ( S_{YY} - \\hat\\beta_1^2 S_{XX}) \\qquad \\because S_{XY} = \\hat\\beta_1 S_{XX} \\\\ &amp; = \\frac{1}{n - 2} \\Big( \\underset{(a)}{\\underline{ES_{YY}}} - S_{XX} \\underset{(b)}{\\underline{E\\hat\\beta_1^2}} \\Big) \\end{split} \\end{equation*}\\] \\[\\begin{equation*} \\begin{split} ES_{YY} &amp; = E\\Big[ \\sum (Y_i - \\overline{Y})^2 \\Big] \\\\ &amp; = E \\Big[ \\sum \\Big( (\\beta_0 + \\beta_1 x_i + \\epsilon_i) - (\\beta_0 + \\beta_1 \\overline{x} + \\overline{\\epsilon}) \\Big)^2 \\Big] \\\\ &amp; = E \\Big[ \\sum \\Big( \\beta_1 (x_i - \\overline{x}) + (\\epsilon_i - \\overline{\\epsilon}) \\Big)^2 \\Big] \\\\ &amp; = \\beta_1^2 S_{XX} + E\\Big( \\sum (\\epsilon_i - \\overline{\\epsilon})^2 \\Big) + 2\\beta_1 \\sum (x_i - \\overline{x}) E(\\epsilon_i - \\overline{\\epsilon}) \\\\ &amp; = \\beta_1^2 S_{XX} + E\\Big( \\sum (\\epsilon_i - \\overline{\\epsilon})^2 \\Big) \\end{split} \\end{equation*}\\] Since \\(E(\\bar\\epsilon) = 0\\) and \\(Var(\\bar\\epsilon) = \\frac{\\sigma^2}{n}\\), \\[\\begin{equation*} \\begin{split} E\\Big( \\sum (\\epsilon_i - \\overline{\\epsilon})^2 \\Big) &amp; = E \\Big( \\sum (\\epsilon_i^2 + \\bar\\epsilon^2 - 2\\epsilon_i \\bar\\epsilon) \\Big) \\\\ &amp; = \\sum E(\\epsilon_i^2) - n E(\\bar\\epsilon^2) \\qquad \\because \\sum \\epsilon = n \\bar\\epsilon \\\\ &amp; = \\sum (Var(\\epsilon_i) + E(\\epsilon_i)^2) - n(Var(\\bar\\epsilon) + E(\\bar\\epsilon)^2) \\\\ &amp; = n\\sigma^2 - \\sigma^2 \\\\ &amp; = (n - 1)\\sigma^2 \\end{split} \\end{equation*}\\] Thus, \\[ES_{YY} = \\beta_1^2 S_{XX} + (n - 1)\\sigma^2\\] \\[\\begin{equation*} \\begin{split} E\\hat\\beta_1^2 &amp; = Var\\hat\\beta_1 + E(\\hat\\beta_1)^2 \\\\ &amp; = \\frac{\\sigma^2}{S_{XX}} + \\beta_1^2 \\end{split} \\end{equation*}\\] It follows that \\[\\begin{equation*} \\begin{split} E(\\hat\\sigma^2) &amp; = \\frac{1}{n - 2} \\Big( \\underset{(a)}{\\underline{ES_{YY}}} - S_{YY} \\underset{(b)}{\\underline{E\\hat\\beta_1^2}} \\Big) \\\\ &amp; = \\frac{1}{n - 2} \\bigg( \\Big(\\beta_1^2 S_{XX} + (n - 1)\\sigma^2 \\Big) - S_{XX}\\Big(\\frac{\\sigma^2}{S_{XX}} + \\beta_1^2 \\Big) \\bigg) \\\\ &amp; = \\frac{1}{n - 2}((n - 2)\\sigma^2) \\\\ &amp; = \\sigma^2 \\end{split} \\end{equation*}\\] 1.3 Maximum Likelihood Estimation In this section, we add an assumption to an random errors \\(\\epsilon_i\\). \\[\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\] Example 1.2 (Gaussian Likelihood) Note that \\(Y_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\). Then the likelihood function is \\[L(\\beta_0, \\beta_1, \\sigma^2) = \\prod_{i = 1}^n\\bigg( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\bigg(- \\frac{(Y_i - \\beta_0 - \\beta_1 x_i)^2}{2 \\sigma^2} \\bigg) \\bigg)\\] and so the log-likelihood function can be computed as \\[l(\\beta_0, \\beta_1, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i = 1}^n(Y_i - \\beta_0 - \\beta_1 x_i)^2\\] 1.3.1 Likelihood equations Definition 1.2 (Maximum Likelihood Estimator) \\[(\\hat\\beta_0^{MLE}, \\hat\\beta_1^{MLE}, \\hat\\sigma^{2MLE}) := \\arg\\sup L(\\beta_0, \\beta_1, \\sigma^2)\\] Since \\(l(\\cdot) = \\ln L(\\cdot)\\) is monotone, Remark. \\[(\\hat\\beta_0^{MLE}, \\hat\\beta_1^{MLE}, \\hat\\sigma^{2MLE}) = \\arg\\sup l(\\beta_0, \\beta_1, \\sigma^2)\\] We can find the maximum of this quadratic function by making first derivative. \\[\\begin{equation} \\frac{\\partial l}{\\partial \\beta_0} = \\frac{1}{\\sigma^2} \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 x_i) = 0 \\tag{1.7} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial l}{\\partial \\beta_1} = \\frac{1}{\\sigma^2} \\sum_{i = 1}^n x_i (Y_i - \\beta_0 - \\beta_1 x_i) = 0 \\tag{1.8} \\end{equation}\\] \\[\\begin{equation} \\frac{\\partial l}{\\partial \\sigma^2} = - \\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 x_i)^2 = 0 \\tag{1.9} \\end{equation}\\] Denote that Equations (1.7) and (1.8) given \\(\\hat\\sigma^2\\) are equivalent to the normal equations. Thus, \\[\\hat\\beta_0^{MLE} = \\hat\\beta_0^{LSE}, \\quad \\hat\\beta_1^{MLE} = \\hat\\beta_1^{LSE}\\] From Equation (1.9), \\[\\hat\\sigma^{2MLE} = \\frac{1}{n}\\sum_{i = 1}^n(Y_i - \\beta_0 - \\beta_1 x_i)^2 = \\frac{n - 2}{n} \\hat\\sigma^{2LSE}\\] Recall that \\(\\hat\\sigma^{2LSE}\\) is an unbiased, i.e. this MLE is not an unbiased estimator. Since \\(\\hat\\sigma^{2MLE} \\approx \\hat\\sigma^{2LSE}\\) for large \\(n\\), howerver, it is asymptotically unbiased. Theorem 1.2 (Rao-Cramer Lower Bound, univariate case) Let \\(X_1, \\ldots, X_n \\stackrel{iid}{\\sim} f(x ; \\theta)\\). If \\(\\hat\\theta\\) is an unbiased estimator of \\(\\theta\\), \\[Var(\\hat\\theta) \\ge \\frac{1}{I_n(\\theta)}\\] where \\(I_n(\\theta) = -E\\bigg(\\frac{\\partial^2 l(\\theta)}{\\partial \\theta^2} \\bigg)\\) To apply this theorem 1.2 in the simple linear regression setting, i.e. \\((\\beta_0, \\beta_1)\\), we need to look at the bivariate case. Theorem 1.3 (Rao-Cramer Lower Bound, bivariate case) Let \\(X_1, \\ldots, X_n \\stackrel{iid}{\\sim} f(x ; \\theta1, \\theta_2)\\) and let \\(\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)^T\\). If each \\(\\hat\\theta_1\\), \\(\\hat\\theta_2\\) is an unbiased estimator of \\(\\theta_1\\) and \\(\\theta_2\\), then \\[ Var(\\boldsymbol{\\theta}) := \\begin{bmatrix} Var(\\hat\\theta_1) &amp; Cov(\\hat\\theta_1, \\hat\\theta_2) \\\\ Cov(\\hat\\theta_1, \\hat\\theta_2) &amp; Var(\\hat\\theta_2) \\end{bmatrix} \\ge I_n^{-1}(\\theta_1, \\theta_2) \\] where \\[ I_n(\\theta_1, \\theta_2) = - \\begin{bmatrix} E\\bigg( \\frac{\\partial^2 l(\\theta_1, \\theta_2)}{\\partial \\theta_1^2} \\bigg) &amp; E\\bigg( \\frac{\\partial^2 l(\\theta_1, \\theta_2)}{\\partial \\theta_1 \\partial \\theta_2} \\bigg) \\\\ E\\bigg( \\frac{\\partial^2 l(\\theta_1, \\theta_2)}{\\partial \\theta_1 \\partial \\theta_2} \\bigg) &amp; E\\bigg( \\frac{\\partial^2 l(\\theta_1, \\theta_2)}{\\partial \\theta_2^2} \\bigg) \\end{bmatrix} \\] Assume that \\(\\sigma^2\\) is known. From the Equations (1.7) and (1.8), \\[ \\begin{cases} \\frac{\\partial^2 l}{\\partial \\beta_0^2} = - \\frac{n}{\\sigma^2} \\\\ \\frac{\\partial^2 l}{\\partial \\beta_1^2} = - \\frac{\\sum x_i^2}{\\sigma^2} \\\\ \\frac{\\partial^2 l}{\\partial \\beta_0 \\partial \\beta_1} = - \\frac{\\sum x_i}{\\sigma^2} \\end{cases} \\] Thus, \\[ I_n(\\beta_0, \\beta_1) = \\begin{bmatrix} \\frac{n}{\\sigma^2} &amp; \\frac{\\sum x_i}{\\sigma^2} \\\\ \\frac{\\sum x_i}{\\sigma^2} &amp; \\frac{\\sum x_i^2}{\\sigma^2} \\end{bmatrix} \\] Applying gaussian elimination, \\[\\begin{equation*} \\begin{split} \\left[ \\begin{array}{cc|cc} \\frac{n}{\\sigma^2} &amp; \\frac{\\sum x_i}{\\sigma^2} &amp; 1 &amp; 0 \\\\ \\frac{\\sum x_i}{\\sigma^2} &amp; \\frac{\\sum x_i^2}{\\sigma^2} &amp; 0 &amp; 1 \\end{array} \\right] &amp; \\leftrightarrow \\left[ \\begin{array}{cc|cc} \\frac{n}{\\sigma^2} &amp; \\frac{\\sum x_i}{\\sigma^2} &amp; 1 &amp; 0 \\\\ \\frac{\\sum x_i}{\\sigma^2}\\Big(\\frac{n}{\\sum x_i} \\Big) &amp; \\frac{\\sum x_i^2}{\\sigma^2}\\Big(\\frac{n}{\\sum x_i} \\Big) &amp; 0 &amp; \\frac{1}{\\overline{x}} \\end{array} \\right] \\\\ &amp; \\leftrightarrow \\left[ \\begin{array}{cc|cc} \\frac{n}{\\sigma^2} &amp; \\frac{\\sum x_i}{\\sigma^2} &amp; 1 &amp; 0 \\\\ 0 &amp; \\frac{\\sum x_i^2 - \\overline{x}\\sum x_i}{\\sigma^2\\overline{x}} = \\frac{S_{XX}}{\\sigma^2\\overline{x}} &amp; -1 &amp; \\frac{1}{\\overline{x}} \\end{array} \\right] \\\\ &amp; \\leftrightarrow \\left[ \\begin{array}{cc|cc} 1 &amp; \\overline{x} &amp; \\frac{\\sigma^2}{n} &amp; 0 \\\\ 0 &amp; 1 &amp; -\\frac{\\overline{x}}{S_{XX}}\\sigma^2 &amp; \\frac{\\sigma^2}{S_{XX}} \\end{array} \\right] \\\\ &amp; \\leftrightarrow \\left[ \\begin{array}{cc|cc} 1 &amp; 0 &amp; \\bigg(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{XX}} \\bigg)\\sigma^2 &amp; -\\frac{\\overline{x}}{S_{XX}}\\sigma^2 \\\\ 0 &amp; 1 &amp; -\\frac{\\overline{x}}{S_{XX}}\\sigma^2 &amp; \\frac{\\sigma^2}{S_{XX}} \\end{array} \\right] \\end{split} \\end{equation*}\\] Hence, \\[ I_n^{-1}(\\beta_0, \\beta_1) = \\begin{bmatrix} \\bigg(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{XX}} \\bigg)\\sigma^2 &amp; -\\frac{\\overline{x}}{S_{XX}}\\sigma^2 \\\\ -\\frac{\\overline{x}}{S_{XX}}\\sigma^2 &amp; \\frac{\\sigma^2}{S_{XX}} \\end{bmatrix} = \\begin{bmatrix} Var(\\hat\\beta_0) &amp; Cov(\\hat\\beta_0, \\hat\\beta_1) \\\\ Cov(\\hat\\beta_0, \\hat\\beta_1) &amp; Var(\\hat\\beta_1) \\end{bmatrix} \\] Since \\(Var(\\boldsymbol{\\hat\\beta}) - I^{-1} = 0\\) is non-negative definite, each \\(Var(\\hat\\beta_0) = \\bigg(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{XX}} \\bigg)\\sigma^2\\) and \\(Var(\\hat\\beta_1) = \\frac{\\sigma^2}{S_{XX}}\\) is a theoretical bound. Remark. This says that \\(\\hat\\beta_0^{LSE} = \\hat\\beta_0^{MLE}\\) and \\(\\hat\\beta_1^{LSE} = \\hat\\beta_1^{MLE}\\) have the smallest variance among all unbiased estimator. This result is stronger than Gauss-Markov theorem 1.1, where the LSE has the smalleset variance among all linear unbiased estimators. It can be simply obtained from the Lehmann-Scheffe Theorem: If some unbiased estimator is a function of complete sufficient statistic, then this estimator is the unique MVUE (Hogg, McKean, and Craig 2018). Remark (Lehmann and Scheffe for regression coefficients). \\(u\\Big(\\sum Y_i, S_{XY} \\Big)\\) is CSS in this regression problem, i.e. known \\(\\sigma^2\\). Proof. From the example 1.2, \\[\\begin{equation*} \\begin{split} L(\\beta_0, \\beta_1) &amp; = (2\\pi\\sigma^2)^{-\\frac{n}{2}}\\exp\\bigg[-\\frac{1}{2\\sigma^2} \\sum(Y_i - \\beta_0 - \\beta_1 x_i)^2 \\bigg] \\\\ &amp; = (2\\pi\\sigma^2)^{-\\frac{n}{2}}\\exp\\bigg[-\\frac{1}{2\\sigma^2} \\sum \\Big(Y_i^2 - (\\beta_0 + \\beta_1 x_i)Y_i + (\\beta_0 + \\beta_1 x_i)^2 \\Big) \\bigg] \\\\ &amp; = (2\\pi\\sigma^2)^{-\\frac{n}{2}}\\exp\\bigg[-\\frac{1}{2\\sigma^2} \\Big( -\\beta_0 \\sum Y_i - \\beta_1 \\sum x_i Y_i \\Big) \\bigg] \\exp\\bigg[-\\frac{1}{2\\sigma^2} \\Big( \\sum Y_i^2 + (\\beta_0 + \\beta_1 x_i)^2 \\Big) \\bigg] \\end{split} \\end{equation*}\\] By the Factorization theorem, both \\(\\sum Y_i\\) and \\(\\sum x_i Y_i\\) are sufficient statistics. Since \\(S_{XY}\\) is one-to-one function of \\(\\sum x_i Y_i\\), it is also a sufficient statistic. Denote that the normal distribution is in exponential family. Hence, \\((\\sum Y_i, S_{XY})\\) are CSS. 1.4 Residuals Definition 1.3 (Residuals) \\[e_i := Y_i - \\hat{Y_i}\\] 1.4.1 Prediction error delv %&gt;% mutate(yhat = predict(lm(y ~ x))) %&gt;% ggplot(aes(x = x, y = y)) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_point() + geom_linerange(aes(ymin = y, ymax = yhat), col = I(&quot;red&quot;), alpha = .7) + labs( x = &quot;Number of Cases&quot;, y = &quot;Delivery Time&quot; ) Figure 1.3: Fit and residuals See Figure 1.3. Each red line is \\(e_i\\). As we can see, \\(e_i\\) represents the difference between observed response and predicted response. A large \\(\\lvert e_i \\rvert\\) indicates a large prediction error. You can call this \\(e_i\\) for each \\(Y_i\\) by lm()$residuals or residuals(). delv_fit &lt;- lm(y ~ x, data = delv) delv_fit$residuals 1 2 3 4 5 6 7 8 9 10 -1.874 1.651 2.181 2.855 -2.628 -0.444 0.327 -0.724 10.634 7.298 11 12 13 14 15 16 17 18 19 20 2.191 -4.082 1.475 3.372 1.094 3.918 -1.028 0.446 -0.349 -5.216 21 22 23 24 25 -7.182 -7.581 -4.156 -0.900 -1.275 \\(\\sum e_i^2\\), which has been minimized in the procedure of LSE, can be used to see overall size of prediction errors. Definition 1.4 (Residual Sum of Squares) \\[SSE := \\sum_{i = 1}^n e_i^2\\] 1.4.2 Residuals and the variance \\(e_i\\) is a random quantity, which contains the information for \\(\\epsilon_i\\). \\(\\sum e_i^2\\) can give information about \\(\\sigma^2 = Var(\\epsilon_i)\\). For this, it is expected that \\(e_i\\) and \\(\\epsilon_i\\) have similar feature. Lemma 1.3 Covriance between Y and each coefficient \\(\\text{(a)}\\: Cov(\\hat\\beta_0, Y_i) = \\bigg( \\frac{1}{n} - \\frac{(x_i - \\overline{x})\\overline{x}}{S_{XX}} \\bigg)\\sigma^2\\) \\(\\text{(b)}\\: Cov(\\hat\\beta_1, Y_i) = \\frac{(x_i - \\overline{x})}{S_{XX}}\\sigma^2\\) Proof. (a) \\[\\begin{equation*} \\begin{split} Cov(\\hat\\beta_0, Y_i) &amp; = Cov(\\sum a_i Y_i, Y_i) \\\\ &amp; = \\bigg( \\frac{1}{n} - \\frac{(x_i - \\overline{x})\\overline{x}}{S_{XX}} \\bigg)\\sigma^2 \\end{split} \\end{equation*}\\] \\[\\begin{equation*} \\begin{split} Cov(\\hat\\beta_1, Y_i) &amp; = Cov(\\sum b_i Y_i, Y_i) \\\\ &amp; = \\frac{(x_i - \\overline{x})}{S_{XX}}\\sigma^2 \\end{split} \\end{equation*}\\] Proposition 1.4 (Properties of residuals) Mean and variance of the residual \\(\\text{(a)}\\: E(e_i) = 0\\) \\(\\text{(b)}\\: Var(e_i) \\neq \\sigma^2\\) \\(\\text{(c)}\\: \\forall i \\neq j : Cov(e_i, e_j) \\neq 0\\) Proof. (a) Recall that this is the assumption of the regression model. Lemma 1.3 implies that \\[\\begin{equation*} \\begin{split} Cov(\\overline{Y}, \\hat\\beta_1) &amp; = Cov(\\frac{1}{n}\\sum Y_i, \\hat\\beta_1) \\\\ &amp; = \\frac{1}{n} \\sum_{i = 1}^n \\frac{(x_i - \\overline{x})}{S_{XX}}\\sigma^2 \\\\ &amp; = 0 \\qquad \\because \\sum (x_i - \\overline{x}) = 0 \\end{split} \\end{equation*}\\] Then \\[\\begin{equation} \\begin{split} Var(\\hat{Y_i}) &amp; = Var(\\hat\\beta_0 + \\hat\\beta_1 x_i) \\\\ &amp; = Var \\bigg[ \\overline{Y} + (x_i - \\overline{x}) \\hat\\beta_1 \\bigg] \\qquad \\because \\hat\\beta_0 = \\overline{Y} - \\hat\\beta_1 \\overline{x} \\\\ &amp; = Var(\\overline{Y}) + (x_i - \\overline{x})^2 Var(\\hat\\beta_1) + 2(x_i - \\overline{x}) Cov(\\overline{Y}, \\hat\\beta_1) \\\\ &amp; = \\frac{\\sigma^2}{n} + (x_i - \\overline{x})^2\\frac{\\sigma^2}{S_{XX}} + 0 \\\\ &amp; = \\bigg( \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{S_{XX}} \\bigg)\\sigma^2 \\end{split} \\tag{1.10} \\end{equation}\\] From the same lemma 1.3, \\[\\begin{equation} \\begin{split} Cov(Y_i, \\hat{Y_i}) &amp; = Cov(Y_i, \\overline{Y} + (x_i - \\overline{x}) \\hat\\beta_1) \\\\ &amp; = Cov(Y_i, \\overline{Y}) + (x_i - \\overline{x}) Cov(Y_i, \\hat\\beta_1) \\\\ &amp; = \\frac{\\sigma^2}{n} + \\frac{(x_i - \\overline{x})^2}{S_{XX}}\\sigma^2 \\qquad \\because Cov(Y_i, \\hat\\beta_1) = \\frac{(x_i - \\overline{x})}{S_{XX}}\\sigma^2 \\\\ &amp; = \\bigg( \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{S_{XX}} \\bigg)\\sigma^2 \\end{split} \\tag{1.11} \\end{equation}\\] These Equations (1.10) and (1.11) give that \\[\\begin{equation*} \\begin{split} Var(e_i) &amp; = Var(Y_i) + Var(\\hat{Y_i}) -2Cov(Y_i, \\hat{Y_i}) \\\\ &amp; = \\sigma^2 + \\bigg( \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{S_{XX}} \\bigg)\\sigma^2 - 2 \\bigg( \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{S_{XX}} \\bigg)\\sigma^2 \\\\ &amp; = \\bigg(1 - \\frac{1}{n} - \\frac{(x_i - \\overline{x})^2}{S_{XX}} \\bigg)\\sigma^2 \\\\ &amp; \\neq \\sigma^2 \\end{split} \\end{equation*}\\] Let \\(i \\neq j\\). Then \\[\\begin{equation*} \\begin{split} Cov(e_i, e_j) &amp; = Cov\\Big( Y_i - (\\hat\\beta_0 + \\hat\\beta_1 x_i), Y_j - (\\hat\\beta_0 + \\hat\\beta_1 x_j) \\Big) \\\\ &amp; = Cov(Y_i, Y_j) - Cov\\Big(Y_i, (\\hat\\beta_0 + \\hat\\beta_1 x_j) \\Big) - Cov((\\hat\\beta_0 + \\hat\\beta_1 x_i), Y_j) + Cov((\\hat\\beta_0 + \\hat\\beta_1 x_i), (\\hat\\beta_0 + \\hat\\beta_1 x_j)) \\\\ &amp; = 0 - \\bigg( \\frac{1}{n} - \\frac{(x_i - \\overline{x})\\overline{x}}{S_{XX}} \\bigg)\\sigma^2 - \\frac{(x_i - \\overline{x})x_j}{S_{XX}}\\sigma^2 \\\\ &amp; \\qquad - \\bigg( \\frac{1}{n} - \\frac{(x_j - \\overline{x})\\overline{x}}{S_{XX}} \\bigg)\\sigma^2 - \\frac{(x_i - \\overline{x})x_i}{S_{XX}}\\sigma^2 \\\\ &amp; \\qquad + \\bigg( \\frac{1}{n} + \\frac{\\overline{x}^2 + x_i x_j - \\overline{x}(x_i + x_j)}{S_{XX}} \\bigg)\\sigma^2 \\\\ &amp; = - \\bigg( \\frac{1}{n} + \\frac{\\overline{x}^2 + x_i x_j - \\overline{x}(x_i + x_j)}{S_{XX}} \\bigg)\\sigma^2 \\\\ &amp; \\neq 0 \\end{split} \\end{equation*}\\] 1.5 Decomposition of Total Variability 1.5.1 Total sum of squares Definition 1.5 (Uncorrected Total Sum of Squares) \\[SST_{uncor} := \\sum_{i = 1}^n Y_i^2\\] Definition 1.6 (Corrected Total Sum of Squares) \\[SST := \\sum_{i = 1}^n (Y_i - \\overline{Y})^2\\] What does this total sum of squares mean? To know this, we should know \\(\\overline{Y}\\) first. delv %&gt;% ggplot(aes(x = x, y = y)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ 1, se = FALSE) + geom_point() + labs( x = &quot;Number of Cases&quot;, y = &quot;Delivery Time&quot; ) Figure 1.4: Regression without predictor See Figure 1.4. The line represents the closest line when we use only intercept term for the regression model. In other words, if we use no information for the response, i.e. no predictor variables, we will get just average of the response variable. Consider \\[Y_i = \\beta_0 + \\epsilon_i\\] Then we can get only one normal equation \\[\\sum (Y_i - \\hat\\beta_0) = 0\\] Hence, \\[\\hat\\beta_0 = \\frac{1}{n} \\sum_{i = 1}^n Y_i \\equiv \\overline{Y}\\] From this fact, \\(SST\\) implies total variance. 1.5.2 Regression sum of squares Definition 1.7 (Regression Sum of Squares) \\[SSR := \\sum_{i = 1}^n (hat{Y_i} - \\overline{Y})^2\\] This \\(SSR\\) compares \\(\\hat{Y_i}\\) versus \\(\\overline{Y}\\), computing the sum of squares for difference between predicted values from regression model and model not using predictors. 1.5.3 Residual sum of squares Now consider the residual sum of squares \\(SSE\\) in the definition 1.4. As mentioned, this is related to the prediction errors, which the regression model could not explain the data. 1.5.4 Decomposition of total sum of squares \\(SST\\) can be decomposed by construction of sum of squares. Proposition 1.5 (Decomposition of SST) \\[SST = SSR + SSE\\] where \\(SST = \\sum (Y_i - \\overline{Y})^2\\), \\(SSR = \\sum (\\hat{Y_i} - \\overline{Y})^2\\), and \\(SSE = \\sum (Y_i - \\hat{Y_i})\\) Proof. From the Example 1.1, \\[\\begin{equation*} \\begin{split} \\sum_{i = 1}^n (Y_i - \\overline{Y})^2 &amp; = \\sum_{i = 1}^n (Y_i - \\hat{Y_i} + \\hat{Y_i} - \\overline{Y})^2 \\\\ &amp; = \\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2 + 2 \\sum_{i = 1}^n (Y_i - \\hat{Y_i})(\\hat{Y_i} - \\overline{Y}) + \\sum_{i = 1}^n (\\hat{Y_i} - \\overline{Y})^2 \\\\ &amp; = \\sum_{i = 1}^n (Y_i - \\hat{Y_i})^2 + \\sum_{i = 1}^n (\\hat{Y_i} - \\overline{Y})^2 \\qquad \\because \\sum (Y_i - \\hat{Y_i}) = 0 \\: \\text{and} \\: \\sum (Y_i - \\hat{Y_i})\\hat{Y_i} = 0 \\end{split} \\end{equation*}\\] This represents each \\(SSR\\) and \\(SSE\\) divides total variability as following. \\[\\overset{SST}{\\text{total variability}} = \\overset{SSR}{\\text{left unexplained by regression}} + \\overset{SSE}{\\text{explained by regression}}\\] Denote that the total variability \\(SST\\) is constant given data set. If our model is good, \\(SSR\\) grows and \\(SSE\\) flattens. Thus the larger \\(SSR\\) is, the better. The lower \\(SSE\\) is, the better. 1.5.5 Coefficient of determination We have discussed in the previous section 1.5.4 that \\(SSR\\) and \\(SSE\\) splits the total variability into explained part and not-explained part by our regression model. Our first interest is whether the model works well for the data well, so we can think about the proportion of explained part to the total variance. The following measure \\(R^2\\) computes this kind of value. Definition 1.8 (Coefficient of Determination) \\[R^2 := \\frac{SSR}{SST} = 1 - \\frac{1 - SSE}{SST}\\] By construction, \\[0 \\le R^2 \\le 1\\] As \\(R^2\\) goes to \\(0\\), the model goes wrong. As \\(R^2\\) is close to \\(1\\), large proportion of variability has been explained. So we prefer large values rather than small. Proposition 1.6 \\(R^2\\) shows the strength of linear relation between two variables \\(x\\) and \\(Y\\) in the simple linear regression. \\[R^2 = \\hat\\rho_{XY}\\] where \\(\\hat\\rho_{XY} := \\frac{\\sum (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sqrt{\\sum (X_i - \\overline{X})^2} \\sqrt{\\sum (Y_i - \\overline{Y})^2}}\\) is the sample correlation coefficients Proof. Note that \\(\\hat{Y_i} - \\overline{Y} = \\hat\\beta_1 (x_i - \\overline{x}) = \\frac{S_{XY}}{S_{XX}} (x_i - \\overline{x})\\). Then \\[\\begin{equation*} \\begin{split} \\sum (\\hat{Y_i} - \\overline{Y})^2 &amp; = \\frac{S_{XY}^2}{S_{XX}^2} \\sum (x_i - \\overline{x})^2 \\\\ &amp; = \\frac{S_{XY}^2}{S_{XX}} \\end{split} \\end{equation*}\\] It follows that \\[\\begin{equation*} \\begin{split} R^2 &amp; = \\frac{\\sum (\\hat{Y_i} - \\overline{Y})^2}{\\sum (Y_i - \\overline{Y})^2} \\\\ &amp; = \\frac{S_{XY}^2}{S_{XX}S_{YY}} \\\\ &amp; =: \\hat\\rho_{XY}^2 \\end{split} \\end{equation*}\\] In this relation, we can know that \\(R^2\\) statistic performs as a measure of the linear relationship in the simple linear regression setting. 1.6 Geometric Interpretations 1.6.1 Fundamental subspaces These linear algebra concepts might be more useful for multiple linear regression, but let’s briefly recap (Leon 2014). Definition 1.9 (Fundamental Subspaces) Let \\(X \\in \\mathbb{R}^{n \\times (p + 1)}\\). Then the Null space is defined by \\[N(X) := \\{ \\mathbf{b} \\in \\mathbb{R}^n \\mid X\\mathbf{b} = \\mathbf{0} \\}\\] The Row space is defined by \\[Row(X) := sp(\\{\\mathbf{r}_1, \\ldots, \\mathbf{r}_{p + 1} \\}) \\quad \\text{where} \\: X^T = [\\mathbf{r}_1^T, \\ldots, \\mathbf{r}_{n}^T]\\] The Column space is defined by \\[Col(X) := sp(\\{\\mathbf{c}_1, \\ldots, \\mathbf{c}_{n} \\}) \\quad \\text{where} \\: X = [\\mathbf{c}_1, \\ldots, \\mathbf{c}_{p + 1}]\\] The Range of \\(X\\) is defined by \\[R(X) := \\{ \\mathbf{y} \\in \\mathbb{R}^n \\mid \\mathbf{y} = X\\mathbf{b} \\quad \\text{for some} \\: \\mathbf{b} \\in \\mathbb{R}^{p + 1} \\}\\] These spaces have some constructional relationship. Theorem 1.4 (Fundamental Subspaces Theorem) Let \\(X \\in \\mathbb{R}^{n \\times (p + 1)}\\). Then \\[N(X) = R(X^T)^{\\perp} = Col(X^T)^{\\perp} = Row(X)^{\\perp}\\] Transposed matrix also satisfy this. \\[N(X^T) = R(X)^{\\perp} = Col(X)^{\\perp}\\] Proof. Let \\(\\mathbf{a} \\in N(X)\\). Then \\(X\\mathbf{a} = \\mathbf{0}\\). Let \\(\\mathbf{y} \\in R(X^T)\\). Then \\(X^T \\mathbf{b} = \\mathbf{y}\\) for some \\(\\mathbf{b} \\in \\mathbb{R}^{p + 1}\\). Choose \\(\\mathbf{b} \\in \\mathbb{R}^{p + 1}\\) such that \\(X^T \\mathbf{b} = \\mathbf{y}\\). Then \\[\\begin{equation*} \\begin{split} \\mathbf{0} &amp; = X\\mathbf{a} \\\\ &amp; = \\mathbf{b}^T X\\mathbf{a} \\\\ &amp; = \\mathbf{y}^T \\mathbf{a} \\end{split} \\end{equation*}\\] Hence, \\[N(X) \\perp R(X^T)\\] Since \\[X^T \\mathbf{b} = \\mathbf{c}_1 \\mathbf{b} + \\cdots + \\mathbf{c}_{p + 1} \\mathbf{b}\\] it is trivial that \\(R(X) = Col(X)\\) and \\(R(X^T) = Col(X^T)\\). If \\(\\mathbf{a} \\in N(X)\\), then \\[ X\\mathbf{a} = \\begin{bmatrix} \\mathbf{r}_1 \\\\ \\mathbf{r}_2 \\\\ \\cdots \\\\ \\mathbf{r}_n \\end{bmatrix} \\begin{bmatrix} a_1 \\\\ \\cdots \\\\ a_{p + 1} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\cdots \\\\ 0 \\end{bmatrix} \\] Thus, \\[\\forall i : \\mathbf{a}^T \\mathbf{r}_i = 0\\] and so \\[N(X) \\subseteq Row(X)^{\\perp}\\] Conversely, if \\(\\mathbf{a} \\in Row(X)^{\\perp}\\), then \\(\\forall i : \\mathbf{a}^T \\mathbf{r}_i = 0\\). This implies that \\(X\\mathbf{a} = \\mathbf{0}\\). Thus, \\[Row(X)^{\\perp} \\subseteq N(X)\\] and so \\[N(X) = Row(X)^{\\perp}\\] \\(N(X^T) = R(X)^{\\perp}\\) part in Theorem 1.4 will give the geometric insight to least squares solution. Theorem 1.5 Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). Then \\[dim S + dim S^{\\perp} = n\\] If \\(\\{ \\mathbf{x}_1, \\ldots, \\mathbf{r} \\}\\) is a basis for \\(S\\) and \\(\\{ \\mathbf{x}_{r + 1}, \\ldots, \\mathbf{n} \\}\\) is a basis for \\(S^{\\perp}\\), then \\(\\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_r, \\mathbf{x}_{r + 1}, \\ldots, \\mathbf{n} \\}\\) is a basis for \\(\\mathbb{R}^n\\). Theorem 1.6 Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). Then \\[\\mathbb{R}^n = S \\oplus S^{\\perp}\\] 1.6.2 Simple linear regression Theorem 1.7 Let \\(S\\) be a subspace of \\(\\mathbb{R}^n\\). For each \\(\\mathbf{y} \\in \\mathbf{R}^n\\), there exists a unique \\(\\mathbf{p} \\in S\\) that is closest to \\(\\mathbf{y}\\), i.e. \\[\\Vert \\mathbf{y} - \\mathbf{p} \\Vert &gt; \\Vert \\mathbf{y} - \\mathbf{\\hat{y}} \\Vert\\] for any \\(\\mathbf{p} \\neq \\mathbf{\\hat{y}}\\). Furthermore, a given vector \\(\\mathbf{p} \\in S\\) will be the closest to a given vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\) if and only if \\[\\mathbf{y} - \\mathbf{\\hat{y}} \\in S^{\\perp}\\] Least square estimator \\((\\hat\\beta_0, \\hat\\beta_1)^T\\) minimizes \\[\\sum_{i = 1}^n (Y_i - \\beta_0 - \\beta_1 x_i)^2 = \\Vert \\mathbf{Y} - (\\beta_0 \\mathbf{1} + \\beta_1 \\mathbf{x}) \\Vert^2\\] with respect to \\((\\hat\\beta_0, \\hat\\beta_1)^T \\in \\mathbb{R}^2\\) (where \\(\\mathbf{1} := (1, 1)^T\\)). Recall that the normal equation gives \\[\\sum_{i = 1}^n(Y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i) = \\Big( \\mathbf{Y} - (\\hat\\beta_0 \\mathbf{1} + \\hat\\beta_1 \\mathbf{x}) \\Big)^T \\mathbf{1} = 0\\] and \\[\\sum_{i = 1}^n (Y_i - \\hat\\beta_0 - \\hat\\beta_1 x_i)x_i = \\Big( \\mathbf{Y} - (\\hat\\beta_0 \\mathbf{1} + \\hat\\beta_1 \\mathbf{x}) \\Big)^T \\mathbf{x} = 0\\] These two relation give \\[\\mathbf{Y} - (\\hat\\beta_0 \\mathbf{1} + \\hat\\beta_1 \\mathbf{x}) \\perp sp(\\{ \\mathbf{1}, \\mathbf{x} \\})^{\\perp}\\] i.e. \\(\\mathbf{\\hat{Y}} = \\hat\\beta_0 \\mathbf{1} + \\hat\\beta_1 \\mathbf{x}\\) is the projection of \\(\\mathbf{Y}\\). Theorem 1.7 can give the same result. \\[\\hat\\beta_0 \\mathbf{1} + \\hat\\beta_1 \\mathbf{x} \\in R([\\mathbf{1}, \\mathbf{x}])^{\\perp} = sp(\\{ \\mathbf{1}, \\mathbf{x} \\})^{\\perp}\\] Figure 1.5: Geometric Illustration of Simple Linear Regression We can see the details from Figure 1.5. In fact, decomposition of \\(SST\\) and \\(R^2\\) are also in here. Figure 1.6: Geometric Illustration of Decomposing SST See Figure 1.6. \\[ \\begin{cases} SST = \\lVert \\mathbf{Y} - \\overline{Y} \\mathbf{1} \\rVert^2 \\\\ SSR = \\lVert \\mathbf{\\hat{Y}} - \\overline{Y} \\mathbf{1} \\rVert^2 \\\\ SSE = \\lVert \\mathbf{Y} - \\mathbf{\\hat{Y}} \\rVert^2 \\end{cases} \\] Pythagorean law implies that \\[SST = SSR + SSE\\] Also, \\[R^2 = \\frac{SSR}{SST} = cos^2\\theta = \\hat\\rho_{XY}^2\\] 1.7 Distributions "]
]
