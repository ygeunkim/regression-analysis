<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 1 Simple Linear Regression | R Lab for Regression Analysis</title>
  <meta name="description" content="This aims at covering materials of regression analysis with R programming.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 1 Simple Linear Regression | R Lab for Regression Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="cover.png" />
  <meta property="og:description" content="This aims at covering materials of regression analysis with R programming." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Simple Linear Regression | R Lab for Regression Analysis" />
  
  <meta name="twitter:description" content="This aims at covering materials of regression analysis with R programming." />
  <meta name="twitter:image" content="cover.png" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="multiple.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Lab for Regression Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#linear-regression-analysis"><i class="fa fa-check"></i>Linear Regression Analysis</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#relation"><i class="fa fa-check"></i>Relation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="simple.html"><a href="simple.html#model"><i class="fa fa-check"></i><b>1.1</b> Model</a></li>
<li class="chapter" data-level="1.2" data-path="simple.html"><a href="simple.html#least-squares-estimation"><i class="fa fa-check"></i><b>1.2</b> Least Squares Estimation</a><ul>
<li class="chapter" data-level="1.2.1" data-path="simple.html"><a href="simple.html#normal-equations"><i class="fa fa-check"></i><b>1.2.1</b> Normal equations</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple.html"><a href="simple.html#prediction-and-mean-response"><i class="fa fa-check"></i><b>1.2.2</b> Prediction and Mean response</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple.html"><a href="simple.html#lseprop"><i class="fa fa-check"></i><b>1.2.3</b> Properties of LSE</a></li>
<li class="chapter" data-level="1.2.4" data-path="simple.html"><a href="simple.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>1.2.4</b> Gauss-Markov Theorem</a></li>
<li class="chapter" data-level="1.2.5" data-path="simple.html"><a href="simple.html#estimation-of-sigma2"><i class="fa fa-check"></i><b>1.2.5</b> Estimation of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple.html"><a href="simple.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>1.3</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="1.3.1" data-path="simple.html"><a href="simple.html#likelihood-equations"><i class="fa fa-check"></i><b>1.3.1</b> Likelihood equations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple.html"><a href="simple.html#residuals"><i class="fa fa-check"></i><b>1.4</b> Residuals</a><ul>
<li class="chapter" data-level="1.4.1" data-path="simple.html"><a href="simple.html#prediction-error"><i class="fa fa-check"></i><b>1.4.1</b> Prediction error</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple.html"><a href="simple.html#residuals-and-the-variance"><i class="fa fa-check"></i><b>1.4.2</b> Residuals and the variance</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple.html"><a href="simple.html#decomposition-of-total-variability"><i class="fa fa-check"></i><b>1.5</b> Decomposition of Total Variability</a><ul>
<li class="chapter" data-level="1.5.1" data-path="simple.html"><a href="simple.html#total-sum-of-squares"><i class="fa fa-check"></i><b>1.5.1</b> Total sum of squares</a></li>
<li class="chapter" data-level="1.5.2" data-path="simple.html"><a href="simple.html#regression-sum-of-squares"><i class="fa fa-check"></i><b>1.5.2</b> Regression sum of squares</a></li>
<li class="chapter" data-level="1.5.3" data-path="simple.html"><a href="simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.5.3</b> Residual sum of squares</a></li>
<li class="chapter" data-level="1.5.4" data-path="simple.html"><a href="simple.html#decompsst"><i class="fa fa-check"></i><b>1.5.4</b> Decomposition of total sum of squares</a></li>
<li class="chapter" data-level="1.5.5" data-path="simple.html"><a href="simple.html#coefficient-of-determination"><i class="fa fa-check"></i><b>1.5.5</b> Coefficient of determination</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple.html"><a href="simple.html#matnot"><i class="fa fa-check"></i><b>1.6</b> Geometric Interpretations</a><ul>
<li class="chapter" data-level="1.6.1" data-path="simple.html"><a href="simple.html#fundamental-subspaces"><i class="fa fa-check"></i><b>1.6.1</b> Fundamental subspaces</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple.html"><a href="simple.html#simple-linear-regression"><i class="fa fa-check"></i><b>1.6.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="1.6.3" data-path="simple.html"><a href="simple.html#solproj"><i class="fa fa-check"></i><b>1.6.3</b> Projection mapping</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="simple.html"><a href="simple.html#simpledist"><i class="fa fa-check"></i><b>1.7</b> Distributions</a><ul>
<li class="chapter" data-level="1.7.1" data-path="simple.html"><a href="simple.html#mean-response-and-response"><i class="fa fa-check"></i><b>1.7.1</b> Mean response and response</a></li>
<li class="chapter" data-level="1.7.2" data-path="simple.html"><a href="simple.html#simplebdist"><i class="fa fa-check"></i><b>1.7.2</b> Regression coefficients</a></li>
<li class="chapter" data-level="1.7.3" data-path="simple.html"><a href="simple.html#mean-response"><i class="fa fa-check"></i><b>1.7.3</b> Mean response</a></li>
<li class="chapter" data-level="1.7.4" data-path="simple.html"><a href="simple.html#response"><i class="fa fa-check"></i><b>1.7.4</b> Response</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="simple.html"><a href="simple.html#statistical-inference"><i class="fa fa-check"></i><b>1.8</b> Statistical Inference</a><ul>
<li class="chapter" data-level="1.8.1" data-path="simple.html"><a href="simple.html#confidence-interval"><i class="fa fa-check"></i><b>1.8.1</b> Confidence interval</a></li>
<li class="chapter" data-level="1.8.2" data-path="simple.html"><a href="simple.html#prediction-interval"><i class="fa fa-check"></i><b>1.8.2</b> Prediction interval</a></li>
<li class="chapter" data-level="1.8.3" data-path="simple.html"><a href="simple.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.8.3</b> Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="simple.html"><a href="simple.html#analysis-of-variance"><i class="fa fa-check"></i><b>1.9</b> Analysis of Variance</a><ul>
<li class="chapter" data-level="1.9.1" data-path="simple.html"><a href="simple.html#useful-distributions"><i class="fa fa-check"></i><b>1.9.1</b> Useful distributions</a></li>
<li class="chapter" data-level="1.9.2" data-path="simple.html"><a href="simple.html#quadratic-form"><i class="fa fa-check"></i><b>1.9.2</b> Quadratic form</a></li>
<li class="chapter" data-level="1.9.3" data-path="simple.html"><a href="simple.html#anova-for-testing-significance-of-regression"><i class="fa fa-check"></i><b>1.9.3</b> ANOVA for testing significance of regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple.html"><a href="multiple.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple.html"><a href="multiple.html#model-1"><i class="fa fa-check"></i><b>2.1</b> Model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple.html"><a href="multiple.html#least-square-estimation"><i class="fa fa-check"></i><b>2.2</b> Least Square Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="multiple.html"><a href="multiple.html#normal-equation"><i class="fa fa-check"></i><b>2.2.1</b> Normal equation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple.html"><a href="multiple.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>2.2.2</b> Orthogonal decomposition</a></li>
<li class="chapter" data-level="2.2.3" data-path="multiple.html"><a href="multiple.html#gram-schmidt-qr-factorization"><i class="fa fa-check"></i><b>2.2.3</b> Gram-Schmidt QR factorization</a></li>
<li class="chapter" data-level="2.2.4" data-path="multiple.html"><a href="multiple.html#successive-orthogonalization"><i class="fa fa-check"></i><b>2.2.4</b> Successive orthogonalization</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/ygeunkim/regression-analysis" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R Lab for Regression Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">





<span class="math inline">\(\DeclareMathOperator*{\argmin}{argmin}\)</span>
<div id="simple" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Simple Linear Regression</h1>
<div id="model" class="section level2">
<h2><span class="header-section-number">1.1</span> Model</h2>
<pre class="sourceCode r"><code class="sourceCode r">delv &lt;-<span class="st"> </span>MPV<span class="op">::</span>p2<span class="fl">.9</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tbl_df</span>()</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Number of Cases&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Delivery Time&quot;</span>
  )</code></pre>
<div class="figure" style="text-align: center"><span id="fig:delivery"></span>
<img src="regression-analysis_files/figure-html/delivery-1.png" alt="The Delivery Time Data" width="70%" />
<p class="caption">
Figure 1.1: The Delivery Time Data
</p>
</div>
<p>Given data <span class="math inline">\((x_1, Y_1), \ldots, (x_n, Y_n)\)</span>, we try to fit linear model</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]</span></p>
<p>Here <span class="math inline">\(\epsilon_i\)</span> is a error term, which is a random variable.</p>
<p><span class="math display">\[\epsilon \stackrel{iid}{\sim} (0, \sigma^2)\]</span></p>
<p>It gives the problem of estimating three parameters <span class="math inline">\((\beta_0, \beta_1, \sigma^2)\)</span>. Before estimating these, we set some assumptions.</p>
<ol style="list-style-type: decimal">
<li>linear relationship</li>
<li><span class="math inline">\(\epsilon_i\)</span>s are independent</li>
<li><span class="math inline">\(\epsilon_i\)</span>s are identically destributed, i.e. <em>constant variance</em></li>
<li>In some setting, <span class="math inline">\(\epsilon_i \sim N\)</span></li>
</ol>
</div>
<div id="least-squares-estimation" class="section level2">
<h2><span class="header-section-number">1.2</span> Least Squares Estimation</h2>
<div class="figure" style="text-align: center"><span id="fig:lsefig"></span>
<img src="regression-analysis_files/figure-html/lsefig-1.png" alt="Idea of the least square estimation" width="70%" />
<p class="caption">
Figure 1.2: Idea of the least square estimation
</p>
</div>
<p>We try to find <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the sum of squares of the vertical distances, i.e.</p>
<p><span class="math display" id="eq:ssq">\[\begin{equation}
  (\beta_0, \beta_1) = \arg\min \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2
  \tag{1.1}
\end{equation}\]</span></p>
<div id="normal-equations" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Normal equations</h3>
<p>Denote that Equation <a href="simple.html#eq:ssq">(1.1)</a> is quadratic. Then we can find its minimum by find the zero point of the first derivative. Set</p>
<p><span class="math display">\[Q(\beta_0, \beta_1) := \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2\]</span></p>
<p>Then we have</p>
<p><span class="math display" id="eq:normbeta0">\[\begin{equation}
  \frac{\partial Q}{\partial \beta_0} = -2 \sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i) = 0
  \tag{1.2}
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display" id="eq:normbeta1">\[\begin{equation}
  \frac{\partial Q}{\partial \beta_1} = -2 \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)x_i = 0
  \tag{1.3}
\end{equation}\]</span></p>
<p>From Equation <a href="simple.html#eq:normbeta0">(1.2)</a>,</p>
<p><span class="math display">\[\sum_{i = 1}^n Y_i - n \hat\beta_0 - \hat\beta_1 \sum_{i = 1}^n x_i = 0\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x}\]</span></p>
<p>Equation <a href="simple.html#eq:normbeta1">(1.3)</a> gives</p>
<p><span class="math display">\[\sum_{i = 1}^n x_i (Y_i - \overline{Y} + \hat\beta_1\overline{x} - \hat\beta_1 x_i) = \sum_{i = 1}^n x_i(Y_i - \overline{Y}) - \hat\beta_1\sum_{i = 1}^n x_i (x_i - \overline{x}) = 0\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\hat\beta_1 = \frac{\sum\limits_{i = 1}^nx_i(Y_i - \overline{Y})}{\sum\limits_{i = 1}^n x_i (x_i - \overline{x})}\]</span></p>

<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> <span class="math display">\[\hat\beta_1 = \frac{S_{XY}}{S_{XX}}\]</span></p>
where <span class="math inline">\(S_{XX} := \sum\limits_{i = 1}^n (x_i - \overline{x})^2\)</span> and <span class="math inline">\(S_{XY} := \sum\limits_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y})\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that <span class="math inline">\(\overline{x}^2 = \frac{1}{n^2}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2\)</span>. Then we have</p>
<p><span class="math display" id="eq:sxx">\[\begin{equation}
  \begin{split}
    S_{XX} &amp; = \sum_{i = 1}^n (x_i - \overline{x})^2 \\
    &amp; = \sum_{i = 1}^n x_i^2 - 2\sum_{i = 1}^n x_i \overline{x} + \sum_{i = 1}^n\overline{x}^2 \\
    &amp; = \sum_{i = 1}^n x_i^2 - \frac{2}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 + \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 \\
    &amp; = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2
  \end{split}
  \tag{1.4}
\end{equation}\]</span></p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \hat\beta_1 &amp; = \frac{\sum x_i(Y_i - \overline{Y})}{\sum x_i (x_i - \overline{x})} \\
    &amp; = \frac{\sum x_i (Y_i - \overline{Y}) - \overline{x}\sum (Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \qquad \because \sum (Y_i - \overline{Y}) = 0 \\
    &amp; = \frac{\sum (x_i - \overline{x})(Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \\
    &amp; = \frac{S_{XY}}{S_{XX}}
  \end{split}
\end{equation*}\]</span>
</div>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> delv)</code></pre>
<pre><code>
Call:
lm(formula = y ~ x, data = delv)

Coefficients:
(Intercept)            x  
       3.32         2.18  </code></pre>
</div>
<div id="prediction-and-mean-response" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Prediction and Mean response</h3>
<blockquote>
<p>“Essentially, all models are wrong, but some are useful.”</p>
<p>—George Box</p>
</blockquote>
<p>Recall that we have assumed the <strong>linear assumption</strong> between the predictor and the response variables, i.e. the true model. Estimating <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is same as estimating the <em>assumed true model</em>.</p>

<div class="definition">
<span id="def:eyx" class="definition"><strong>Definition 1.1  (Mean response)  </strong></span><span class="math display">\[E(Y \mid X = x) = \beta_0 + \beta_1 x\]</span>
</div>

<p>We can estimate this mean resonse by</p>
<p><span class="math display" id="eq:meanres">\[\begin{equation}
  \widehat{E(Y \mid x)} = \hat\beta_0 + \hat\beta_1 x
  \tag{1.5}
\end{equation}\]</span></p>
<p>However, in practice, the model might not be true, which is included in <span class="math inline">\(\epsilon\)</span> term.</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]</span></p>
<p>Our real problem is predicting individual <span class="math inline">\(Y\)</span>, not the mean. The <em>prediction</em> of response can be done by</p>
<p><span class="math display" id="eq:indpred">\[\begin{equation}
  \hat{Y_i}  = \hat\beta_0 + \hat\beta_1 x_i
  \tag{1.6}
\end{equation}\]</span></p>
<p>Observe that the values of Equations <a href="simple.html#eq:meanres">(1.5)</a> and <a href="simple.html#eq:indpred">(1.6)</a> are same. However, due to the <strong>error term in the prediction</strong>, it has larger standard error.</p>
</div>
<div id="lseprop" class="section level3">
<h3><span class="header-section-number">1.2.3</span> Properties of LSE</h3>
<p>Parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> have some properties related to the expectation and variance. We can notice that these lse’s are <strong>unbiased linear estimator</strong>. In fact, these are the <em>best unbiased linear estimator</em>. This will be covered in the Gauss-Markov theorem.</p>

<div class="lemma">
<p><span id="lem:sxy" class="lemma"><strong>Lemma 1.1  </strong></span><span class="math display">\[S_{XX} = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 = \sum_{i = 1}^n x_i(x_i - \overline{x})\]</span></p>
<span class="math display">\[S_{XY} = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg) = \sum_{i = 1}^n Y_i(x_i - \overline{x})\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We already proven the first part of <span class="math inline">\(S_{XX}\)</span>. See the Equation <a href="simple.html#eq:sxx">(1.4)</a>. The second part is tivial. Since <span class="math inline">\(\sum (x_i - \overline{x}) = 0\)</span>,</p>
<p><span class="math display">\[S_{XX} = \sum_{i = 1}^n (x_i - \overline{x})^2 = \sum_{i = 1}^n (x_i - \overline{x})x_i\]</span></p>
<p>For the first part of <span class="math inline">\(S_{XY}\)</span>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    S_{XY} &amp; = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    &amp; = \sum_{i = 1}^n x_i Y_i - \overline{x} \sum_{i = 1}^n Y_i - \overline{Y} \sum_{i = 1}^n x_i + n \overline{x} \overline{Y} \\
    &amp; = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg)
  \end{split}
\end{equation*}\]</span></p>
<p>Second part of <span class="math inline">\(S_{XY}\)</span> also can be proven from the definition.</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    S_{XY} &amp; = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    &amp; = \sum_{i = 1}^n Y_i (x_i - \overline{x}) - \overline{Y} \sum_{i = 1}^n (x_i - \overline{x}) \\
    &amp; = \sum_{i = 1}^n Y_i (x_i - \overline{x}) \qquad \because \sum_{i = 1}^n (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}\]</span>
</div>


<div class="lemma">
<p><span id="lem:linbet" class="lemma"><strong>Lemma 1.2  (Linearity)  </strong></span>Each coefficient is a linear estimator.</p>
<p><span class="math display">\[\hat\beta_1 = \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i\]</span></p>
<span class="math display">\[\hat\beta_0 = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})}{S_{XX}} \bigg) Y_i\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From lemma <a href="simple.html#lem:sxy">1.1</a>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    \hat\beta_1 &amp; = \frac{S_{XY}}{S_{XX}} \\
    &amp; = \frac{1}{S_{XX}}\sum_{i = 1}^n (x_i - \overline{x}) Y_i
  \end{split}
\end{equation*}\]</span></p>
<p>It gives that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \hat\beta_0 &amp; = \overline{Y} - \hat\beta_1 \overline{x} \\
    &amp; = \frac{1}{n}\sum_{i = 1}^n Y_i - \overline{x} \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i \\
    &amp; = \sum_{i = 1}^n\bigg(\frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)Y_i
  \end{split}
\end{equation*}\]</span>
</div>


<div class="proposition">
<p><span id="prp:ue" class="proposition"><strong>Proposition 1.1  (Unbiasedness)  </strong></span>Both coefficients are unbiased.</p>
<p><span class="math inline">\(\text{(a)}\: E\hat\beta_1 = \beta_1\)</span></p>
<span class="math inline">\(\text{(b)}\: E\hat\beta_0 = \beta_0\)</span>
</div>

<p>From the model, <span class="math inline">\(Y_1, \ldots, Y_n \stackrel{indep}{\sim} (\beta_0 + \beta_1 x_i, \sigma^2)\)</span>.</p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From lemma <a href="simple.html#lem:sxy">1.1</a>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    E\hat\beta_1 &amp; = \sum_{i = 1}^n \bigg[ \frac{(x_i - \overline{x})}{S_{XX}} E(Y_i) \bigg] \\
    &amp; = \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}(\beta_0 + \beta_1 x_i) \\
    &amp; = \frac{\beta_1 \sum (x_i - \overline{x})x_i}{\sum (x_i - \overline{x})x_i} \qquad \because \sum (x_i - \overline{x}) = 0 \\
    &amp; = \beta_1
  \end{split}
\end{equation*}\]</span></p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    E\hat\beta_0 &amp; = E(\overline{Y} - \hat\beta_1 \overline{x}) \\
    &amp; = E(\overline{Y}) - \overline{x}E(\hat\beta_1) \\
    &amp; = E(\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) - \beta_1 \overline{x} \\
    &amp; = \beta_0 + \beta_1 \overline{x} - \beta_1 \overline{x} \\
    &amp; = \beta_0
  \end{split}
\end{equation*}\]</span>
</div>


<div class="proposition">
<p><span id="prp:vb" class="proposition"><strong>Proposition 1.2  (Variances)  </strong></span>Variances and covariance of coefficients</p>
<p><span class="math inline">\(\text{(a)}\: Var\hat\beta_1 = \frac{\sigma^2}{S_{XX}}\)</span></p>
<p><span class="math inline">\(\text{(b)}\: Var\hat\beta_0 = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2\)</span></p>
<span class="math inline">\(\text{(c)}\: Cov(\hat\beta_0, \hat\beta_1) = - \frac{\overline{x}}{S_{XX}} \sigma^2\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Proving is just arithmetic.</p>
<ol style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Var\hat\beta_1 &amp; = \frac{1}{S_{XX}^2}\sum_{i = 1}^n \bigg[ (x_i - \overline{x})^2 Var(Y_i) \bigg] + \frac{1}{S_{XX}^2} \sum_{j \neq k}^n \bigg[ (x_j - \overline{x})(x_k - \overline{x}) Cov(Y_j, Y_k) \bigg] \\
    &amp; = \frac{\sigma^2}{S_{XX}} \qquad \because Cov(Y_j, Y_k) = 0 \: \text{if} \: j \neq k
  \end{split}
\end{equation*}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Var\hat\beta_0 &amp; = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)^2Var(Y_i) + \sum_{j \neq k} \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\bigg( \frac{1}{n} - \frac{(x_k - \overline{x})\overline{x}}{S_{XX}} \bigg) Cov(Y_j, Y_k) \\
    &amp; = \frac{\sigma^2}{n} - 2 \sigma^2 \frac{\overline{x}}{S_{XX}} \sum_{i = 1}^n (x_i - \overline{x}) + \frac{\sigma^2 \overline{x}^2 \sum (x_i - \overline{x})^2}{S_{XX}^2} \\
    &amp; = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \sigma^2 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li></li>
</ol>
<span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, \hat\beta_1) &amp; = Cov(\overline{Y} - \hat\beta_1 \overline{x}, \hat\beta_1) \\
    &amp; = - \overline{x} Var\hat\beta_1 \\
    &amp; = - \frac{\overline{x}}{S_{XX}} \sigma^2
  \end{split}
\end{equation*}\]</span>
</div>

</div>
<div id="gauss-markov-theorem" class="section level3">
<h3><span class="header-section-number">1.2.4</span> Gauss-Markov Theorem</h3>
<p>Chapter <a href="simple.html#lseprop">1.2.3</a> shows that the <span class="math inline">\(\beta_0^{LSE}\)</span> and <span class="math inline">\(\beta_1^{LSE}\)</span> are the <strong>linear unbiased estimators</strong>. Are these good? Good compared to <em>what estimators</em>? Here we consider <em>linear unbiased estimator</em>. If variances in the proposition <a href="simple.html#prp:vb">1.2</a> are lower than any parameters in this parameter family, <span class="math inline">\(\beta_0^{LSE}\)</span> and <span class="math inline">\(\beta_1^{LSE}\)</span> are the <strong>best linear unbiased estimators</strong>.</p>

<div class="theorem">
<p><span id="thm:gmt" class="theorem"><strong>Theorem 1.1  (Gauss Markov Theorem)  </strong></span><span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are BLUE, i.e. the best linear unbiased estimator.</p>
<p><span class="math display">\[Var(\hat\beta_0) \le Var\Big( \sum_{i = 1}^n a_i Y_i \Big) \: \forall a_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n a_i Y_i \Big) = \beta_0\]</span></p>
<span class="math display">\[Var(\hat\beta_1) \le Var\Big( \sum_{i = 1}^n b_i Y_i \Big) \: \forall b_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n b_i Y_i \Big) = \beta_1\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof</em> (Bestness of <span class="math inline">\(\beta_1\)</span>). </span> Consider <span class="math inline">\(\Theta := \bigg\{ \sum\limits_{i = 1}^n b_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n b_i Y_i \Big) = \beta_1 \bigg\}\)</span>.</p>
<p>Claim: <span class="math inline">\(Var( \sum b_i Y_i) - Var(\hat\beta_1) \ge 0\)</span></p>
<p>Let <span class="math inline">\(\sum b_i Y_i \in \Theta\)</span>. Then <span class="math inline">\(E(\sum b_i Y_i) = \beta_1\)</span>.</p>
<p>Since <span class="math inline">\(E(Y_i) = \beta_0 + \beta_1 x_i\)</span>,</p>
<p><span class="math display">\[\beta_0 \sum b_i + \beta_1 \sum b_i x_i = \beta_1\]</span></p>
<p>It gives</p>
<p><span class="math display">\[\begin{equation} \label{eq:ule}
  \begin{cases}
    \sum b_i = 0 \\
    \sum b_i x_i = 1
  \end{cases}
\end{equation}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum b_i Y_i - \hat\beta_1\Big) &amp; = Var\Big( \sum b_i Y_i - \sum \frac{(x_i - \bar{x})}{S_{XX}} Y_i \Big) \\
    &amp; \stackrel{indep}{=} \sum \bigg( b_i - \frac{(x_i - \bar{x})}{S_{XX}} \bigg)^2 \sigma^2 \\
    &amp; = \sum \bigg( b_i^2 - \frac{2b_i (x_i - \bar{x})}{S_{XX}} + \frac{(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \sigma^2 \\
    &amp; = \sum b_i^2 \sigma^2 - \frac{2 \sigma^2}{S_{XX}} \sum b_i x_i + \frac{2 \bar{x} \sigma^2}{S_{XX}} \sum b_i + \sigma^2 \frac{\sum (x_i - \bar{x})^2}{S_{XX}^2} \\
    &amp; = \sum b_i^2 \sigma^2 - \frac{\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule} \:\text{and}\: S_{XX} = \sum (x_i - \bar{x})^2 \\
    &amp; = Var(\sum b_i Y_i) - Var(\hat\beta_1)
  \end{split}
\end{equation*}\]</span></p>
<p>Hence,</p>
<span class="math display">\[Var(\sum b_i Y_i) \ge Var(\hat\beta_1)\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof</em> (Bestness of <span class="math inline">\(\beta_0\)</span>). </span> Consider <span class="math inline">\(\Theta := \bigg\{ \sum\limits_{i = 1}^n a_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n a_i Y_i \Big) = \beta_0 \bigg\}\)</span>.</p>
<p>Claim: <span class="math inline">\(Var( \sum a_i Y_i) - Var(\hat\beta_0) \ge 0\)</span></p>
<p>Let <span class="math inline">\(\sum a_i Y_i \in \Theta\)</span>. Then <span class="math inline">\(E(\sum a_i Y_i) = \beta_0\)</span>.</p>
<p>Since <span class="math inline">\(E(Y_i) = \beta_0 + \beta_1 x_i\)</span>,</p>
<p><span class="math display">\[\beta_0 \sum a_i + \beta_1 \sum a_i x_i = \beta_0\]</span></p>
<p>It gives</p>
<p><span class="math display">\[\begin{equation} \label{eq:ule0}
  \begin{cases}
    \sum a_i = 1 \\
    \sum a_i x_i = 0
  \end{cases}
\end{equation}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum a_iY_i - \hat\beta_0 \Big) &amp; = Var\bigg[\sum a_iY_i - \sum\Big( \frac{1}{n} - \frac{(x_k - \bar{x})\bar{x}}{S_{XX}} \Big) Y_k \bigg] \\
    &amp; = \sum \bigg(a_i - \frac{1}{n} +  \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \bigg)^2\sigma^2 \\
    &amp; = \sum \bigg[ a_i^2 - 2a_i\Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big) + \Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big)^2 \bigg]\sigma^2 \\
        &amp; = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n}\sum a_i + \frac{2\bar{x}\sigma^2\sum a_ix_i}{S_{XX}} - \frac{2\bar{x}^2\sigma^2\sum a_i}{S_{XX}} \\
        &amp; \qquad + \sigma^2\bigg( \frac{1}{n} - \frac{2\bar{x}}{nS_{XX}} \sum(x_i - \bar{x}) + \frac{\bar{x}^2\sum(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \\
        &amp; = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n} - \frac{2\bar{x}^2\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule0} \\
        &amp; \qquad + \bigg(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \qquad \because \sum(x_i - \bar{x}) = 0 \: \text{and} \: S_{XX} := \sum (x_i - \bar{x})^2 \\
        &amp; = \sum a_i^2\sigma^2 - \bigg( \frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \\
        &amp; = Var\Big( \sum a_i Y_i \Big) - Var\hat\beta_0
  \end{split}
\end{equation*}\]</span></p>
<p>Hence,</p>
<span class="math display">\[Var(\sum a_i Y_i) \ge Var(\hat\beta_0)\]</span>
</div>


<div class="example">
<span id="exm:usingnormal" class="example"><strong>Example 1.1  </strong></span>Show that <span class="math inline">\(\sum (Y_i - \hat{Y_i}) = 0\)</span>, <span class="math inline">\(\sum x_i (Y_i - \hat{Y_i}) = 0\)</span>, and <span class="math inline">\(\sum \hat{Y_i} (Y_i - \hat{Y_i}) = 0\)</span>.
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span> Consider the two normal equations <a href="simple.html#eq:normbeta0">(1.2)</a> and <a href="simple.html#eq:normbeta1">(1.3)</a>. Note that <span class="math inline">\(\hat{Y_i} = \hat\beta_0 + \hat\beta_1 x_i\)</span>.</p>
<p>From the Equation <a href="simple.html#eq:normbeta0">(1.2)</a>, we have <span class="math inline">\(\sum (Y_i - \hat{Y_i}) = 0\)</span>.</p>
<p>From the Equation <a href="simple.html#eq:normbeta1">(1.3)</a>, we have <span class="math inline">\(\sum x_i (Y_i - \hat{Y_i}) = 0\)</span>.</p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \sum \hat{Y_i} (Y_i - \hat{Y_i}) &amp; = \sum (\hat\beta_0 + \hat\beta_1 x_i) (Y_i - \hat{Y_i}) \\
    &amp; = \hat\beta_0 \sum (Y_i - \hat{Y_i}) + \hat\beta_1 \sum x_i (Y_i - \hat{Y_i}) \\
    &amp; = 0
  \end{split}
\end{equation*}\]</span>
</div>

</div>
<div id="estimation-of-sigma2" class="section level3">
<h3><span class="header-section-number">1.2.5</span> Estimation of <span class="math inline">\(\sigma^2\)</span></h3>
<p>There is the last parameter, <span class="math inline">\(\sigma^2 = Var(Y_i)\)</span>. In the <em>least squares estimation literary</em>, we estimate <span class="math inline">\(\sigma^2\)</span> by</p>
<p><span class="math display" id="eq:siglse">\[\begin{equation}
  \hat\sigma^2 = \frac{1}{n - 2} \sum_{i = 1}^n (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2
  \tag{1.7}
\end{equation}\]</span></p>
<p>Why <span class="math inline">\(n - 2\)</span>? This makes the estimator unbiased.</p>

<div class="proposition">
<span id="prp:sigex" class="proposition"><strong>Proposition 1.3  (Unbiasedness)  </strong></span><span class="math display">\[E(\hat\sigma^2) = \sigma^2\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that</p>
<p><span class="math display">\[(Y_i - \hat\beta_0 - \hat\beta_1 x_i) = (Y_i - \overline{Y}) - \hat\beta_1(x_i - \overline{x})\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) &amp; = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2 \bigg] \\
    &amp; = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \overline{Y})^2 + \hat\beta_1^2 \sum (x_i - \overline{x})^2 -2\hat\beta_1 \sum (Y_i - \overline{Y})(x_i - \overline{x}) \bigg] \\
    &amp; = \frac{1}{n - 2} E ( S_{YY} + \hat\beta_1^2 S_{XX} - 2 \hat\beta_1 S_{XY}) \\
    &amp; = \frac{1}{n - 2} E ( S_{YY} - \hat\beta_1^2 S_{XX}) \qquad \because S_{XY} = \hat\beta_1 S_{XX} \\
    &amp; = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{XX} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big)
  \end{split}
\end{equation*}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    ES_{YY} &amp; = E\Big[ \sum (Y_i - \overline{Y})^2 \Big] \\
    &amp; = E \Big[ \sum \Big( (\beta_0 + \beta_1 x_i + \epsilon_i) - (\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) \Big)^2 \Big] \\
    &amp; = E \Big[ \sum \Big( \beta_1 (x_i - \overline{x}) + (\epsilon_i - \overline{\epsilon}) \Big)^2 \Big] \\
    &amp; = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) + 2\beta_1 \sum (x_i - \overline{x}) E(\epsilon_i - \overline{\epsilon}) \\
    &amp; = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big)
  \end{split}
\end{equation*}\]</span></p>
<p>Since <span class="math inline">\(E(\bar\epsilon) = 0\)</span> and <span class="math inline">\(Var(\bar\epsilon) = \frac{\sigma^2}{n}\)</span>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) &amp; = E \Big( \sum (\epsilon_i^2 + \bar\epsilon^2 - 2\epsilon_i \bar\epsilon) \Big) \\
    &amp; = \sum E(\epsilon_i^2) - n E(\bar\epsilon^2) \qquad \because \sum \epsilon = n \bar\epsilon \\
    &amp; = \sum (Var(\epsilon_i) + E(\epsilon_i)^2) - n(Var(\bar\epsilon) + E(\bar\epsilon)^2) \\
    &amp; = n\sigma^2 - \sigma^2 \\
    &amp; = (n - 1)\sigma^2
  \end{split}
\end{equation*}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[ES_{YY} = \beta_1^2 S_{XX} + (n - 1)\sigma^2\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    E\hat\beta_1^2 &amp; = Var\hat\beta_1 + E(\hat\beta_1)^2 \\
    &amp; = \frac{\sigma^2}{S_{XX}} + \beta_1^2
  \end{split}
\end{equation*}\]</span></p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) &amp; = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{YY} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big) \\
    &amp; = \frac{1}{n - 2} \bigg( \Big(\beta_1^2 S_{XX} + (n - 1)\sigma^2 \Big) - S_{XX}\Big(\frac{\sigma^2}{S_{XX}} + \beta_1^2 \Big) \bigg) \\
    &amp; = \frac{1}{n - 2}((n - 2)\sigma^2) \\
    &amp; = \sigma^2
  \end{split}
\end{equation*}\]</span>
</div>

</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2">
<h2><span class="header-section-number">1.3</span> Maximum Likelihood Estimation</h2>
<p>In this section, we add an assumption to an random errors <span class="math inline">\(\epsilon_i\)</span>.</p>
<p><span class="math display">\[\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\]</span></p>

<div class="example">
<p><span id="exm:gmle" class="example"><strong>Example 1.2  (Gaussian Likelihood)  </strong></span>Note that <span class="math inline">\(Y_i \stackrel{indep}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)\)</span>. Then the likelihood function is</p>
<p><span class="math display">\[L(\beta_0, \beta_1, \sigma^2) = \prod_{i = 1}^n\bigg( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \bigg(- \frac{(Y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2} \bigg) \bigg)\]</span></p>
<p>and so the log-likelihood function can be computed as</p>
<span class="math display">\[l(\beta_0, \beta_1, \sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2\]</span>
</div>

<div id="likelihood-equations" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Likelihood equations</h3>

<div class="definition">
<span id="def:mledef" class="definition"><strong>Definition 1.2  (Maximum Likelihood Estimator)  </strong></span><span class="math display">\[(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) := \arg\sup L(\beta_0, \beta_1, \sigma^2)\]</span>
</div>

<p>Since <span class="math inline">\(l(\cdot) = \ln L(\cdot)\)</span> is monotone,</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> <span class="math display">\[(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) = \arg\sup l(\beta_0, \beta_1, \sigma^2)\]</span>
</div>

<p>We can find the maximum of this <em>quadratic</em> function by making first derivative.</p>
<p><span class="math display" id="eq:mlbeta0">\[\begin{equation}
  \frac{\partial l}{\partial \beta_0} = \frac{1}{\sigma^2} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i) = 0
  \tag{1.8}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:mlbeta1">\[\begin{equation}
  \frac{\partial l}{\partial \beta_1} = \frac{1}{\sigma^2} \sum_{i = 1}^n x_i (Y_i - \beta_0 - \beta_1 x_i) = 0
  \tag{1.9}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:mlsig">\[\begin{equation}
  \frac{\partial l}{\partial \sigma^2} = - \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = 0
  \tag{1.10}
\end{equation}\]</span></p>
<p>Denote that Equations <a href="simple.html#eq:mlbeta0">(1.8)</a> and <a href="simple.html#eq:mlbeta1">(1.9)</a> given <span class="math inline">\(\hat\sigma^2\)</span> are equivalent to the normal equations. Thus,</p>
<p><span class="math display">\[\hat\beta_0^{MLE} = \hat\beta_0^{LSE}, \quad \hat\beta_1^{MLE} = \hat\beta_1^{LSE}\]</span></p>
<p>From Equation <a href="simple.html#eq:mlsig">(1.10)</a>,</p>
<p><span class="math display">\[\hat\sigma^{2MLE} = \frac{1}{n}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2 = \frac{n - 2}{n} \hat\sigma^{2LSE}\]</span></p>
<p>While <span class="math inline">\(\hat\sigma^{2LSE}\)</span> is an unbiased, above <em>MLE is not an unbiased estimator</em>. Since <span class="math inline">\(\hat\sigma^{2MLE} \approx \hat\sigma^{2LSE}\)</span> for large <span class="math inline">\(n\)</span>, howerver, it is <em>asymptotically unbiased</em>.</p>

<div class="theorem">
<p><span id="thm:rclb" class="theorem"><strong>Theorem 1.2  (Rao-Cramer Lower Bound, univariate case)  </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta)\)</span>. If <span class="math inline">\(\hat\theta\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[Var(\hat\theta) \ge \frac{1}{I_n(\theta)}\]</span></p>
where <span class="math inline">\(I_n(\theta) = -E\bigg(\frac{\partial^2 l(\theta)}{\partial \theta^2} \bigg)\)</span>
</div>

<p>To apply this theorem <a href="simple.html#thm:rclb">1.2</a> in the simple linear regression setting, i.e. <span class="math inline">\((\beta_0, \beta_1)\)</span>, we need to look at the <em>bivariate case</em>.</p>

<div class="theorem">
<p><span id="thm:rclb2" class="theorem"><strong>Theorem 1.3  (Rao-Cramer Lower Bound, bivariate case)  </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta1, \theta_2)\)</span> and let <span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \theta_2)^T\)</span>. If each <span class="math inline">\(\hat\theta_1\)</span>, <span class="math inline">\(\hat\theta_2\)</span> is an unbiased estimator of <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, then</p>
<p><span class="math display">\[
Var(\boldsymbol{\theta}) := \begin{bmatrix}
Var(\hat\theta_1) &amp; Cov(\hat\theta_1, \hat\theta_2) \\
Cov(\hat\theta_1, \hat\theta_2) &amp; Var(\hat\theta_2)
\end{bmatrix} \ge I_n^{-1}(\theta_1, \theta_2)
\]</span></p>
<p>where</p>
<span class="math display">\[
I_n(\theta_1, \theta_2) = - \begin{bmatrix}
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1^2} \bigg) &amp; E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) \\
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) &amp; E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_2^2} \bigg)
\end{bmatrix}
\]</span>
</div>

<p>Assume that <span class="math inline">\(\sigma^2\)</span> is <strong>known</strong>. From the Equations <a href="simple.html#eq:mlbeta0">(1.8)</a> and <a href="simple.html#eq:mlbeta1">(1.9)</a>,</p>
<p><span class="math display">\[
\begin{cases}
  \frac{\partial^2 l}{\partial \beta_0^2} = - \frac{n}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_1^2} = - \frac{\sum x_i^2}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_0 \partial \beta_1} = - \frac{\sum x_i}{\sigma^2}
\end{cases}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
I_n(\beta_0, \beta_1) = \begin{bmatrix}
  \frac{n}{\sigma^2} &amp; \frac{\sum x_i}{\sigma^2} \\
  \frac{\sum x_i}{\sigma^2} &amp; \frac{\sum x_i^2}{\sigma^2}
\end{bmatrix}
\]</span></p>
<p>Applying gaussian elimination,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} &amp; \frac{\sum x_i}{\sigma^2} &amp; 1 &amp; 0 \\
      \frac{\sum x_i}{\sigma^2} &amp; \frac{\sum x_i^2}{\sigma^2} &amp; 0 &amp; 1
    \end{array}
    \right] &amp; \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} &amp; \frac{\sum x_i}{\sigma^2} &amp; 1 &amp; 0 \\
      \frac{\sum x_i}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) &amp; \frac{\sum x_i^2}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) &amp; 0 &amp; \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    &amp; \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} &amp; \frac{\sum x_i}{\sigma^2} &amp; 1 &amp; 0 \\
      0 &amp; \frac{\sum x_i^2 - \overline{x}\sum x_i}{\sigma^2\overline{x}} = \frac{S_{XX}}{\sigma^2\overline{x}} &amp; -1 &amp; \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    &amp; \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 &amp; \overline{x} &amp; \frac{\sigma^2}{n} &amp; 0 \\
      0 &amp; 1 &amp; -\frac{\overline{x}}{S_{XX}}\sigma^2 &amp; \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right] \\
    &amp; \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 &amp; 0 &amp; \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 &amp; -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
      0 &amp; 1 &amp; -\frac{\overline{x}}{S_{XX}}\sigma^2 &amp; \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right]
  \end{split}
\end{equation*}\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
I_n^{-1}(\beta_0, \beta_1) = \begin{bmatrix}
  \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 &amp; -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
  -\frac{\overline{x}}{S_{XX}}\sigma^2 &amp; \frac{\sigma^2}{S_{XX}}
\end{bmatrix} = \begin{bmatrix}
  Var(\hat\beta_0) &amp; Cov(\hat\beta_0, \hat\beta_1) \\
  Cov(\hat\beta_0, \hat\beta_1) &amp; Var(\hat\beta_1)
\end{bmatrix}
\]</span></p>
<p>Since <span class="math inline">\(Var(\boldsymbol{\hat\beta}) - I^{-1} = 0\)</span> is non-negative definite, each <span class="math inline">\(Var(\hat\beta_0) = \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2\)</span> and <span class="math inline">\(Var(\hat\beta_1) = \frac{\sigma^2}{S_{XX}}\)</span> is a theoretical bound.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> This says that <span class="math inline">\(\hat\beta_0^{LSE} = \hat\beta_0^{MLE}\)</span> and <span class="math inline">\(\hat\beta_1^{LSE} = \hat\beta_1^{MLE}\)</span> have the smallest variance among all unbiased estimator.
</div>

<p>This result is <em>stronger than Gauss-Markov theorem</em> <a href="simple.html#thm:gmt">1.1</a>, where the LSE has the smalleset variance among all <em>linear unbiased</em> estimators. It can be simply obtained from the <em>Lehmann-Scheffe Theorem</em>: If some unbiased estimator is a function of complete sufficient statistic, then this estimator is the unique MVUE <span class="citation">(Hogg, McKean, and Craig <a href="#ref-Hogg:2018aa" role="doc-biblioref">2018</a>)</span>.</p>

<div class="remark">
 <span class="remark"><em>Remark</em> (Lehmann and Scheffe for regression coefficients). </span> <span class="math inline">\(u\Big(\sum Y_i, S_{XY} \Big)\)</span> is CSS in this regression problem, i.e. known <span class="math inline">\(\sigma^2\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From the example <a href="simple.html#exm:gmle">1.2</a>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    L(\beta_0, \beta_1) &amp; = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum(Y_i - \beta_0 - \beta_1 x_i)^2 \bigg] \\
    &amp; = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum \Big(Y_i^2 - (\beta_0 + \beta_1 x_i)Y_i + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg] \\
    &amp; = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \Big( -\beta_0 \sum Y_i - \beta_1 \sum x_i Y_i \Big) \bigg] \exp\bigg[-\frac{1}{2\sigma^2} \Big( \sum Y_i^2 + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg]
  \end{split}
\end{equation*}\]</span></p>
<p>By the Factorization theorem, both <span class="math inline">\(\sum Y_i\)</span> and <span class="math inline">\(\sum x_i Y_i\)</span> are sufficient statistics. Since <span class="math inline">\(S_{XY}\)</span> is one-to-one function of <span class="math inline">\(\sum x_i Y_i\)</span>, it is also a sufficient statistic.</p>
<p>Denote that the normal distribution is in exponential family.</p>
Hence, <span class="math inline">\((\sum Y_i, S_{XY})\)</span> are CSS.
</div>

</div>
</div>
<div id="residuals" class="section level2">
<h2><span class="header-section-number">1.4</span> Residuals</h2>

<div class="definition">
<span id="def:res" class="definition"><strong>Definition 1.3  (Residuals)  </strong></span><span class="math display">\[e_i := Y_i - \hat{Y_i}\]</span>
</div>

<div id="prediction-error" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Prediction error</h3>
<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">yhat =</span> <span class="kw">predict</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x))) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_linerange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> y, <span class="dt">ymax =</span> yhat), <span class="dt">col =</span> <span class="kw">I</span>(<span class="st">&quot;red&quot;</span>), <span class="dt">alpha =</span> <span class="fl">.7</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Number of Cases&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Delivery Time&quot;</span>
  )</code></pre>
<div class="figure" style="text-align: center"><span id="fig:regplot"></span>
<img src="regression-analysis_files/figure-html/regplot-1.png" alt="Fit and residuals" width="70%" />
<p class="caption">
Figure 1.3: Fit and residuals
</p>
</div>
<p>See Figure <a href="simple.html#fig:regplot">1.3</a>. Each red line is <span class="math inline">\(e_i\)</span>. As we can see, <span class="math inline">\(e_i\)</span> represents the difference between <em>observed</em> response and <em>predicted</em> response. A large <span class="math inline">\(\lvert e_i \rvert\)</span> indicates a large prediction error. You can call this <span class="math inline">\(e_i\)</span> for each <span class="math inline">\(Y_i\)</span> by <code>lm()$residuals</code> or <code>residuals()</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">delv_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> delv)
delv_fit<span class="op">$</span>residuals</code></pre>
<pre><code>     1      2      3      4      5      6      7      8      9     10 
-1.874  1.651  2.181  2.855 -2.628 -0.444  0.327 -0.724 10.634  7.298 
    11     12     13     14     15     16     17     18     19     20 
 2.191 -4.082  1.475  3.372  1.094  3.918 -1.028  0.446 -0.349 -5.216 
    21     22     23     24     25 
-7.182 -7.581 -4.156 -0.900 -1.275 </code></pre>
<p><span class="math inline">\(\sum e_i^2\)</span>, which has been minimized in the procedure of LSE, can be used to see <em>overall size of prediction errors</em>.</p>

<div class="definition">
<span id="def:sse" class="definition"><strong>Definition 1.4  (Residual Sum of Squares)  </strong></span><span class="math display">\[SSE := \sum_{i = 1}^n e_i^2\]</span>
</div>

</div>
<div id="residuals-and-the-variance" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Residuals and the variance</h3>
<p><span class="math inline">\(e_i\)</span> is a random quantity, which contains the information for <span class="math inline">\(\epsilon_i\)</span>. <span class="math inline">\(\sum e_i^2\)</span> can give information about <span class="math inline">\(\sigma^2 = Var(\epsilon_i)\)</span>. For this, it is expected that <span class="math inline">\(e_i\)</span> and <span class="math inline">\(\epsilon_i\)</span> have similar feature.</p>

<div class="lemma">
<p><span id="lem:yandbet" class="lemma"><strong>Lemma 1.3  </strong></span>Covriance between Y and each coefficient</p>
<p><span class="math inline">\(\text{(a)}\: Cov(\hat\beta_0, Y_i) = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2\)</span></p>
<span class="math inline">\(\text{(b)}\: Cov(\hat\beta_1, Y_i) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> (a)</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, Y_i) &amp; = Cov(\sum a_i Y_i, Y_i) \\
    &amp; = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2
  \end{split}
\end{equation*}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(\hat\beta_1, Y_i) &amp; = Cov(\sum b_i Y_i, Y_i) \\
    &amp; = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2
  \end{split}
\end{equation*}\]</span>
</div>


<div class="proposition">
<p><span id="prp:resprop" class="proposition"><strong>Proposition 1.4  (Properties of residuals)  </strong></span>Mean and variance of the residual</p>
<p><span class="math inline">\(\text{(a)}\: E(e_i) = 0\)</span></p>
<p><span class="math inline">\(\text{(b)}\: Var(e_i) \neq \sigma^2\)</span></p>
<span class="math inline">\(\text{(c)}\: \forall i \neq j : Cov(e_i, e_j) \neq 0\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> (a) Recall that this is the assumption of the regression model.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Lemma <a href="simple.html#lem:yandbet">1.3</a> implies that</li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(\overline{Y}, \hat\beta_1) &amp; = Cov(\frac{1}{n}\sum Y_i, \hat\beta_1) \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    &amp; = 0 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}\]</span></p>
<p>Then</p>
<p><span class="math display" id="eq:predvar">\[\begin{equation}
  \begin{split}
    Var(\hat{Y_i}) &amp; = Var(\hat\beta_0 + \hat\beta_1 x_i) \\
    &amp; = Var \bigg[ \overline{Y} + (x_i - \overline{x}) \hat\beta_1 \bigg] \qquad \because \hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x} \\
    &amp; = Var(\overline{Y}) + (x_i - \overline{x})^2 Var(\hat\beta_1) + 2(x_i - \overline{x}) Cov(\overline{Y}, \hat\beta_1) \\
    &amp; = \frac{\sigma^2}{n} + (x_i - \overline{x})^2\frac{\sigma^2}{S_{XX}} + 0 \\
    &amp; = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
  \tag{1.11}
\end{equation}\]</span></p>
<p>From the same lemma <a href="simple.html#lem:yandbet">1.3</a>,</p>
<p><span class="math display" id="eq:yyhat">\[\begin{equation}
  \begin{split}
    Cov(Y_i, \hat{Y_i}) &amp; = Cov(Y_i, \overline{Y} + (x_i - \overline{x}) \hat\beta_1) \\
    &amp; = Cov(Y_i, \overline{Y}) + (x_i - \overline{x}) Cov(Y_i, \hat\beta_1) \\
    &amp; = \frac{\sigma^2}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}}\sigma^2 \qquad \because Cov(Y_i, \hat\beta_1) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    &amp; = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
  \tag{1.12}
\end{equation}\]</span></p>
<p>These Equations <a href="simple.html#eq:predvar">(1.11)</a> and <a href="simple.html#eq:yyhat">(1.12)</a> give that</p>
<p><span class="math display" id="eq:residvar">\[\begin{equation}
  \begin{split}
    Var(e_i) &amp; = Var(Y_i) + Var(\hat{Y_i}) -2Cov(Y_i, \hat{Y_i}) \\
    &amp; = \sigma^2 + \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 - 2 \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    &amp; = \bigg(1 - \frac{1}{n} - \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    &amp; \neq \sigma^2
  \end{split}
  \tag{1.13}
\end{equation}\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Let <span class="math inline">\(i \neq j\)</span>. Then</li>
</ol>
<span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(e_i, e_j) &amp; = Cov\Big( Y_i - (\hat\beta_0 + \hat\beta_1 x_i), Y_j - (\hat\beta_0 + \hat\beta_1 x_j) \Big) \\
    &amp; = Cov(Y_i, Y_j) - Cov\Big(Y_i, (\hat\beta_0 + \hat\beta_1 x_j) \Big) - Cov((\hat\beta_0 + \hat\beta_1 x_i), Y_j) + Cov((\hat\beta_0 + \hat\beta_1 x_i), (\hat\beta_0 + \hat\beta_1 x_j)) \\
    &amp; = 0 - \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_j}{S_{XX}}\sigma^2 \\
    &amp; \qquad - \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_i}{S_{XX}}\sigma^2 \\
    &amp; \qquad + \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    &amp; = - \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    &amp; = - \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})(x_j - \overline{x})}{S_{XX}} \bigg)\sigma^2 \\
    &amp; \neq 0
  \end{split}
\end{equation*}\]</span>
</div>

</div>
</div>
<div id="decomposition-of-total-variability" class="section level2">
<h2><span class="header-section-number">1.5</span> Decomposition of Total Variability</h2>
<div id="total-sum-of-squares" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Total sum of squares</h3>

<div class="definition">
<span id="def:unsst" class="definition"><strong>Definition 1.5  (Uncorrected Total Sum of Squares)  </strong></span><span class="math display">\[SST_{uncor} := \sum_{i = 1}^n Y_i^2\]</span>
</div>


<div class="definition">
<span id="def:sst" class="definition"><strong>Definition 1.6  (Corrected Total Sum of Squares)  </strong></span><span class="math display">\[SST := \sum_{i = 1}^n (Y_i - \overline{Y})^2\]</span>
</div>

<p>What does this total sum of squares mean? To know this, we should know <span class="math inline">\(\overline{Y}\)</span> first.</p>
<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Number of Cases&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Delivery Time&quot;</span>
  )</code></pre>
<div class="figure" style="text-align: center"><span id="fig:ybarpred"></span>
<img src="regression-analysis_files/figure-html/ybarpred-1.png" alt="Regression without predictor" width="70%" />
<p class="caption">
Figure 1.4: Regression without predictor
</p>
</div>
<p>See Figure <a href="simple.html#fig:ybarpred">1.4</a>. The line represents the closest line when we use only intercept term for the regression model. In other words, <em>if we use no information for the response</em>, i.e. no predictor variables, we will get just average of the response variable. Consider</p>
<p><span class="math display">\[Y_i = \beta_0 + \epsilon_i\]</span></p>
<p>Then we can get only one normal equation</p>
<p><span class="math display">\[\sum (Y_i - \hat\beta_0) = 0\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[\hat\beta_0 = \frac{1}{n} \sum_{i = 1}^n Y_i \equiv \overline{Y}\]</span></p>
<p>From this fact, <span class="math inline">\(SST\)</span> implies <strong>total variance</strong>.</p>
</div>
<div id="regression-sum-of-squares" class="section level3">
<h3><span class="header-section-number">1.5.2</span> Regression sum of squares</h3>

<div class="definition">
<span id="def:ssr" class="definition"><strong>Definition 1.7  (Regression Sum of Squares)  </strong></span><span class="math display">\[SSR := \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2\]</span>
</div>

<p>This <span class="math inline">\(SSR\)</span> compares <span class="math inline">\(\hat{Y_i}\)</span> versus <span class="math inline">\(\overline{Y}\)</span>, computing the sum of squares for difference between predicted values from <em>regression model</em> and <em>model not using predictors</em>.</p>
</div>
<div id="residual-sum-of-squares" class="section level3">
<h3><span class="header-section-number">1.5.3</span> Residual sum of squares</h3>
<p>Now consider the <em>residual sum of squares</em> <span class="math inline">\(SSE\)</span> in the definition <a href="simple.html#def:sse">1.4</a>. As mentioned, this is related to the <em>prediction errors</em>, which the regression model could not explain the data.</p>
</div>
<div id="decompsst" class="section level3">
<h3><span class="header-section-number">1.5.4</span> Decomposition of total sum of squares</h3>
<p><span class="math inline">\(SST\)</span> can be decomposed by construction of sum of squares.</p>

<div class="proposition">
<p><span id="prp:decom" class="proposition"><strong>Proposition 1.5  (Decomposition of SST)  </strong></span><span class="math display">\[SST = SSR + SSE\]</span></p>
where <span class="math inline">\(SST = \sum (Y_i - \overline{Y})^2\)</span>, <span class="math inline">\(SSR = \sum (\hat{Y_i} - \overline{Y})^2\)</span>, and <span class="math inline">\(SSE = \sum (Y_i - \hat{Y_i})\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From the Example <a href="simple.html#exm:usingnormal">1.1</a>,</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \sum_{i = 1}^n (Y_i - \overline{Y})^2 &amp; = \sum_{i = 1}^n (Y_i - \hat{Y_i} + \hat{Y_i} - \overline{Y})^2 \\
    &amp; = \sum_{i = 1}^n (Y_i - \hat{Y_i})^2 + 2 \sum_{i = 1}^n (Y_i - \hat{Y_i})(\hat{Y_i} - \overline{Y}) + \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2 \\
    &amp; = \sum_{i = 1}^n (Y_i - \hat{Y_i})^2 + \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2 \qquad \because \sum (Y_i - \hat{Y_i}) = 0 \: \text{and} \: \sum (Y_i - \hat{Y_i})\hat{Y_i} = 0
  \end{split}
\end{equation*}\]</span>
</div>

<p>This represents each <span class="math inline">\(SSR\)</span> and <span class="math inline">\(SSE\)</span> divides total variability as following.</p>
<p><span class="math display">\[\overset{SST}{\text{total variability}} = \overset{SSR}{\text{explained by regression}} + \overset{SSE}{\text{left unexplained by regression}}\]</span></p>
<p>Denote that the total variability <span class="math inline">\(SST\)</span> is <em>constant given data set</em>. If our model is good, <span class="math inline">\(SSR\)</span> grows and <span class="math inline">\(SSE\)</span> flattens. Thus the larger <span class="math inline">\(SSR\)</span> is, the better. The lower <span class="math inline">\(SSE\)</span> is, the better.</p>
</div>
<div id="coefficient-of-determination" class="section level3">
<h3><span class="header-section-number">1.5.5</span> Coefficient of determination</h3>
<p>We have discussed in the previous section <a href="simple.html#decompsst">1.5.4</a> that <span class="math inline">\(SSR\)</span> and <span class="math inline">\(SSE\)</span> splits the total variability into <em>explained part and not-explained part by our regression model</em>. Our first interest is whether the model works well for the data well, so we can think about the <em>proportion of explained part to the total variance</em>. The following measure <span class="math inline">\(R^2\)</span> computes this kind of value.</p>

<div class="definition">
<span id="def:rsq" class="definition"><strong>Definition 1.8  (Coefficient of Determination)  </strong></span><span class="math display">\[R^2 := \frac{SSR}{SST} = 1 - \frac{1 - SSE}{SST}\]</span>
</div>

<p>By construction,</p>
<p><span class="math display">\[0 \le R^2 \le 1\]</span></p>
<p>As <span class="math inline">\(R^2\)</span> goes to <span class="math inline">\(0\)</span>, the model goes wrong. As <span class="math inline">\(R^2\)</span> is close to <span class="math inline">\(1\)</span>, large proportion of variability has been explained. So we prefer large values rather than small.</p>

<div class="proposition">
<p><span id="prp:rsqlin" class="proposition"><strong>Proposition 1.6  </strong></span><span class="math inline">\(R^2\)</span> shows the strength of linear relation between two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(Y\)</span> in the simple linear regression.</p>
<p><span class="math display">\[R^2 = \hat\rho_{XY}\]</span></p>
where <span class="math inline">\(\hat\rho_{XY} := \frac{\sum (X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum (X_i - \overline{X})^2} \sqrt{\sum (Y_i - \overline{Y})^2}}\)</span> is the sample correlation coefficients
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that <span class="math inline">\(\hat{Y_i} - \overline{Y} = \hat\beta_1 (x_i - \overline{x}) = \frac{S_{XY}}{S_{XX}} (x_i - \overline{x})\)</span>. Then</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    \sum (\hat{Y_i} - \overline{Y})^2 &amp; = \frac{S_{XY}^2}{S_{XX}^2} \sum (x_i - \overline{x})^2 \\
    &amp; = \frac{S_{XY}^2}{S_{XX}}
  \end{split}
\end{equation*}\]</span></p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    R^2 &amp; = \frac{\sum (\hat{Y_i} - \overline{Y})^2}{\sum (Y_i - \overline{Y})^2} \\
    &amp; = \frac{S_{XY}^2}{S_{XX}S_{YY}} \\
    &amp; =: \hat\rho_{XY}^2
  \end{split}
\end{equation*}\]</span>
</div>

<p>In this relation, we can know that <span class="math inline">\(R^2\)</span> statistic performs as a measure of the linear relationship in the simple linear regression setting.</p>
</div>
</div>
<div id="matnot" class="section level2">
<h2><span class="header-section-number">1.6</span> Geometric Interpretations</h2>
<div id="fundamental-subspaces" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Fundamental subspaces</h3>
<p>These linear algebra concepts might be more useful for <em>multiple linear regression</em>, but let’s briefly recap <span class="citation">(Leon <a href="#ref-Leon:2014aa" role="doc-biblioref">2014</a>)</span>.</p>

<div class="definition">
<p><span id="def:subspace" class="definition"><strong>Definition 1.9  (Fundamental Subspaces)  </strong></span>Let <span class="math inline">\(X \in \mathbb{R}^{n \times (p + 1)}\)</span>.</p>
<p>Then the Null space is defined by</p>
<p><span class="math display">\[N(X) := \{ \mathbf{b} \in \mathbb{R}^n \mid X\mathbf{b} = \mathbf{0} \}\]</span></p>
<p>The Row space is defined by</p>
<p><span class="math display">\[Row(X) := sp(\{\mathbf{r}_1, \ldots, \mathbf{r}_{p + 1} \}) \quad \text{where} \: X^T = [\mathbf{r}_1^T, \ldots, \mathbf{r}_{n}^T]\]</span></p>
<p>The Column space is defined by</p>
<p><span class="math display">\[Col(X) := sp(\{\mathbf{c}_1, \ldots, \mathbf{c}_{n} \}) \quad \text{where} \: X = [\mathbf{c}_1, \ldots, \mathbf{c}_{p + 1}]\]</span></p>
<p>The Range of <span class="math inline">\(X\)</span> is defined by</p>
<span class="math display">\[R(X) := \{ \mathbf{y} \in \mathbb{R}^n \mid \mathbf{y} = X\mathbf{b} \quad \text{for some} \: \mathbf{b} \in \mathbb{R}^{p + 1} \}\]</span>
</div>

<p>These spaces have some constructional relationship.</p>

<div class="theorem">
<p><span id="thm:fundsub" class="theorem"><strong>Theorem 1.4  (Fundamental Subspaces Theorem)  </strong></span>Let <span class="math inline">\(X \in \mathbb{R}^{n \times (p + 1)}\)</span>. Then</p>
<p><span class="math display">\[N(X) = R(X^T)^{\perp} = Col(X^T)^{\perp} = Row(X)^{\perp}\]</span></p>
<p>Transposed matrix also satisfy this.</p>
<span class="math display">\[N(X^T) = R(X)^{\perp} = Col(X)^{\perp}\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Let <span class="math inline">\(\mathbf{a} \in N(X)\)</span>. Then <span class="math inline">\(X\mathbf{a} = \mathbf{0}\)</span>.</p>
<p>Let <span class="math inline">\(\mathbf{y} \in R(X^T)\)</span>. Then <span class="math inline">\(X^T \mathbf{b} = \mathbf{y}\)</span> for some <span class="math inline">\(\mathbf{b} \in \mathbb{R}^{p + 1}\)</span>.</p>
<p>Choose <span class="math inline">\(\mathbf{b} \in \mathbb{R}^{p + 1}\)</span> such that <span class="math inline">\(X^T \mathbf{b} = \mathbf{y}\)</span>. Then</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    \mathbf{0} &amp; = X\mathbf{a} \\
    &amp; = \mathbf{b}^T X\mathbf{a} \\
    &amp; = \mathbf{y}^T \mathbf{a}
  \end{split}
\end{equation*}\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[N(X) \perp R(X^T)\]</span></p>
<p>Since</p>
<p><span class="math display">\[X^T \mathbf{b} = \mathbf{c}_1 \mathbf{b} + \cdots + \mathbf{c}_{p + 1} \mathbf{b}\]</span></p>
<p>it is trivial that <span class="math inline">\(R(X) = Col(X)\)</span> and <span class="math inline">\(R(X^T) = Col(X^T)\)</span>.</p>
<p>If <span class="math inline">\(\mathbf{a} \in N(X)\)</span>, then</p>
<p><span class="math display">\[
X\mathbf{a} = \begin{bmatrix}
  \mathbf{r}_1 \\
  \mathbf{r}_2 \\
  \cdots \\
  \mathbf{r}_n
\end{bmatrix} \begin{bmatrix}
  a_1 \\
  \cdots \\
  a_{p + 1}
\end{bmatrix} = \begin{bmatrix}
  0 \\
  0 \\
  \cdots \\
  0
\end{bmatrix}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\forall i :  \mathbf{a}^T \mathbf{r}_i = 0\]</span></p>
<p>and so</p>
<p><span class="math display">\[N(X) \subseteq Row(X)^{\perp}\]</span></p>
<p>Conversely, if <span class="math inline">\(\mathbf{a} \in Row(X)^{\perp}\)</span>, then <span class="math inline">\(\forall i : \mathbf{a}^T \mathbf{r}_i = 0\)</span>. This implies that <span class="math inline">\(X\mathbf{a} = \mathbf{0}\)</span>. Thus,</p>
<p><span class="math display">\[Row(X)^{\perp} \subseteq N(X)\]</span></p>
<p>and so</p>
<span class="math display">\[N(X) = Row(X)^{\perp}\]</span>
</div>

<p><span class="math inline">\(N(X^T) = R(X)^{\perp}\)</span> part in Theorem <a href="simple.html#thm:fundsub">1.4</a> will give the geometric insight to <em>least squares solution</em>.</p>

<div class="theorem">
<p><span id="thm:perpbasis" class="theorem"><strong>Theorem 1.5  </strong></span>Let <span class="math inline">\(S\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. Then</p>
<p><span class="math display">\[dim S + dim S^{\perp} = n\]</span></p>
If <span class="math inline">\(\{ \mathbf{x}_1, \ldots, \mathbf{x}_r \}\)</span> is a basis for <span class="math inline">\(S\)</span> and <span class="math inline">\(\{ \mathbf{x}_{r + 1}, \ldots, \mathbf{x}_n \}\)</span> is a basis for <span class="math inline">\(S^{\perp}\)</span>, then <span class="math inline">\(\{ \mathbf{x}_1, \ldots, \mathbf{x}_r, \mathbf{x}_{r + 1}, \ldots, \mathbf{x}_n \}\)</span> is a basis for <span class="math inline">\(\mathbb{R}^n\)</span>.
</div>


<div class="theorem">
<p><span id="thm:dsum" class="theorem"><strong>Theorem 1.6  </strong></span>Let <span class="math inline">\(S\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. Then</p>
<span class="math display">\[\mathbb{R}^n = S \oplus S^{\perp}\]</span>
</div>

</div>
<div id="simple-linear-regression" class="section level3">
<h3><span class="header-section-number">1.6.2</span> Simple linear regression</h3>

<div class="theorem">
<p><span id="thm:projection" class="theorem"><strong>Theorem 1.7  </strong></span>Let <span class="math inline">\(S\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. For each <span class="math inline">\(\mathbf{y} \in \mathbf{R}^n\)</span>, there exists a unique <span class="math inline">\(\mathbf{p} \in S\)</span> that is closest to <span class="math inline">\(\mathbf{y}\)</span>, i.e.</p>
<p><span class="math display">\[\Vert \mathbf{y} - \mathbf{p}  \Vert &gt; \Vert \mathbf{y} - \mathbf{\hat{y}} \Vert\]</span></p>
<p>for any <span class="math inline">\(\mathbf{p} \neq \mathbf{\hat{y}}\)</span>. Furthermore, a given vector <span class="math inline">\(\mathbf{p} \in S\)</span> will be the closest to a given vector <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span> if and only if</p>
<span class="math display">\[\mathbf{y} - \mathbf{\hat{y}} \in S^{\perp}\]</span>
</div>

<p>Least square estimator <span class="math inline">\((\hat\beta_0, \hat\beta_1)^T\)</span> minimizes</p>
<p><span class="math display" id="eq:qmatrix">\[\begin{equation}
  \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = \Vert \mathbf{Y} - (\beta_0 \mathbf{1} + \beta_1 \mathbf{x}) \Vert^2
  \tag{1.14}
\end{equation}\]</span></p>
<p>with respect to <span class="math inline">\((\hat\beta_0, \hat\beta_1)^T \in \mathbb{R}^2\)</span> (where <span class="math inline">\(\mathbf{1} := (1, 1)^T\)</span>). Recall that the normal equation gives</p>
<p><span class="math display">\[\sum_{i = 1}^n(Y_i - \hat\beta_0 - \hat\beta_1 x_i) = \Big( \mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \Big)^T \mathbf{1} = 0\]</span></p>
<p>and</p>
<p><span class="math display">\[\sum_{i = 1}^n (Y_i - \hat\beta_0 - \hat\beta_1 x_i)x_i = \Big( \mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \Big)^T \mathbf{x} = 0\]</span></p>
<p>These two relation give</p>
<p><span class="math display">\[\mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \perp sp(\{ \mathbf{1}, \mathbf{x} \})^{\perp}\]</span></p>
<p>i.e. <span class="math inline">\(\mathbf{\hat{Y}} = \hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}\)</span> is the projection of <span class="math inline">\(\mathbf{Y}\)</span>.</p>
<p>Theorem <a href="simple.html#thm:projection">1.7</a> can give the same result.</p>
<p><span class="math display">\[\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x} \in R([\mathbf{1}, \mathbf{x}])^{\perp} = sp(\{ \mathbf{1}, \mathbf{x} \})^{\perp}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:simpledraw"></span>
<img src="regression-analysis_files/figure-html/simpledraw-1.png" alt="Geometric Illustration of Simple Linear Regression" width="70%" />
<p class="caption">
Figure 1.5: Geometric Illustration of Simple Linear Regression
</p>
</div>
<p>We can see the details from Figure <a href="simple.html#fig:simpledraw">1.5</a>. In fact, decomposition of <span class="math inline">\(SST\)</span> and <span class="math inline">\(R^2\)</span> are also in here.</p>
<div class="figure" style="text-align: center"><span id="fig:simpledraw2"></span>
<img src="regression-analysis_files/figure-html/simpledraw2-1.png" alt="Geometric Illustration of Decomposing SST" width="70%" />
<p class="caption">
Figure 1.6: Geometric Illustration of Decomposing SST
</p>
</div>
<p>See Figure <a href="simple.html#fig:simpledraw2">1.6</a>.</p>
<p><span class="math display">\[
\begin{cases}
  SST = \lVert \mathbf{Y} - \overline{Y} \mathbf{1} \rVert^2 \\
  SSR = \lVert \mathbf{\hat{Y}} - \overline{Y} \mathbf{1} \rVert^2 \\
  SSE = \lVert \mathbf{Y} - \mathbf{\hat{Y}} \rVert^2
\end{cases}
\]</span></p>
<p>Pythagorean law implies that</p>
<p><span class="math display">\[SST = SSR + SSE\]</span></p>
<p>Also,</p>
<p><span class="math display">\[R^2 = \frac{SSR}{SST} = cos^2\theta = \hat\rho_{XY}^2\]</span></p>
</div>
<div id="solproj" class="section level3">
<h3><span class="header-section-number">1.6.3</span> Projection mapping</h3>
<p>Look again Figure <a href="simple.html#fig:simpledraw">1.5</a>. Let <span class="math inline">\(X \equiv [\mathbf{1}, \mathbf{x}] \in \mathbb{R}^{n \times 2}\)</span> and let <span class="math inline">\(\boldsymbol\beta \equiv (\beta_0, \beta_1)^T\)</span>. By the fundamental subspaces theorem <a href="simple.html#thm:fundsub">1.4</a>,</p>
<p><span class="math display">\[\mathbf{Y} - X\boldsymbol{\hat\beta} \in Col(X)^{\perp} = N(X^T)\]</span></p>
<p>Thus,</p>
<p><span class="math display" id="eq:projeq">\[\begin{equation}
  X^T(\mathbf{Y} - X\boldsymbol{\hat\beta}) = \mathbf{0}
  \tag{1.15}
\end{equation}\]</span></p>
<p>This is the another representation of normal equation. Then we now have</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    &amp; X^T\mathbf{Y} - X^TX\boldsymbol{\hat\beta} = \mathbf{0} \\
    &amp; \Leftrightarrow X^T\mathbf{Y} = X^TX\boldsymbol{\hat\beta}
  \end{split}
\end{equation*}\]</span></p>
<p>If <span class="math inline">\(X^TX\)</span> is nonsingular,</p>
<p><span class="math display">\[\boldsymbol{\hat\beta} = (X^TX)^{-1}X^T \mathbf{Y}\]</span></p>
<p>It follows that</p>
<p><span class="math display">\[\mathbf{\hat{Y}} = X\boldsymbol{\hat\beta} = X(X^TX)^{-1}X^T \mathbf{Y}\]</span></p>
<p>Combining this equation and our figure, we can know that <span class="math inline">\(X(X^TX)^{-1}X^T\)</span> projects <span class="math inline">\(\mathbf{Y}\)</span> from <span class="math inline">\(\mathbb{R}^n\)</span> onto <span class="math inline">\(Col(X) = R(X)\)</span>. This is called projection operator/mapping.</p>

<div class="definition">
<p><span id="def:projop" class="definition"><strong>Definition 1.10  (Projection matrix)  </strong></span>Projection operator or mapping from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(W\)</span> is written by</p>
<span class="math display">\[\Pi(\cdot \mid W) := X(X^TX)^{-1}X^T\]</span>
</div>

<p>As mentioned, <span class="math inline">\(X^TX\)</span> should be invertible to get the LSE solution.</p>

<div class="theorem">
<p><span id="thm:fullrank" class="theorem"><strong>Theorem 1.8  </strong></span>Let <span class="math inline">\(\mathbf{Y} = X\boldsymbol\beta\)</span> inconsistent and let <span class="math inline">\(X \in \mathbb{R}^{n \times (p + 1)}\)</span> with <span class="math inline">\(n &gt; p + 1\)</span>.</p>
If <span class="math inline">\(rank(X) = p + 1\)</span>, i.e. full rank, then <span class="math inline">\(X^T X\)</span> is invertible.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Suppose that <span class="math inline">\((X^TX)\mathbf{b} = \mathbf{0}\)</span>. Then</p>
<p><span class="math display">\[X^T (X\mathbf{b}) = \mathbf{0}\]</span></p>
<p>By the fundamental subspaces theorem <a href="simple.html#thm:fundsub">1.4</a>,</p>
<p><span class="math display">\[X\mathbf{b} \in N(X^T) = Col(X)^{\perp}\]</span></p>
<p>By construction,</p>
<p><span class="math display">\[X\mathbf{b} \in Col(X) = N(X^T)^{\perp}\]</span></p>
<p>Then</p>
<p><span class="math display">\[X\mathbf{b} \in N(X^T) \cap N(X^T)^{\perp} = \{ \mathbf{0} \}\]</span></p>
<p>It follows that</p>
<p><span class="math display">\[X\mathbf{b} = \mathbf{0}\]</span></p>
If <span class="math inline">\(rank(X) = n\)</span>, then the linear equation system has trivial solution <span class="math inline">\(\mathbf{b} = \mathbf{0}\)</span> and so does <span class="math inline">\(X^T (X\mathbf{b}) = \mathbf{0}\)</span>. Hence, <span class="math inline">\(X^T X\)</span> is invertible.
</div>

<p>Using projection matrix <span class="math inline">\(\Pi_W\)</span>, we can re-express each sum of squares. Recall that when we only use <span class="math inline">\(y_i\)</span> for regression fitting, the result becomes its average. It is because <span class="math inline">\(\mathbf{Y}\)</span> vector has been projected onto <span class="math inline">\(sp(\{ \mathbf{1} \})\)</span> line.</p>

<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> <span class="math display">\[\overline{Y}\mathbf{1} = \mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T\mathbf{Y} = \Pi_{\mathbf{1}}\mathbf{Y}\]</span></p>
<span class="math display">\[\mathbf{\hat{Y}} = X(X^TX)^{-1}X^T \mathbf{Y} = \Pi_X \mathbf{Y}\]</span>
</div>

<p>Intuitively, every projection matrix is idempotent and symmetric. Once projected, the result is same when projecting it again.</p>

<div class="corollary">
<p><span id="cor:projss" class="corollary"><strong>Corollary 1.1  (Sum of squares)  </strong></span><span class="math inline">\(\Pi_{\mathbf{1}}\)</span> and <span class="math inline">\(\Pi_X\)</span> can express each <span class="math inline">\(SS\)</span> as following.</p>
<ol style="list-style-type: lower-roman">
<li></li>
</ol>
<p><span class="math display">\[SST = \mathbf{Y}^T (I - \Pi_{\mathbf{1}}) \mathbf{Y}\]</span></p>
<ol start="2" style="list-style-type: lower-roman">
<li></li>
</ol>
<p><span class="math display">\[SSR = \mathbf{Y}^T (\Pi_X - \Pi_{\mathbf{1}}) \mathbf{Y}\]</span></p>
<ol start="3" style="list-style-type: lower-roman">
<li></li>
</ol>
<span class="math display">\[SSE = \mathbf{Y}^T (I - \Pi_X) \mathbf{Y}\]</span>
</div>

</div>
</div>
<div id="simpledist" class="section level2">
<h2><span class="header-section-number">1.7</span> Distributions</h2>
<div id="mean-response-and-response" class="section level3">
<h3><span class="header-section-number">1.7.1</span> Mean response and response</h3>
<p>We have already look at predicting each mean response and response from equation <a href="simple.html#eq:meanres">(1.5)</a> and <a href="simple.html#eq:indpred">(1.6)</a>.</p>

<div class="theorem">
<span id="thm:mux" class="theorem"><strong>Theorem 1.9  (Estimation of the mean response)  </strong></span><span class="math display">\[\hat\mu_x \equiv \widehat{E(Y \mid x)} = \hat\beta_0 + \hat\beta_1 x\]</span>
</div>


<div class="theorem">
<span id="thm:yhatx" class="theorem"><strong>Theorem 1.10  ((out of sample) Prediction of a response)  </strong></span><span class="math display">\[\hat{Y_x}  = \hat\beta_0 + \hat\beta_1 x\]</span>
</div>

<p>Recall that predicting <a href="simple.html#thm:mux">1.9</a> targets at</p>
<p><span class="math display">\[\mu_x \equiv E(Y \mid x) = \beta_0 + \beta_1 x\]</span></p>
<p>which have been assumed to be true model. On the other hand, predicting <a href="simple.html#thm:yhatx">1.10</a> targets at</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 + \epsilon_x\]</span></p>
<p>The linearity is not true in reality. So the errors caused by modeling linear model are included in <span class="math inline">\(\epsilon_x\)</span>. This error term makes difference between properties of <a href="simple.html#thm:mux">1.9</a> and <a href="simple.html#thm:yhatx">1.10</a>.</p>
<p>To derive their distribution and see the difference, we additionaly assume Normality, i.e.</p>
<p><span class="math display">\[\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\]</span></p>
</div>
<div id="simplebdist" class="section level3">
<h3><span class="header-section-number">1.7.2</span> Regression coefficients</h3>
<p>Under Normality, we have</p>
<p><span class="math display">\[Y_i \stackrel{indep}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\mathbf{Y} = \begin{bmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
\end{bmatrix} \sim MVN_n\Bigg( \boldsymbol\mu \equiv \begin{bmatrix}
  \beta_0 + \beta_1 x_1 \\
  \beta_0 + \beta_1 x_2 \\
  \vdots \\
  \beta_0 + \beta_1 x_n
\end{bmatrix}, \Sigma \equiv \sigma^2 I = \begin{bmatrix}
  \sigma^2 &amp; 0 &amp; \cdots &amp; 0 \\
  0 &amp; \sigma^2 &amp; \cdots &amp; 0 \\
  \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
  0 &amp; 0 &amp; 0 &amp; \sigma^2
\end{bmatrix} \Bigg)
\]</span></p>
<p>Write <span class="math inline">\(\boldsymbol{\hat\beta} = (\hat\beta_0, \hat\beta_1)^T\)</span>. From Lemma <a href="simple.html#lem:linbet">1.2</a>,</p>
<p><span class="math display">\[\hat\beta_0 = \mathbf{a}^T\mathbf{Y}\]</span></p>
<p>where <span class="math inline">\(\mathbf{a} = (a_1, \ldots, a_n)^T \in \mathbb{R}^n\)</span> with <span class="math inline">\(a_i = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\)</span></p>
<p>and</p>
<p><span class="math display">\[\hat\beta_1 = \mathbf{b}^T\mathbf{Y}\]</span></p>
<p>where <span class="math inline">\(\mathbf{b} = (b_1, \ldots, b_n)^T \in \mathbb{R}^n\)</span> with <span class="math inline">\(b_i = \frac{(x_i - \overline{x})}{S_{XX}}\)</span>.</p>
<p>Let</p>
<p><span class="math display">\[A^T = [ \mathbf{a}^T, \mathbf{b}^T ]\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\boldsymbol{\hat\beta} = A\mathbf{Y}
\]</span></p>
<p>Linearity of the multivariate normal distribution, Proposition <a href="simple.html#prp:ue">1.1</a> and <a href="simple.html#prp:vb">1.2</a> imply that</p>
<p><span class="math display" id="eq:b01mvn">\[\begin{equation}
  \boldsymbol{\hat\beta} = \begin{bmatrix}
    \hat\beta_0 \\ \hline
    \hat\beta_1
  \end{bmatrix} \sim MVN \bigg( A\boldsymbol\mu = \begin{bmatrix}
    \beta_0 \\ \hline
    \beta_1
  \end{bmatrix},
  A\Sigma A^T = \sigma^2 AA^T = \begin{bmatrix}
    \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 &amp; - \frac{\overline{x}}{S_{XX}}\sigma^2 \\ \hline
    - \frac{\overline{x}}{S_{XX}}\sigma^2 &amp; \frac{\sigma^2}{S_{XX}}
  \end{bmatrix} \bigg)
  \tag{1.16}
\end{equation}\]</span></p>
<p>Since the joint random vector follows multivariate normal distribution, each <em>partitioned subset follow normal</em>. For this theorem, see <span class="citation">Johnson and Wichern (<a href="#ref-Johnson:2013aa" role="doc-biblioref">2013</a>)</span>. Hence, we finally get the following result.</p>

<div class="theorem">
<p><span id="thm:b01dist" class="theorem"><strong>Theorem 1.11  (Distributions of regression coefficients)  </strong></span>Each regression coefficient follows Normal distribution.</p>
<p><span class="math display">\[\hat\beta_0 \sim N \bigg( \beta_0, \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 \bigg)\]</span></p>
<span class="math display">\[\hat\beta_1 \sim N \bigg( \beta_1, \frac{\sigma^2}{S_{XX}} \bigg)\]</span>
</div>

</div>
<div id="mean-response" class="section level3">
<h3><span class="header-section-number">1.7.3</span> Mean response</h3>
<p>In simple linear regression setting, we assume <span class="math inline">\(\mu_x = E(Y \mid x) = \beta_0 + \beta_1 x\)</span> is true.</p>
<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">.7</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Number of Cases&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Delivery Time&quot;</span>
  )</code></pre>
<div class="figure" style="text-align: center"><span id="fig:smoothline"></span>
<img src="regression-analysis_files/figure-html/smoothline-1.png" alt="Mean response and its standard deviation" width="70%" />
<p class="caption">
Figure 1.7: Mean response and its standard deviation
</p>
</div>
<p>For example, in the Figure <a href="simple.html#fig:smoothline">1.7</a>, the blue line indicates <span class="math inline">\(E(Y \mid X = x)\)</span> for each point <span class="math inline">\(x\)</span>. Without fitting using <code>lm()</code>, <code>geom_smooth(method = "lm")</code> let us visualize the fitted line. Since the default method is not the linear regression, the <code>method</code> option should be specified.</p>
<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">eyx =</span> <span class="kw">predict</span>(delv_fit, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x)))</code></pre>
<pre><code># A tibble: 25 x 3
       y     x   eyx
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
 1  16.7     7 18.6 
 2  11.5     3  9.85
 3  12.0     3  9.85
 4  14.9     4 12.0 
 5  13.8     6 16.4 
 6  18.1     7 18.6 
 7   8       2  7.67
 8  17.8     7 18.6 
 9  79.2    30 68.6 
10  21.5     5 14.2 
# … with 15 more rows</code></pre>
<p>We have already seen in section <a href="simple.html#simplebdist">1.7.2</a> that the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are random variables. So <span class="math inline">\(\hat\mu_x\)</span> is. In fact, the ribbon of the line in Figure <a href="simple.html#fig:smoothline">1.7</a> represents upper and lower confidence limits on mean response. In the later section, we get to know that it is <span class="math inline">\(+ t(n - 2)\widehat{SE}(\hat\mu_x)\)</span> and <span class="math inline">\(- t(n - 2) \widehat{SE}(\hat\mu_x)\)</span>. It can be drawn by default with the option of the <code>geom_smooth(se = TRUE)</code>.</p>

<div class="theorem">
<p><span id="thm:mrdist" class="theorem"><strong>Theorem 1.12  (Distribution of mean response estimator)  </strong></span><span class="math inline">\(\hat\mu_x\)</span> is also Normally distributed.</p>
<span class="math display">\[\hat\mu_x \sim N\bigg( \mu_x, \sigma^2\bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Since <span class="math inline">\(\hat\mu_x = \hat\beta_0 + \hat\beta_1 x\)</span> is the linear combination of <span class="math inline">\((\hat\beta_0, \hat\beta_1)^T\)</span>,</p>
<p><span class="math display">\[\hat\mu_x \sim N\Big(E(\hat\mu_x), Var(\hat\mu_x)\Big)\]</span></p>
<p>From Theorem <a href="simple.html#thm:b01dist">1.11</a>,</p>
<p><span class="math display">\[E(\hat\mu_x) = E(\hat\beta_0) + E(\hat\beta_1)x = \beta_0 + \beta_1x \equiv \mu_x\]</span></p>
<p>and from Proposition <a href="simple.html#prp:vb">1.2</a></p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    Var(\hat\mu_x) &amp; = Var(\hat\beta_0 + \hat\beta_1 x) \\
    &amp; = Var(\hat\beta_0) + x^2Var(\hat\beta_1) + 2xCov(\hat\beta_0, \hat\beta_1) \\
    &amp; = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 + \frac{x^2\sigma^2}{S_{XX}} - \frac{2\overline{x}x\sigma^2}{S_{XX}} \\
    &amp; = \sigma^2\bigg(\frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)
  \end{split}
\end{equation*}\]</span>
</div>


<div class="corollary">
<span id="cor:mrdiff" class="corollary"><strong>Corollary 1.2  </strong></span><span class="math display">\[\hat\mu_x - \mu_x \sim N\bigg( 0, \sigma^2\bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)\]</span>
</div>

<p>Denote that in both Theorem <a href="simple.html#thm:mrdist">1.12</a> and Corollary <a href="simple.html#cor:mrdiff">1.2</a>, <span class="math inline">\(\sigma^2\)</span> is parameter. So to use <span class="math inline">\(SE(\hat\mu_x) = \sqrt{Var(\hat\mu_x)}\)</span> in practice we plug in its estimator, usually Equation <a href="simple.html#eq:siglse">(1.7)</a>.</p>

<div class="corollary">
<p><span id="cor:mrse" class="corollary"><strong>Corollary 1.3  (Standard error of mean response estimator)  </strong></span><span class="math display">\[\widehat{SE}(\hat\mu_x) = \hat\sigma^2\bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)\]</span></p>
where <span class="math inline">\(\hat\sigma^2 = MSE\)</span>
</div>

</div>
<div id="response" class="section level3">
<h3><span class="header-section-number">1.7.4</span> Response</h3>
<p>Our goal is to predict each response at each point, i.e. <span class="math inline">\(Y_x = \beta_0 + \beta_1 x + \epsilon_x\)</span>. <span class="math inline">\(\epsilon_x \sim N(0, \sigma^2)\)</span> is independent of the given data (<span class="math inline">\(\epsilon_1, \ldots, \epsilon_n\)</span>). In this sense, this prediction is called <em>out of sample prediction</em>. This setting makes difference between the <em>residuals, which are correlated to the data</em>. See Proposition <a href="simple.html#prp:resprop">1.4</a> for this. This is occurred because each <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> is linear combination of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, not <span class="math inline">\(Y_x\)</span>.</p>
<p>While <span class="math inline">\(Cov(Y_i, \hat{Y_i}) &gt; 0, i = 1, \ldots, n\)</span> (See Equation <a href="simple.html#eq:yyhat">(1.12)</a>), in case of out-of-sample <span class="math inline">\(Y_x\)</span>,</p>
<p><span class="math display">\[Cov(Y_x, \hat{Y_x}) = Cov(Y_x, \hat\beta_0 + \hat\beta_1 x) = 0\]</span></p>
<p>Hence, arithmetically, this <em>out of sample prediction becomes to have larger standard error</em>.</p>

<div class="proposition">
<span id="prp:bepsmvn" class="proposition"><strong>Proposition 1.7  (Joint distribution of coefficients and error term)  </strong></span><span class="math inline">\((\hat\beta_0, \hat\beta_1, \epsilon_x)^T\)</span> is Normally distributed.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Want 1: <span class="math inline">\((\hat\beta_0, \hat\beta_1)^T \perp\!\!\!\perp \epsilon_x\)</span></p>
<p>We have</p>
<p><span class="math display" id="eq:betaepsind">\[\begin{equation}
  \begin{split}
    Cov((\hat\beta_0, \hat\beta_1)^T, \epsilon_x) &amp; = \Big[Cov(\hat\beta_i, \epsilon_x) \Big]_{2 \times 1} \\
    &amp; = \bigg[Cov\bigg(\sum_{i = 1}^n k_i Y_i, \epsilon_x \bigg) \bigg]_{2 \times 1} \qquad k_i = \text{each linear coefficient for}\: \hat\beta_0, \hat\beta_1 \\
    &amp; = \mathbf{0}
  \end{split}
  \tag{1.17}
\end{equation}\]</span></p>
<p>From Equation <a href="simple.html#eq:b01mvn">(1.16)</a>,</p>
<p><span class="math display">\[(\hat\beta_0, \hat\beta_1)^T \sim MVN\]</span></p>
<p>and from assumption,</p>
<p><span class="math display">\[\epsilon_x \sim N(0, \sigma^2)\]</span></p>
<p>It follows from Equation <a href="simple.html#eq:betaepsind">(1.17)</a> that (<span class="citation">Johnson and Wichern (<a href="#ref-Johnson:2013aa" role="doc-biblioref">2013</a>)</span>)</p>
<p><span class="math display">\[(\hat\beta_0, \hat\beta_1)^T \perp\!\!\!\perp \epsilon_x\]</span></p>
<p>Want 2: <span class="math inline">\((\hat\beta_0, \hat\beta_1, \epsilon_x)^T \sim MVN\)</span></p>
<p>From independency, we have (<span class="citation">Johnson and Wichern (<a href="#ref-Johnson:2013aa" role="doc-biblioref">2013</a>)</span>)</p>
<span class="math display">\[
\begin{bmatrix}
  \hat\beta_0 \\
  \hat\beta_1 \\ \hline
  \epsilon_x
\end{bmatrix} \sim MVN_{2 + 1} \bigg( \begin{bmatrix}
  \beta_0 \\
  \beta_1 \\ \hline
  0
\end{bmatrix}, \left[
  \begin{array}{c|c}
    Cov(\boldsymbol{\hat\beta}) \in \mathbb{R}^{2 \times 2} &amp; \mathbf{0} \in \mathbb{R}^2 \\ \hline
    \mathbf{0}^T \in \mathbb{R}^{2 \times 1} &amp; \sigma^2
  \end{array}
\right] \bigg)
\]</span>
</div>

<p>This proposition gives clue to distribution of prediction error.</p>

<div class="theorem">
<p><span id="thm:preddist" class="theorem"><strong>Theorem 1.13  (Distribution of out-of-sample prediction error)  </strong></span>Out of sample prediction error <span class="math inline">\(\hat{Y_x} - Y_x\)</span> is Normally distributed</p>
<span class="math display">\[\hat{Y_x} - Y_x \sim N\bigg( 0, \sigma^2 \bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    \hat{Y_x} - Y_x &amp; = (\hat\beta_0 + \hat\beta_1 x) - (\beta_0 + \beta_1 x + \epsilon_x) \\
    &amp; = [1, x, -1] (\hat\beta_0, \hat\beta_1, \epsilon_x)^T - \beta_0 - \beta_1 x
  \end{split}
\end{equation*}\]</span></p>
<p>i.e. <span class="math inline">\(\hat{Y_x} - Y_x\)</span> is a linear combination of <span class="math inline">\((\hat\beta_0, \hat\beta_1, \epsilon_x)^T\)</span>. From prosition <a href="simple.html#prp:bepsmvn">1.7</a>,</p>
<span class="math display" id="eq:prederrmvn">\[\begin{equation}
  \begin{split}
    \hat{Y_x} - Y_x &amp; \sim MVN \Bigg( [1, x, -1]\begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    0
    \end{bmatrix} - \beta_0 - \beta_1 x,
    [1, x, -1]
    \left[
      \begin{array}{c|c}
        Cov(\boldsymbol{\hat\beta}) \in \mathbb{R}^{2 \times 2} &amp; \mathbf{0} \in \mathbb{R}^2 \\ \hline
        \mathbf{0}^T \in \mathbb{R}^{2 \times 1} &amp; \sigma^2
      \end{array}
    \right]
    \begin{bmatrix}
      1 \\
      x \\
      -1
    \end{bmatrix}
     \Bigg) \\
    &amp; \stackrel{d}{=} MVN \bigg( 0, \sigma^2\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} - 2 \frac{\overline{x}x}{S_{XX}} + \frac{x^2}{S_{XX}} \bigg) + 1 \bigg) \\
    &amp; \stackrel{d}{=} MVN \bigg( 0, \sigma^2\bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)
  \end{split}
  \tag{1.18}
\end{equation}\]</span>
</div>

<p>Now we know the standard error of this out-of-sample prediction error.</p>
<p><span class="math display">\[SE(\hat{Y_x} - Y_x) = \sigma^2\bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)\]</span></p>
<p>We can see this standard error is <em>always larger than of mean response estimator</em> due to <span class="math inline">\(1\)</span> in the bracket, i.e. <span class="math inline">\(\sigma^2\)</span>. As mentioned, this is due to <span class="math inline">\(\epsilon\)</span> term. When we estimate or predict the mean response the model have been assumed to be true. In this out-of-sample prediction setting, however, the model can be wrong. This assumption error is also included in <span class="math inline">\(\epsilon\)</span> term and it is called <em>irreducible error</em>, which cannot be reduced anymore.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> <span class="math display">\[SE(\hat\mu_x - \mu_x) &lt; SE(\hat{Y_x} - Y_x)\]</span>
</div>

<p>It might be more clear if we see the inequality in the above remark. We know the fact that <span class="math inline">\(\hat{Y_x}\)</span> and <span class="math inline">\(Y_x\)</span> are uncorrelated in this out-of-sample setting. <span class="math inline">\(Y_x\)</span> is random variable, while <span class="math inline">\(\mu_x\)</span> is constant. Then we can re-express the inequality as</p>
<p><span class="math display">\[SE(\hat\mu_x) &lt; SE(\hat{Y_x}) + SE(Y_x)\]</span></p>
<p>Actually, both <span class="math inline">\(\hat\mu_x\)</span> and <span class="math inline">\(\hat{Y_x}\)</span> are estimated as <span class="math inline">\(\hat\beta_0 + \hat\beta_1 x\)</span>. Thus, <span class="math inline">\(SE(Y_x) = \sigma^2\)</span> makes out-of-sample more noisy.</p>
<p>To use standard error practically, we use <span class="math inline">\(\hat\sigma^2\)</span> as in corollary <a href="simple.html#cor:mrse">1.3</a>.</p>

<div class="corollary">
<p><span id="cor:predse" class="corollary"><strong>Corollary 1.4  (Standard error of out-of-sample prediction error)  </strong></span><span class="math display">\[\widehat{SE}(\hat{Y_x} - Y_x) = \hat\sigma^2\bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)\]</span></p>
where <span class="math inline">\(\hat\sigma^2 = MSE\)</span>
</div>

</div>
</div>
<div id="statistical-inference" class="section level2">
<h2><span class="header-section-number">1.8</span> Statistical Inference</h2>
<p>Based on each distribution of estimator in section <a href="simple.html#simpledist">1.7</a>, we can construct various inferece for each</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span></li>
<li><span class="math inline">\(\beta_1\)</span></li>
<li><span class="math inline">\(\mu_x\)</span></li>
<li><span class="math inline">\(Y_x\)</span></li>
<li><span class="math inline">\(\sigma^2\)</span></li>
</ul>
<p>We can get the standard error for each coefficient through <code>summary()</code> function.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(delv_fit)</code></pre>
<pre><code>
Call:
lm(formula = y ~ x, data = delv)

Residuals:
   Min     1Q Median     3Q    Max 
-7.581 -1.874 -0.349  2.181 10.634 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    3.321      1.371    2.42    0.024 *  
x              2.176      0.124   17.55  8.2e-15 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 4.18 on 23 degrees of freedom
Multiple R-squared:  0.93,  Adjusted R-squared:  0.927 
F-statistic:  308 on 1 and 23 DF,  p-value: 8.22e-15</code></pre>
<p>Or more state-or-art way, <code>broom:tidy()</code> function has a method for each model object to make tidy data: <code>tibble</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">broom<span class="op">::</span><span class="kw">tidy</span>(delv_fit)</code></pre>
<pre><code># A tibble: 2 x 5
  term        estimate std.error statistic  p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1 (Intercept)     3.32     1.37       2.42 2.37e- 2
2 x               2.18     0.124     17.5  8.22e-15</code></pre>
<div id="confidence-interval" class="section level3">
<h3><span class="header-section-number">1.8.1</span> Confidence interval</h3>
<p>Consider standardization.</p>
<p><span class="math display">\[\frac{\hat\theta - \theta}{\sqrt{SE(\hat\theta)}}\]</span></p>
<p>Each <span class="math inline">\(SE\)</span> includes <span class="math inline">\(\sigma^2\)</span> as we have already seen. First think about <strong>known</strong> <span class="math inline">\(\sigma^2\)</span> setting. All three estimators follow Normal distribution, and <span class="math inline">\(SE\)</span> is constant by our the setting. Then we can construct each confidence interval as</p>
<p><span class="math display">\[\hat\theta \pm z_{\frac{\alpha}{2}} \sqrt{SE(\hat\theta)}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:estci"></span>
<img src="regression-analysis_files/figure-html/estci-1.png" alt="Confidence Interval when $\sigma^2$ is known" width="70%" />
<p class="caption">
Figure 1.8: Confidence Interval when <span class="math inline">\(\sigma^2\)</span> is known
</p>
</div>
<p>Now just plug in the results of section <a href="simple.html#simpledist">1.7</a>. For each regression coefficient,</p>

<div class="proposition">
<p><span id="prp:betaci" class="proposition"><strong>Proposition 1.8  (Confidence intervals on <span class="math inline">\(\beta\)</span>)  </strong></span>With known <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\((1 - \alpha)100 \%\)</span> confidence intervals on <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are given as</p>
<p><span class="math display">\[\beta_0 : \quad \hat\beta_0 \pm z_{\frac{\alpha}{2}} \sqrt{\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \sigma^2}\]</span></p>
<span class="math display">\[\beta_1 : \quad \hat\beta_1 \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma^2}{S_{XX}}}\]</span>
</div>


<div class="proposition">
<p><span id="prp:mrci" class="proposition"><strong>Proposition 1.9  (Confidence interval on <span class="math inline">\(\hat\mu_x\)</span>)  </strong></span>With known <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\((1 - \alpha)100 \%\)</span> confidence interval on <span class="math inline">\(\hat\mu_x\)</span> is given as</p>
<span class="math display">\[\mu_x : \quad \hat\mu_x \pm z_{\frac{\alpha}{2}} \sqrt{\sigma^2 \bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}\]</span>
</div>

<p>In practice, however, we do not know <span class="math inline">\(\sigma^2\)</span>. In this case, we replace <span class="math inline">\(\sigma^2\)</span> with <span class="math inline">\(\hat\sigma^2 = \frac{1}{n - 2}\sum\limits_{i = 1}^n (\hat{Y_i} - Y_i)^2 = MSE\)</span>. Then</p>
<p><span class="math display">\[\frac{\hat\theta - \theta}{\sqrt{\widehat{SE}}} = \frac{\frac{\hat\theta - \theta}{\sqrt{SE = \sigma^2(\cdot)}}}{\sqrt{\frac{\frac{SSE}{\sigma^2}}{n - 2}\bigg( \cdot \bigg)}} =  \frac{\frac{\hat\theta - \theta}{\sqrt{SE = \sigma^2}} \sim N(0, 1)}{\sqrt{\frac{\frac{SSE}{\sigma^2} \sim \chi^2(n - 2)}{n - 2}}} \sim t(n - 2)\]</span></p>
<p>Thus, we need to replace <span class="math inline">\(z_{\frac{\alpha}{2}}\)</span> with <span class="math inline">\(t_{\frac{\alpha}{2}}(n - 2)\)</span>.</p>

<div class="proposition">
<p><span id="prp:betaci2" class="proposition"><strong>Proposition 1.10  (Confidence intervals on <span class="math inline">\(\beta\)</span> when unknown <span class="math inline">\(\sigma^2\)</span>)  </strong></span>With unknown <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\((1 - \alpha)100 \%\)</span> confidence intervals on <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are given as</p>
<p><span class="math display">\[\beta_0 : \quad \hat\beta_0 \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \hat\sigma^2}\]</span></p>
<p><span class="math display">\[\beta_1 : \quad \hat\beta_1 \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\frac{\hat\sigma^2}{S_{XX}}}\]</span></p>
where <span class="math inline">\(\hat\sigma^2 = MSE\)</span>
</div>

<p>Here we can estimate the intervals. Basically, <code>confint()</code> function gives this interval.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(delv_fit, <span class="dt">level =</span> <span class="fl">.95</span>)</code></pre>
<pre><code>            2.5 % 97.5 %
(Intercept) 0.484   6.16
x           1.920   2.43</code></pre>

<div class="proposition">
<p><span id="prp:mrci2" class="proposition"><strong>Proposition 1.11  (Confidence interval on <span class="math inline">\(\hat\mu_x\)</span> when unknown <span class="math inline">\(\sigma^2\)</span>)  </strong></span>With unknown <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\((1 - \alpha)100 \%\)</span> confidence interval on <span class="math inline">\(\hat\mu_x\)</span> is given as</p>
<p><span class="math display">\[\mu_x : \quad \hat\mu_x \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\hat\sigma^2 \bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}\]</span></p>
where <span class="math inline">\(\hat\sigma^2 = MSE\)</span>
</div>

<p><code>predict()</code> provides options for this confidence interval. Specify <code>interval = "confidence"</code>. This argument has three option.</p>
<ol style="list-style-type: decimal">
<li><code>"none"</code>: just compute fitted value, by default.</li>
<li><code>"confidence"</code>: confidence interval of mean response</li>
<li><code>"prediction"</code>: prediction interval of out-of-sample prediction</li>
</ol>
<p>Default <code>level</code> is <code>0.95</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(delv_fit, <span class="dt">interval =</span> <span class="st">&quot;confidence&quot;</span>, <span class="dt">level =</span> <span class="fl">.95</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tbl_df</span>()</code></pre>
<pre><code># A tibble: 25 x 3
     fit   lwr   upr
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
 1 18.6  16.8   20.3
 2  9.85  7.57  12.1
 3  9.85  7.57  12.1
 4 12.0   9.91  14.1
 5 16.4  14.5   18.2
 6 18.6  16.8   20.3
 7  7.67  5.22  10.1
 8 18.6  16.8   20.3
 9 68.6  62.9   74.3
10 14.2  12.2   16.2
# … with 15 more rows</code></pre>
</div>
<div id="prediction-interval" class="section level3">
<h3><span class="header-section-number">1.8.2</span> Prediction interval</h3>
<p>One proceeds in a similar way for out-of-sample <span class="math inline">\(Y_x\)</span>.</p>

<div class="proposition">
<p><span id="prp:predci" class="proposition"><strong>Proposition 1.12  (Prediction interval on <span class="math inline">\(\hat{Y_x}\)</span>)  </strong></span>With known <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\((1 - \alpha)100 \%\)</span> confidence interval on <span class="math inline">\(\hat\mu_x\)</span> is given as</p>
<span class="math display">\[Y_x : \quad \hat{Y_x} \pm z_{\frac{\alpha}{2}} \sqrt{\sigma^2 \bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}\]</span>
</div>

<p>Also, with unknown <span class="math inline">\(\sigma^2\)</span>,</p>

<div class="proposition">
<p><span id="prp:predci2" class="proposition"><strong>Proposition 1.13  (Prediction interval on <span class="math inline">\(\hat{Y_x}\)</span> when unknown <span class="math inline">\(\sigma^2\)</span>)  </strong></span>With unknown <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\((1 - \alpha)100 \%\)</span> confidence interval on <span class="math inline">\(\hat\mu_x\)</span> is given as</p>
<p><span class="math display">\[Y_x : \quad \hat{Y_x} \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\hat\sigma^2 \bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}\]</span></p>
where <span class="math inline">\(\hat\sigma^2 = MSE\)</span>
</div>

<p>Since this is out-of-sample setting, we should also give <code>newdata</code> option. Otherwise, we will get warning message. Denote that this argument only receive <code>data.frame</code> object with same element names.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(delv_fit, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="dv">31</span><span class="op">:</span><span class="dv">35</span>), <span class="dt">interval =</span> <span class="st">&quot;prediction&quot;</span>, <span class="dt">level =</span> <span class="fl">.95</span>)</code></pre>
<pre><code>   fit  lwr  upr
1 70.8 60.3 81.3
2 73.0 62.3 83.6
3 75.1 64.3 85.9
4 77.3 66.4 88.3
5 79.5 68.4 90.6</code></pre>
</div>
<div id="hypothesis-testing" class="section level3">
<h3><span class="header-section-number">1.8.3</span> Hypothesis testing</h3>
<p>Look again the output of <code>summary.lm()</code> and <code>broom::tidy.lm()</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(delv_fit)</code></pre>
<pre><code>
Call:
lm(formula = y ~ x, data = delv)

Residuals:
   Min     1Q Median     3Q    Max 
-7.581 -1.874 -0.349  2.181 10.634 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    3.321      1.371    2.42    0.024 *  
x              2.176      0.124   17.55  8.2e-15 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 4.18 on 23 degrees of freedom
Multiple R-squared:  0.93,  Adjusted R-squared:  0.927 
F-statistic:  308 on 1 and 23 DF,  p-value: 8.22e-15</code></pre>
<p>We can see <code>t value</code> and <code>Pr(&gt;|t|)</code>. At the same time, <code>statistic</code> and <code>p.value</code>. What are these values? These are the results of the following tests.</p>
<p><span class="math display">\[H_0 : \beta_0 = \alpha_0 \qquad \text{vs} \qquad H_1 : \beta_0 \neq \alpha_0\]</span></p>
<p><span class="math display" id="eq:b0test">\[\begin{equation}
  T = \frac{\hat\beta_0 - \alpha_0}{\hat\sigma \sqrt{\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}} \bigg)}} \stackrel{H_0}{\sim} t(n - 2)
  \tag{1.19}
\end{equation}\]</span></p>
<p>For this test statistic <a href="simple.html#eq:b0test">(1.19)</a>,</p>
<p><span class="math display">\[\text{reject}\: H_0 \quad \text{if} \: \lvert T \rvert &gt; t_{\frac{\alpha}{2}}(n - 2)\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:b0rjt"></span>
<img src="regression-analysis_files/figure-html/b0rjt-1.png" alt="Rejection region for $\beta_0$" width="70%" />
<p class="caption">
Figure 1.9: Rejection region for <span class="math inline">\(\beta_0\)</span>
</p>
</div>
<p>More importantly, we test <span class="math inline">\(\beta_1\)</span> which means slope</p>
<p><span class="math display">\[H_0 : \beta_1 = \alpha_1 \qquad \text{vs} \qquad H_1 : \beta_1 \neq \alpha_1\]</span></p>
<p><span class="math display" id="eq:b1test">\[\begin{equation}
  T = \frac{\hat\beta_0 - \alpha_0}{\hat\sigma \sqrt{\frac{1}{S_{xx}}}} \stackrel{H_0}{\sim} t(n - 2)
  \tag{1.20}
\end{equation}\]</span></p>
<p>For this test statistic <a href="simple.html#eq:b1test">(1.20)</a>,</p>
<p><span class="math display">\[\text{reject}\: H_0 \quad \text{if} \: \lvert T \rvert &gt; t_{\frac{\alpha}{2}}(n - 2)\]</span></p>
<p>Looking at these two statistics, we can intuitively know the meaning. As <span class="math inline">\(\lvert \hat\beta_1 - \alpha_1 \rvert\)</span> becomes larger, the data support <span class="math inline">\(H_1\)</span>.</p>
</div>
</div>
<div id="analysis-of-variance" class="section level2">
<h2><span class="header-section-number">1.9</span> Analysis of Variance</h2>
<div id="useful-distributions" class="section level3">
<h3><span class="header-section-number">1.9.1</span> Useful distributions</h3>
<p>In linear regression setting, we usually assume <span class="math inline">\(\epsilon_i \stackrel{iid}{\sim}N(0, \sigma^2)\)</span>. There are some useful distributions around Normal.</p>

<div class="proposition">
<p><span id="prp:chisq" class="proposition"><strong>Proposition 1.14  (<span class="math inline">\(\chi^2\)</span>-distribution)  </strong></span>Square of standard normal follows <span class="math inline">\(\chi^2\)</span>-distribution.</p>
<p>If <span class="math inline">\(Z \sim N(0, 1)\)</span>, then <span class="math inline">\(Z^2 \sim \chi^2(1)\)</span></p>
If <span class="math inline">\(Z_i \stackrel{indep}{\sim}N(0, 1)\)</span>, then <span class="math inline">\(Z_1^2 + \cdots + Z_n^2 \sim \chi^2(n)\)</span>
</div>


<div class="proposition">
<p><span id="prp:tdist" class="proposition"><strong>Proposition 1.15  (t-distribution)  </strong></span>Let <span class="math inline">\(Z \sim N(0, 1) \perp\!\!\!\perp V \sim \chi^2(m)\)</span>. Then</p>
<span class="math display">\[T = \frac{Z}{\sqrt{V / m}} \sim t(m)\]</span>
</div>


<div class="proposition">
<p><span id="prp:fdist" class="proposition"><strong>Proposition 1.16  (F-distribution)  </strong></span>Let <span class="math inline">\(V \sim \chi^2(m) \perp\!\!\!\perp W \sim \chi^2(n)\)</span>. Then</p>
<span class="math display">\[F = \frac{V / m}{W / n} \sim F(m, n)\]</span>
</div>

<p>Also, there is <em>non-central analogue</em> of these three distributions, i.e. starting from <span class="math inline">\(Z \sim N(\mu, 1)\)</span>.</p>

<div class="proposition">
<p><span id="prp:nonchi" class="proposition"><strong>Proposition 1.17  (Noncentral <span class="math inline">\(\chi^2\)</span>-distribution)  </strong></span>Square of scaled normal follows non-central <span class="math inline">\(\chi^2\)</span>-distribution.</p>
<p>If <span class="math inline">\(Z_i \stackrel{indep}{\sim}N(\mu_i, 1)\)</span>, then <span class="math inline">\(Z_1^2 + \cdots + Z_n^2 \sim \chi^2(n, \sum\limits_{i = 1}^n \mu_i^2)\)</span></p>
<span class="math inline">\(\sum\limits_{i = 1}^n \mu_i^2\)</span> is called a non-central parameter.
</div>


<div class="proposition">
<p><span id="prp:nontdist" class="proposition"><strong>Proposition 1.18  (Noncentral t-distribution)  </strong></span>Let <span class="math inline">\(X \sim N(\mu, 1) \perp\!\!\!\perp V \sim \chi^2(m)\)</span>. Then</p>
<p><span class="math display">\[T = \frac{Z}{\sqrt{V / m}} \sim t(m, \mu)\]</span></p>
<span class="math inline">\(\mu\)</span> is called a non-central parameter.
</div>


<div class="proposition">
<p><span id="prp:nonfdist" class="proposition"><strong>Proposition 1.19  (Noncentral F-distribution)  </strong></span>Let <span class="math inline">\(V \sim \chi^2(m, \delta) \perp\!\!\!\perp W \sim \chi^2(n)\)</span>. Then</p>
<p><span class="math display">\[F = \frac{V / m}{W / n} \sim F(m, n, \delta)\]</span></p>
<span class="math inline">\(\delta\)</span> is called a non-central parameter.
</div>

</div>
<div id="quadratic-form" class="section level3">
<h3><span class="header-section-number">1.9.2</span> Quadratic form</h3>
<p>Now we can determine the distributions of various quadratic forms. The reason we are taking care of this is ANOVA deals with sum of squares, i.e. quadratic form.</p>

<div class="theorem">
<p><span id="thm:idem" class="theorem"><strong>Theorem 1.14  (Idempotent and symmetric)  </strong></span>Let <span class="math inline">\(A \in \mathbb{R}^{k \times k}\)</span> be idempotent and symmetric. Then</p>
<p><span class="math inline">\(\text{(a)}\: A^n\)</span> is also idempotent</p>
<p><span class="math inline">\(\text{(b)}\: I - A\)</span> is also idempotent</p>
<span class="math inline">\(\text{(c)}\:\)</span> Every eigenvalue of <span class="math inline">\(A\)</span> is either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> so that <span class="math inline">\(tr(A) = rank(A)\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> (a) and (b) are trivial.</p>
<p><span class="math display">\[(A^n)^2 = (A^2)^n = A^n\]</span></p>
<p><span class="math display">\[(I - A)^2 = I - 2A + A^2 = I - A\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p>Fix <span class="math inline">\(\lambda\)</span> an eigenvalue of <span class="math inline">\(A\)</span>. Let <span class="math inline">\(\mathbf{v} \neq \mathbf{0}\)</span> be the corresponding eigenvector.</p>
<p>By definition,</p>
<p><span class="math display">\[A\mathbf{v} = \lambda \mathbf{v}\]</span></p>
<p>Then</p>
<p><span class="math display">\[A^2\mathbf{v} = \lambda(A\mathbf{v}) = \lambda^2\mathbf{v}\]</span></p>
<p>and so <span class="math inline">\(\lambda^2\)</span> is eigenvalue of <span class="math inline">\(A^2\)</span>.</p>
<p>Since <span class="math inline">\(A^2 = A\)</span>,</p>
<p><span class="math display">\[\lambda = \lambda^2\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[\lambda = 0 \:\text{or}\: 1\]</span></p>
<p>Note that for every matrix and its eigenvalues <span class="math inline">\(\lambda_j\)</span></p>
<p><span class="math display">\[tr(X) = \sum_{j = 1}^p \lambda_j, \quad rank(X) = \text{the number of non-zero}\: \lambda_j\]</span></p>
<p>Since <span class="math inline">\(\lambda = 0, 1\)</span> of A,</p>
<span class="math display">\[tr(A) = rank(A)\]</span>
</div>


<div class="proposition">
<p><span id="prp:quadmvn" class="proposition"><strong>Proposition 1.20  (Independence)  </strong></span>Assume <span class="math inline">\(\mathbf{Y} \sim MVN(\mathbf\mu, \Sigma)\)</span>. Then</p>
<ol style="list-style-type: lower-roman">
<li>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are symmetric,</li>
</ol>
<p><span class="math display">\[Y^T AY \perp\!\!\!\perp Y^T BY \Leftrightarrow A\Sigma B = 0\]</span></p>
<ol start="2" style="list-style-type: lower-roman">
<li>If <span class="math inline">\(A\)</span> is symmetric,</li>
</ol>
<span class="math display">\[Y^T AY \perp\!\!\!\perp BY \Leftrightarrow B\Sigma A = 0\]</span>
</div>


<div class="theorem">
<p><span id="thm:quaddist" class="theorem"><strong>Theorem 1.15  (Distribution of quadratic form)  </strong></span>Assume that <span class="math inline">\(\mathbf{Y} \sim MVN(\mathbf\mu, I)\)</span> and that <span class="math inline">\(A\)</span> is symmetric and idempotent. Then</p>
<p><span class="math display">\[Y^T AT \sim \chi^2(K, \delta)\]</span></p>
<p>where <span class="math inline">\(K = rank(A)\)</span> and <span class="math inline">\(\delta = \boldsymbol\mu^T A \boldsymbol\mu\)</span>. Furthermore,</p>
<span class="math display">\[
\begin{cases}
  E(Y^T AT) = K + \delta \\
  Var(Y^T AT) = 2(K + 2\delta)
\end{cases}
\]</span>
</div>


<div class="corollary">
<p><span id="cor:mvnchi" class="corollary"><strong>Corollary 1.5  (Inner product of standard normal vector)  </strong></span>Let <span class="math inline">\(\mathbf{Z} = (Z_1, \ldots, Z_n)^T \sim MVN(\mathbf{0}, I_n)\)</span>. Then</p>
<span class="math display">\[\mathbf{Z}^T\mathbf{Z} = \sum_{i = 1}^n Z_i^2 \sim \chi^2(n)\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From Theorem <a href="simple.html#thm:quaddist">1.15</a> point of view,</p>
<p><span class="math display">\[\mathbf{Z}^T\mathbf{Z} = \mathbf{Z}^T I_n \mathbf{Z}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[K = rank(I_n) = n\]</span></p>
<span class="math display">\[\delta = \mathbf{0}\]</span>
</div>

<p>Using the above facts, we can now show distributions of sums of squares. First recall that</p>
<p><span class="math display">\[\mathbf{Y} \sim MVN(X\boldsymbol\beta, \sigma^2 I)\]</span></p>

<div class="proposition">
<span id="prp:ssedist" class="proposition"><strong>Proposition 1.21  (Distribution of SSE)  </strong></span><span class="math display">\[\frac{SSE}{\sigma^2} \sim \chi^2(n - 2, 0)\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From Corollary <a href="simple.html#cor:projss">1.1</a>, write</p>
<p><span class="math display">\[\frac{SSE}{\sigma^2} = \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)^T (I - \Pi_X) \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)\]</span></p>
<p>Note that</p>
<p><span class="math display">\[\frac{\mathbf{Y}}{\sigma} \sim MVN(\frac{1}{\sigma}X\boldsymbol\beta, I)\]</span></p>
<p>Since <span class="math inline">\(I - \Pi_X\)</span> is idempotent and symmetric,</p>
<p><span class="math display">\[K = rank(I - \Pi_X) = tr(I - \Pi_X) = n - rank(\Pi_X) = n - 2\]</span></p>
<p><span class="math display">\[\begin{equation} \label{eq:delta1}
  \begin{split}
    \delta &amp; = \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg)^T (I - \Pi_X) \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg) \\
    &amp; = \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} - \frac{(\boldsymbol\beta^TX^T)X(X^TX)^{-1}X^T(X\boldsymbol\beta)}{\sigma^2} \\
    &amp; = \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} - \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} \\
    &amp; = 0
  \end{split}
\end{equation}\]</span></p>
<p>Hence,</p>
<span class="math display">\[\frac{SSE}{\sigma^2} \sim \chi^2(n - 2)\]</span>
</div>


<div class="proposition">
<p><span id="prp:ssrdist" class="proposition"><strong>Proposition 1.22  (Distribution of SSR)  </strong></span><span class="math display">\[\frac{SSR}{\sigma^2} \sim \chi^2(1, \delta)\]</span></p>
where <span class="math inline">\(\delta = \sum\limits_{i = 1}^n (x_i - \overline{x})^2 \beta_1^2 = \frac{S_{xx}\beta_1^2}{\sigma^2}\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From Corollary <a href="simple.html#cor:projss">1.1</a>, write</p>
<p><span class="math display">\[\frac{SSR}{\sigma^2} = \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)^T (\Pi_X - \Pi_{\mathbf{1}}) \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)\]</span></p>
<p>Note that <span class="math inline">\(\Pi_X - \Pi_{\mathbf{1}}\)</span> is symmetric idempotent. One proceeds in a similar way.</p>
<p><span class="math display">\[K = rank(\Pi_X - \Pi_{\mathbf{1}}) = tr(\Pi_X - \Pi_{\mathbf{1}}) = rank(\Pi_X) - rank(\Pi_{\mathbf{1}}) = 2 - 1 = 1\]</span></p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    \delta &amp; = \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg)^T (\Pi_X - \Pi_{\mathbf{1}}) \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg) \qquad \because \frac{\mathbf{Y}}{\sigma} \sim MVN(\frac{1}{\sigma}X\boldsymbol\beta, I) \\
    &amp; = \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} - \frac{\boldsymbol\beta^TX^T\Pi_{\mathbf{1}}X\boldsymbol\beta}{\sigma^2} \\
    &amp; = \frac{\boldsymbol\beta^T(X^TX - X^T \Pi_{\mathbf{1}}X)\boldsymbol\beta}{\sigma^2} \\
    &amp; = \frac{\boldsymbol\beta^T \Big\{ X^T(I - \Pi_{\mathbf{1}})X \Big\} \boldsymbol\beta }{\sigma^2}
  \end{split}
\end{equation*}\]</span></p>
<p>Since <span class="math inline">\(\mathbf{1} \in sp(\{ \mathbf{1} \})\)</span>,</p>
<p><span class="math display">\[\Pi_{\mathbf{1}} \mathbf{1} = \mathbf{1}\]</span></p>
<p>It gives that</p>
<p><span class="math display">\[\mathbf{1}^T(I - \Pi_{\mathbf{1}})\mathbf{1} = 0\]</span></p>
<p>If <span class="math inline">\(\mathbf{x} \neq \mathbf{1}\)</span>, then we have</p>
<p><span class="math display">\[\mathbf{x}^T(I - \Pi_{\mathbf{1}})\mathbf{x} = \sum_{i = 1}^n (x_i - \overline{x})^2 = S_{xx}\]</span></p>
<p>Recall that</p>
<p><span class="math display">\[\overline{x}\mathbf{1} = \mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T\mathbf{x} = \Pi_{\mathbf{1}}\mathbf{x}\]</span></p>
<p>Then we have</p>
<p><span class="math display">\[\mathbf{1}^T(I - \Pi_{\mathbf{1}})\mathbf{x} = \sum x_i - n \overline{x} = 0\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[\mathbf{x}^T(I - \Pi_{\mathbf{1}})\mathbf{1} = n \overline{x} - \sum x_i = 0\]</span></p>
<p>Hence by partitioning <span class="math inline">\(X = [\mathbf{1} \mid \mathbf{x}]\)</span>,</p>
<span class="math display">\[\begin{equation} \label{eq:delta2}
  \begin{split}
    \delta &amp; = \frac{\boldsymbol\beta^T \Big\{ [\mathbf{1} \mid \mathbf{x}]^T(I - \Pi_{\mathbf{1}})[\mathbf{1} \mid \mathbf{x}] \Big\} \boldsymbol\beta }{\sigma^2} \\
    &amp; = \frac{\boldsymbol\beta^T \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; S_{xx} \end{bmatrix} \boldsymbol\beta}{\sigma^2} \\
    &amp; = \frac{S_{xx}\beta_1^2}{\sigma^2}
  \end{split}
\end{equation}\]</span>
</div>


<div class="proposition">
<p><span id="prp:ssind" class="proposition"><strong>Proposition 1.23  (Independence)  </strong></span>SSE and SSR are independent, i.e.</p>
<span class="math display">\[SSE \perp\!\!\!\perp SSR\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that both <span class="math inline">\(SSE\)</span> and <span class="math inline">\(SSR\)</span> are quadratic forms of <span class="math inline">\(\mathbf{Y} \sim MVN(X\boldsymbol\beta, \sigma^2 I)\)</span> and that each <span class="math inline">\(I - \Pi_X\)</span> and <span class="math inline">\(\Pi_X - \Pi_{\mathbf{1}}\)</span> is symmetric. Then from Proposition <a href="simple.html#prp:quadmvn">1.20</a>,</p>
<p>Claim: <span class="math inline">\((I - \Pi_X)(\sigma^2I)(\Pi_X - \Pi_{\mathbf{1}}) = 0\)</span>, i.e. <span class="math inline">\((I - \Pi_X)(\Pi_X - \Pi_{\mathbf{1}}) = 0\)</span></p>
<p>It is obvious that</p>
<p><span class="math display">\[\Pi_X\Pi_{\mathbf{1}} = \Pi_{\mathbf{1}}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    (I - \Pi_X)(\Pi_X - \Pi_{\mathbf{1}}) &amp; = \Pi_X - \Pi_{\mathbf{1}} - \Pi_X^2 + \Pi_X\Pi_{\mathbf{1}} \\
    &amp; = \Pi_X - \Pi_{\mathbf{1}} - \Pi_X + \Pi_{\mathbf{1}} \qquad \because \text{idempotent} \\
    &amp; = 0
  \end{split}
\end{equation*}\]</span></p>
This completes the proof.
</div>


<div class="proposition">
<p><span id="prp:ssbind" class="proposition"><strong>Proposition 1.24  (Independence)  </strong></span>SSE and <span class="math inline">\((\hat\beta_0, \hat\beta_1)\)</span> are independent, i.e.</p>
<span class="math display">\[SSE \perp\!\!\!\perp(\hat\beta_0, \hat\beta_1)^T\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that</p>
<p><span class="math display">\[\boldsymbol{\hat\beta}= (\hat\beta_0, \hat\beta_1)^T = (X^TX)^{-1}X^T\mathbf{Y}\]</span></p>
<p>Since <span class="math inline">\(I - \Pi_X\)</span> of <span class="math inline">\(SSE\)</span> is symmetric, from Proposition <a href="simple.html#prp:quadmvn">1.20</a>,</p>
<p>Claim: <span class="math inline">\(((X^TX)^{-1}X^T)(\sigma^2I)(I - \Pi_X) = 0\)</span>, i.e. <span class="math inline">\(((X^TX)^{-1}X^T)(I - \Pi_X) = 0\)</span></p>
<p>Since <span class="math inline">\(\Pi_X = X(X^TX)^{-1}X^T\)</span>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    ((X^TX)^{-1}X^T)(I - \Pi_X) &amp; = (X^TX)^{-1}X^T - (X^TX)^{-1}X^TX(X^TX)^{-1}X^T \\
    &amp; = (X^TX)^{-1}X^T - (X^TX)^{-1}X^T \\
    &amp; = 0
  \end{split}
\end{equation*}\]</span></p>
This completes the proof.
</div>


<div class="proposition">
<p><span id="prp:sstdist" class="proposition"><strong>Proposition 1.25  (Distribution of SST)  </strong></span><span class="math display">\[\frac{SST}{\sigma^2} \sim \chi^2(n - 1, \delta)\]</span></p>
where <span class="math inline">\(\delta = \sum\limits_{i = 1}^n (x_i - \overline{x})^2 \beta_1^2 = \frac{S_{xx}\beta_1^2}{\sigma^2}\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> It proceedes in a similary way from Corollary <a href="simple.html#cor:projss">1.1</a></p>
<p><span class="math display">\[\frac{SST}{\sigma^2} = \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)^T (I - \Pi_{\mathbf{1}}) \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)\]</span></p>
<p>Since <span class="math inline">\(I - \Pi_{\mathbf{1}}\)</span> is symmetric idempotent,</p>
<p><span class="math display">\[K = rank(I - \Pi_{\mathbf{1}}) = tr(I - \Pi_{\mathbf{1}}) = n - rank(\Pi_{\mathbf{1}}) = n - 1\]</span></p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \delta &amp; = \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg)^T (I - \Pi_{\mathbf{1}}) \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg) \\
    &amp; = \frac{S_{xx}\beta_1^2}{\sigma^2} \qquad \because \eqref{eq:delta1} \:\text{and}\: \eqref{eq:delta2} 
  \end{split}
\end{equation*}\]</span>
</div>

</div>
<div id="anova-for-testing-significance-of-regression" class="section level3">
<h3><span class="header-section-number">1.9.3</span> ANOVA for testing significance of regression</h3>
<p>Recall that</p>
<p><span class="math display">\[SST = SSR + SSE\]</span></p>
<ul>
<li><span class="math inline">\(SST\)</span>: the variation of a response itself</li>
<li><span class="math inline">\(SSR\)</span>: the variation of a response <em>explained by the model</em></li>
<li><span class="math inline">\(SSE\)</span>: the variation of a response that <em>cannot be explained by the model</em></li>
</ul>
<p>As mentioned in section <a href="simple.html#decompsst">1.5.4</a>, whether the model is useful or not can depend on the proportion of <span class="math inline">\(SSR\)</span> versus <span class="math inline">\(SSE\)</span> in constant <span class="math inline">\(SST\)</span>. When <span class="math inline">\(SSR\)</span> is large compared to <span class="math inline">\(SSE\)</span>, we can say that the model is good. On the other hand, when <span class="math inline">\(SSR\)</span> is not large, the model might be poor. This is what <span class="math inline">\(R^2\)</span> measures intuitively.</p>
<p>However, this direct comparison somtimes does not work in many times. Both <span class="math inline">\(SSR\)</span> and <span class="math inline">\(SSE\)</span> comes from different distribution, which have different degrees of freedom. So we <em>compare standardized versions</em>, i.e. divided by the degrees of freedom.</p>

<div class="definition">
<p><span id="def:dof" class="definition"><strong>Definition 1.11  (Degrees of freedom)  </strong></span>Degrees of freedom of each sum of squares is</p>
<span class="math display">\[df = \text{the number of deviation} - \text{the number of linear constraints}\]</span>
</div>


<div class="corollary">
<p><span id="cor:dfss" class="corollary"><strong>Corollary 1.6  (df of SS)  </strong></span><span class="math inline">\(df\)</span> of each sum of square is computed as</p>
<p><span class="math inline">\(\text{(a)}\: df(SST) = n - 1\)</span></p>
<p><span class="math inline">\(\text{(b)}\: df(SSR) = 1\)</span></p>
<span class="math inline">\(\text{(c)}\: df(SSE) = n - 2\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> (a)</p>
<p>Since <span class="math inline">\(\sum (Y_i - \overline{Y}) = 0\)</span>, we have <span class="math inline">\(1\)</span> linear constraints. Thus,</p>
<p><span class="math display">\[df(SST) = n - 1\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p>Note that <span class="math inline">\(\hat{Y_i} - \overline{Y} = \hat\beta_1(x_i - \overline{x})\)</span></p>
<p>where <span class="math inline">\(\sum (x_i - \overline{x}) = 0\)</span>.</p>
<p>Thus,</p>
<p><span class="math display">\[df(SSR) = n - (n - 1) = 1\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p>From Example <a href="simple.html#exm:usingnormal">1.1</a>, <span class="math inline">\(\sum (Y_i - \hat{Y_i}) = 0\)</span> and <span class="math inline">\(\sum x_i (Y_i - \hat{Y_i}) = 0\)</span>.</p>
<p>Thus,</p>
<span class="math display">\[df(SSE) = n - 2\]</span>
</div>

<p>Dividing sum of squares in <span class="math inline">\(df\)</span>, we can standardize it.</p>

<div class="definition">
<p><span id="def:ms" class="definition"><strong>Definition 1.12  (Mean square)  </strong></span>Mean square is a sum of square <span class="math inline">\(SS\)</span> divided by its degree of freedom <span class="math inline">\(df\)</span></p>
<span class="math display">\[MS := \frac{SS}{df}\]</span>
</div>

<p>Using the values of corollary <a href="simple.html#cor:dfss">1.6</a> we can define each mean square for <span class="math inline">\(SSR\)</span> and <span class="math inline">\(SSE\)</span>.</p>

<div class="definition">
<span id="def:msr" class="definition"><strong>Definition 1.13  (Regression mean square)  </strong></span><span class="math display">\[MSR := \frac{SSR}{1} = SSR\]</span>
</div>

<p>From Proposition <a href="simple.html#prp:ssrdist">1.22</a>, the following corollary can be drawn.</p>

<div class="corollary">
<p><span id="cor:msrdist" class="corollary"><strong>Corollary 1.7  (Distribution of MSR)  </strong></span>Under <span class="math inline">\(H_0: \beta_1 = 0\)</span>,</p>
<span class="math display">\[\frac{SSR}{\sigma^2} \stackrel{H_0}{\sim} \chi^2(1)\]</span>
</div>

<p>Now standardize residual sum of square.</p>

<div class="definition">
<span id="def:mse" class="definition"><strong>Definition 1.14  (Residual mean square)  </strong></span><span class="math display">\[MSE := \frac{SSE}{n - 2}\]</span>
</div>

<p>From Proposition <a href="simple.html#prp:ssrdist">1.22</a>, we can construct same statistic. In fact, <span class="math inline">\(\frac{SSE}{\sigma^2}\)</span> follows <span class="math inline">\(\chi^2(n - 2)\)</span> whether or not <span class="math inline">\(\beta_1\)</span> is zero. Its <span class="math inline">\(\delta = 0\)</span>.</p>

<div class="corollary">
<span id="cor:msedist" class="corollary"><strong>Corollary 1.8  (Distribution of MSE)  </strong></span><span class="math display">\[\frac{SSE}{\sigma^2} \sim \chi^2(n - 2)\]</span>
</div>

<p>Finally, we can now use Proposition <a href="simple.html#prp:fdist">1.16</a> so that</p>
<p><span class="math display">\[
F \equiv \frac{MSR}{MSE} = \frac{\frac{SSE / \sigma^2 \sim \chi^2(1)}{1}}{\frac{SSR / \sigma^2 \stackrel{H_0}{\sim}\chi^2(n - 2)}{n - 2}} \stackrel{H_0}{\sim}F(1, n - 2)
\]</span></p>
<p>By construction, this test statistic is used for</p>
<p><span class="math display">\[H_0: \beta_1 = 0\]</span></p>
<p>which means that the predictor does not explain the response anything. In other words, we are testing that</p>
<p><span class="math display" id="eq:goodfit">\[\begin{equation}
  H_0: \text{Model is not useful at all} \qquad \text{vs} \qquad H_1: \text{Model can explain data}
  \tag{1.21}
\end{equation}\]</span></p>

<div class="remark">
<p> <span class="remark"><em>Remark</em> (F statistic on testing significance). </span> Null hypothesis <a href="simple.html#eq:goodfit">(1.21)</a> can be tested with <span class="math inline">\(F\)</span>-statistic.</p>
<span class="math display">\[F_0 = \frac{MSR}{MSE} = \frac{SSR / df(SSR)}{SSE / df(SSE)} \stackrel{H_0}{\sim}F(df(SSR), df(SSE))\]</span>
</div>

<p>Then we reject <span class="math inline">\(H_0\)</span> if</p>
<p><span class="math display">\[F_0 &gt; F_\alpha\bigg( df(SSR), df(SSE) \bigg)\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:goodfitfig"></span>
<img src="regression-analysis_files/figure-html/goodfitfig-1.png" alt="Rejection region for significance testing" width="70%" />
<p class="caption">
Figure 1.10: Rejection region for significance testing
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(delv_fit)</code></pre>
<pre><code>
Call:
lm(formula = y ~ x, data = delv)

Residuals:
   Min     1Q Median     3Q    Max 
-7.581 -1.874 -0.349  2.181 10.634 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    3.321      1.371    2.42    0.024 *  
x              2.176      0.124   17.55  8.2e-15 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 4.18 on 23 degrees of freedom
Multiple R-squared:  0.93,  Adjusted R-squared:  0.927 
F-statistic:  308 on 1 and 23 DF,  p-value: 8.22e-15</code></pre>
<p>This statistic is <code>F-statistic</code> included in <code>summary.lm()</code> output. This is saved as <code>$fstatistic</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(delv_fit)<span class="op">$</span>fstatistic</code></pre>
<pre><code>value numdf dendf 
  308     1    23 </code></pre>
<p>We usually summarize these statistic in table form, so called <em>ANOVA table</em>.</p>
<table>
<thead>
<tr class="header">
<th align="center">Source</th>
<th align="center">SS</th>
<th align="center">df</th>
<th align="center">MS</th>
<th align="center">F</th>
<th align="center">p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Model</td>
<td align="center"><span class="math inline">\(SSR\)</span></td>
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(MSR\)</span></td>
<td align="center"><span class="math inline">\(F_0\)</span></td>
<td align="center">p-value</td>
</tr>
<tr class="even">
<td align="center">Error</td>
<td align="center"><span class="math inline">\(SSE\)</span></td>
<td align="center"><span class="math inline">\(n - 2\)</span></td>
<td align="center"><span class="math inline">\(MSE\)</span></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">Total</td>
<td align="center"><span class="math inline">\(SST\)</span></td>
<td align="center"><span class="math inline">\(n - 1\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<p>To get this table, just use <code>anova()</code> for <code>lm</code> object.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(delv_fit)</code></pre>
<pre><code>Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value  Pr(&gt;F)    
x          1   5382    5382     308 8.2e-15 ***
Residuals 23    402      17                    
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Since the last <code>Total</code> row is just sum of the model and error, the function does not give it. To use this table as <code>data.frame</code> more easily, just implement <code>broom::tidy</code> as before.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(delv_fit) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span>broom<span class="op">::</span><span class="kw">tidy</span>()</code></pre>
<pre><code># A tibble: 2 x 6
  term         df sumsq meansq statistic   p.value
  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
1 x             1 5382. 5382.       308.  8.22e-15
2 Residuals    23  402.   17.5       NA  NA       </code></pre>
<p>Denote that here <em>simple linear regression setting</em> <span class="math inline">\(F\)</span>-statistic and <span class="math inline">\(t\)</span>-statistic of Equation <a href="simple.html#eq:b1test">(1.20)</a> perform exactly same thing, <span class="math inline">\(H_0 : \beta_1 = 0\)</span>. In fact, we know that</p>
<p><span class="math display">\[F(1, k) \stackrel{d}{=} T_k^2\]</span></p>

<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> In the simple linear regression setting, <span class="math inline">\(F\)</span>-test for significance and <span class="math inline">\(t\)</span>-test for no slope are equivalent, i.e. under <span class="math inline">\(H_0 : \beta_1 = 0\)</span></p>
<span class="math display">\[F_0 = \frac{\hat\beta_1 S_{xx}}{\sigma^2} = \bigg( \frac{\hat\beta_1}{\sigma / \sqrt{S_xx}} \bigg) = T_0^2\]</span>
</div>


</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Hogg:2018aa">
<p>Hogg, Robert V., Joseph W. McKean, and Allen Thornton Craig. 2018. <em>Introduction to Mathematical Statistics</em>. 8th ed. Pearson College Division.</p>
</div>
<div id="ref-Leon:2014aa">
<p>Leon, Steve. 2014. <em>Linear Algebra with Applications</em>. Pearson Higher Ed.</p>
</div>
<div id="ref-Johnson:2013aa">
<p>Johnson, Richard Arnold, and Dean W. Wichern. 2013. <em>Applied Multivariate Statistical Analysis</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["regression-analysis.pdf", "regression-analysis.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
