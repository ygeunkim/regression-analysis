<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 2 Simple Linear Regression | R Lab for Regression Analysis</title>
  <meta name="description" content="This aims at covering materials of regression analysis with R programming.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 2 Simple Linear Regression | R Lab for Regression Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This aims at covering materials of regression analysis with R programming." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Simple Linear Regression | R Lab for Regression Analysis" />
  
  <meta name="twitter:description" content="This aims at covering materials of regression analysis with R programming." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Lab for Regression Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Linear Regression Analysis</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#relation"><i class="fa fa-check"></i><b>1.1</b> Relation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="simple.html"><a href="simple.html#model"><i class="fa fa-check"></i><b>2.1</b> Model</a></li>
<li class="chapter" data-level="2.2" data-path="simple.html"><a href="simple.html#least-squares-estimation"><i class="fa fa-check"></i><b>2.2</b> Least Squares Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="simple.html"><a href="simple.html#normal-equations"><i class="fa fa-check"></i><b>2.2.1</b> Normal equations</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple.html"><a href="simple.html#prediction-and-mean-response"><i class="fa fa-check"></i><b>2.2.2</b> Prediction and Mean response</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple.html"><a href="simple.html#lseprop"><i class="fa fa-check"></i><b>2.2.3</b> Properties of LSE</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple.html"><a href="simple.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>2.2.4</b> Gauss-Markov Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple.html"><a href="simple.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.3</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="2.3.1" data-path="simple.html"><a href="simple.html#likelihood-equations"><i class="fa fa-check"></i><b>2.3.1</b> Likelihood equations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="simple.html"><a href="simple.html#residuals"><i class="fa fa-check"></i><b>2.4</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/ygeunkim/regression-analysis" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R Lab for Regression Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Simple Linear Regression</h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<div id="model" class="section level2">
<h2><span class="header-section-number">2.1</span> Model</h2>
<pre class="sourceCode r"><code class="sourceCode r">delv &lt;-<span class="st"> </span>MPV<span class="op">::</span>p2<span class="fl">.9</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tbl_df</span>()</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Number of Cases&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Delivery Time&quot;</span>
  )</code></pre>
<div class="figure" style="text-align: center"><span id="fig:delivery"></span>
<img src="regression-analysis_files/figure-html/delivery-1.png" alt="The Delivery Time Data\label{fig:delivery}" width="70%" />
<p class="caption">
Figure 2.1: The Delivery Time Data
</p>
</div>
<p>Given data <span class="math inline">\((x_1, Y_1), \ldots, (x_n, Y_n)\)</span>, we try to fit linear model</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]</span></p>
<p>Here <span class="math inline">\(\epsilon_i\)</span> is a error term, which is a random variable.</p>
<p><span class="math display">\[\epsilon \stackrel{iid}{\sim} (0, \sigma^2)\]</span></p>
<p>It gives the problem of estimating three parameters <span class="math inline">\((\beta_0, \beta_1, \sigma^2)\)</span>. Before estimating these, we set some assumptions.</p>
<ol style="list-style-type: decimal">
<li>linear relationship</li>
<li><span class="math inline">\(\epsilon_i\)</span>s are independent</li>
<li><span class="math inline">\(\epsilon_i\)</span>s are identically destributed, i.e.Â <em>constant variance</em></li>
<li>In some setting, <span class="math inline">\(\epsilon_i \sim N\)</span></li>
</ol>
</div>
<div id="least-squares-estimation" class="section level2">
<h2><span class="header-section-number">2.2</span> Least Squares Estimation</h2>
<div class="figure" style="text-align: center"><span id="fig:lsefig"></span>
<img src="regression-analysis_files/figure-html/lsefig-1.png" alt="Idea of the least square estimation\label{fig:lsefig}" width="70%" />
<p class="caption">
Figure 2.2: Idea of the least square estimation
</p>
</div>
<p>We try to find <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the sum of squares of the vertical distances, i.e.</p>
<p><span class="math display">\[\begin{equation} \label{eq:ssq}
  (\beta_0, \beta_1) = \arg\min \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2
\end{equation}\]</span></p>
<div id="normal-equations" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Normal equations</h3>
<p>Denote that Equation <span class="math inline">\(\eqref{eq:ssq}\)</span> is quadratic. Then we can find its minimum by find the zero point of the first derivative. Set</p>
<p><span class="math display">\[Q(\beta_0, \beta_1) := \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2\]</span></p>
<p>Then we have</p>
<p><span class="math display">\[\begin{equation} \label{eq:normbeta0}
  \frac{\partial Q}{\partial \beta_0} = -2 \sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i) = 0
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation} \label{eq:normbeta1}
  \frac{\partial Q}{\partial \beta_1} = -2 \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)x_i = 0
\end{equation}\]</span></p>
<p>From <span class="math inline">\(\eqref{eq:normbeta0}\)</span>,</p>
<p><span class="math display">\[\sum_{i = 1}^n Y_i - n \hat\beta_0 - \hat\beta_1 \sum_{i = 1}^n x_i = 0\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x}\]</span></p>
<p><span class="math inline">\(\eqref{eq:normbeta1}\)</span> gives</p>
<p><span class="math display">\[\sum_{i = 1}^n x_i (Y_i - \overline{Y} + \hat\beta_1\overline{x} - \hat\beta_1 x_i) = \sum_{i = 1}^n x_i(Y_i - \overline{Y}) - \hat\beta_1\sum_{i = 1}^n x_i (x_i - \overline{x}) = 0\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\hat\beta_1 = \frac{\sum\limits_{i = 1}^nx_i(Y_i - \overline{Y})}{\sum\limits_{i = 1}^n x_i (x_i - \overline{x})}\]</span></p>

<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> <span class="math display">\[\hat\beta_1 = \frac{S_{XY}}{S_{XX}}\]</span></p>
where <span class="math inline">\(S_{XX} := \sum\limits_{i = 1}^n (x_i - \overline{x})^2\)</span> and <span class="math inline">\(S_{XY} := \sum\limits_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y})\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that <span class="math inline">\(\overline{x}^2 = \frac{1}{n^2}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2\)</span>. Then we have</p>
<p><span class="math display">\[\begin{equation} \label{eq:sxx}
  \begin{split}
    S_{XX} &amp; = \sum_{i = 1}^n (x_i - \overline{x})^2 \\
    &amp; = \sum_{i = 1}^n x_i^2 - 2\sum_{i = 1}^n x_i \overline{x} + \sum_{i = 1}^n\overline{x}^2 \\
    &amp; = \sum_{i = 1}^n x_i^2 - \frac{2}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 + \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 \\
    &amp; = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2
  \end{split}
\end{equation}\]</span></p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \hat\beta_1 &amp; = \frac{\sum x_i(Y_i - \overline{Y})}{\sum x_i (x_i - \overline{x})} \\
    &amp; = \frac{\sum x_i (Y_i - \overline{Y}) - \overline{x}\sum (Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \qquad \because \sum (Y_i - \overline{Y}) = 0 \\
    &amp; = \frac{\sum (x_i - \overline{x})(Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \\
    &amp; = \frac{S_{XY}}{S_{XX}}
  \end{split}
\end{equation*}\]</span>
</div>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> delv)</code></pre>
<pre><code>
Call:
lm(formula = y ~ x, data = delv)

Coefficients:
(Intercept)            x  
       3.32         2.18  </code></pre>
</div>
<div id="prediction-and-mean-response" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Prediction and Mean response</h3>
<blockquote>
<p>âEssentially, all models are wrong, but some are useful.â</p>
<p>âGeorge Box</p>
</blockquote>
<p>Recall that we have assumed the <strong>linear assumption</strong> between the predictor and the response variables, i.e.Â the true model. Estimating <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is same as estimating the <em>assumed true model</em>.</p>

<div class="definition">
<span id="def:eyx" class="definition"><strong>Definition 2.1  (Mean response)  </strong></span><span class="math display">\[E(Y \mid X = x) = \beta_0 + \beta_1 x\]</span>
</div>

<p>We can estimate this mean resonse by</p>
<p><span class="math display">\[\begin{equation} \label{eq:meanres}
  \widehat{E(Y \mid x)} = \hat\beta_0 + \hat\beta_1 x
\end{equation}\]</span></p>
<p>However, in practice, the model might not be true, which is included in <span class="math inline">\(\epsilon\)</span> term.</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]</span></p>
<p>Our real problem is predicting individual <span class="math inline">\(Y\)</span>, not the mean. The <em>prediction</em> of response can be done by</p>
<p><span class="math display">\[\begin{equation} \label{eq:indpred}
  \hat{Y_i}  = \hat\beta_0 + \hat\beta_1 x_i
\end{equation}\]</span></p>
<p>Observe that the values of Equation <span class="math inline">\(\eqref{eq:meanres}\)</span> and <span class="math inline">\(\eqref{eq:indpred}\)</span> are same. However, due to the <strong>error term in the prediction</strong>, it has larger standard error.</p>
</div>
<div id="lseprop" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Properties of LSE</h3>
<p>Parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> have some properties related to the expectation and variance. We can notice that these lseâs are <strong>unbiased linear estimator</strong>. In fact, these are the <em>best unbiased linear estimator</em>. This will be covered in the Gauss-Markov theorem.</p>

<div class="lemma">
<p><span id="lem:sxy" class="lemma"><strong>Lemma 2.1  </strong></span><span class="math display">\[S_{XX} = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 = \sum_{i = 1}^n x_i(x_i - \overline{x})\]</span></p>
<span class="math display">\[S_{XY} = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg) = \sum_{i = 1}^n Y_i(x_i - \overline{x})\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We already proven the first part of <span class="math inline">\(S_{XX}\)</span>. See the Equation <span class="math inline">\(\eqref{eq:sxx}\)</span>. The second part is tivial. Since <span class="math inline">\(\sum (x_i - \overline{x}) = 0\)</span>,</p>
<p><span class="math display">\[S_{XX} = \sum_{i = 1}^n (x_i - \overline{x})^2 = \sum_{i = 1}^n (x_i - \overline{x})x_i\]</span></p>
<p>For the first part of <span class="math inline">\(S_{XY}\)</span>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    S_{XY} &amp; = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    &amp; = \sum_{i = 1}^n x_i Y_i - \overline{x} \sum_{i = 1}^n Y_i - \overline{Y} \sum_{i = 1}^n x_i + n \overline{x} \overline{Y} \\
    &amp; = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg)
  \end{split}
\end{equation*}\]</span></p>
<p>Second part of <span class="math inline">\(S_{XY}\)</span> also can be proven from the definition.</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    S_{XY} &amp; = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    &amp; = \sum_{i = 1}^n Y_i (x_i - \overline{x}) - \overline{Y} \sum_{i = 1}^n (x_i - \overline{x}) \\
    &amp; = \sum_{i = 1}^n Y_i (x_i - \overline{x}) \qquad \because \sum_{i = 1}^n (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}\]</span>
</div>


<div class="lemma">
<p><span id="lem:linbet" class="lemma"><strong>Lemma 2.2  (Linearity)  </strong></span>Each coefficient is a linear estimator.</p>
<p><span class="math display">\[\hat\beta_1 = \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i\]</span></p>
<span class="math display">\[\hat\beta_0 = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})}{S_{XX}} \bigg) Y_i\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From lemma <a href="simple.html#lem:sxy">2.1</a>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    \hat\beta_1 &amp; = \frac{S_{XY}}{S_{XX}} \\
    &amp; = \frac{1}{S_{XX}}\sum_{i = 1}^n (x_i - \overline{x}) Y_i
  \end{split}
\end{equation*}\]</span></p>
<p>It gives that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \hat\beta_0 &amp; = \overline{Y} - \hat\beta_1 \overline{x} \\
    &amp; = \frac{1}{n}\sum_{i = 1}^n Y_i - \overline{x} \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i \\
    &amp; = \sum_{i = 1}^n\bigg(\frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)Y_i
  \end{split}
\end{equation*}\]</span>
</div>


<div class="proposition">
<p><span id="prp:ue" class="proposition"><strong>Proposition 2.1  (Unbiasedness)  </strong></span>Both coefficients are unbiased.</p>
<p><span class="math inline">\(\text{(a)}\: E\hat\beta_1 = \beta_1\)</span></p>
<span class="math inline">\(\text{(b)}\: E\hat\beta_0 = \beta_0\)</span>
</div>

<p>From the model, <span class="math inline">\(Y_1, \ldots, Y_n \stackrel{indep}{\sim} (\beta_0 + \beta_1 x_i, \sigma^2)\)</span>.</p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From lemma <a href="simple.html#lem:sxy">2.1</a>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    E\hat\beta_1 &amp; = \sum_{i = 1}^n \bigg[ \frac{(x_i - \overline{x})}{S_{XX}} E(Y_i) \bigg] \\
    &amp; = \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}(\beta_0 + \beta_1 x_i) \\
    &amp; = \frac{\beta_1 \sum (x_i - \overline{x})x_i}{\sum (x_i - \overline{x})x_i} \qquad \because \sum (x_i - \overline{x}) = 0 \\
    &amp; = \beta_1
  \end{split}
\end{equation*}\]</span></p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    E\hat\beta_0 &amp; = E(\overline{Y} - \hat\beta_1 \overline{x}) \\
    &amp; = E(\overline{Y}) - \overline{x}E(\hat\beta_1) \\
    &amp; = E(\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) - \beta_1 \overline{x} \\
    &amp; = \beta_0 + \beta_1 \overline{x} - \beta_1 \overline{x} \\
    &amp; = \beta_0
  \end{split}
\end{equation*}\]</span>
</div>


<div class="proposition">
<p><span id="prp:vb" class="proposition"><strong>Proposition 2.2  (Variances)  </strong></span>Variances and covariance of coefficients</p>
<p><span class="math inline">\(\text{(a)}\: Var\hat\beta_1 = \frac{\sigma^2}{S_{XX}}\)</span></p>
<p><span class="math inline">\(\text{(b)}\: Var\hat\beta_0 = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2\)</span></p>
<span class="math inline">\(\text{(c)}\: Cov(\hat\beta_0, \hat\beta_1) = - \frac{\overline{x}}{S_{XX}} \sigma^2\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Proving is just arithmetic.</p>
<ol style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Var\hat\beta_1 &amp; = \frac{1}{S_{XX}^2}\sum_{i = 1}^n \bigg[ (x_i - \overline{x})^2 Var(Y_i) \bigg] + \frac{1}{S_{XX}^2} \sum_{j \neq k}^n \bigg[ (x_j - \overline{x})(x_k - \overline{x}) Cov(Y_j, Y_k) \bigg] \\
    &amp; = \frac{\sigma^2}{S_{XX}} \qquad \because Cov(Y_j, Y_k) = 0 \: \text{if} \: j \neq k
  \end{split}
\end{equation*}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Var\hat\beta_0 &amp; = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)^2Var(Y_i) + \sum_{j \neq k} \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\bigg( \frac{1}{n} - \frac{(x_k - \overline{x})\overline{x}}{S_{XX}} \bigg) Cov(Y_j, Y_k) \\
    &amp; = \frac{\sigma^2}{n} - 2 \sigma^2 \frac{\overline{x}}{S_{XX}} \sum_{i = 1}^n (x_i - \overline{x}) + \frac{\sigma^2 \overline{x}^2 \sum (x_i - \overline{x})^2}{S_{XX}^2} \\
    &amp; = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \sigma^2 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li></li>
</ol>
<span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, \hat\beta_1) &amp; = Cov(\overline{Y} - \hat\beta_1 \overline{x}, \hat\beta_1) \\
    &amp; = - \overline{x} Var\hat\beta_1 \\
    &amp; = - \frac{\overline{x}}{S_{XX}} \sigma^2
  \end{split}
\end{equation*}\]</span>
</div>

</div>
<div id="gauss-markov-theorem" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Gauss-Markov Theorem</h3>
<p>Chapter <a href="simple.html#lseprop">2.2.3</a> shows that the <span class="math inline">\(\beta_0^{LSE}\)</span> and <span class="math inline">\(\beta_1^{LSE}\)</span> are the <strong>linear unbiased estimators</strong>. Are these good? Good compared to <em>what estimators</em>? Here we consider <em>linear unbiased estimator</em>. If variances in the proposition <a href="simple.html#prp:vb">2.2</a> are lower than any parameters in this parameter family, <span class="math inline">\(\beta_0^{LSE}\)</span> and <span class="math inline">\(\beta_1^{LSE}\)</span> are the <strong>best linear unbiased estimators</strong>.</p>

<div class="theorem">
<p><span id="thm:gmt" class="theorem"><strong>Theorem 2.1  (Gauss Markov Theorem)  </strong></span><span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are BLUE, i.e.Â the best linear unbiased estimator.</p>
<p><span class="math display">\[Var(\hat\beta_0) \le Var\Big( \sum_{i = 1}^n a_i Y_i \Big) \: \forall a_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n a_i Y_i \Big) = \beta_0\]</span></p>
<span class="math display">\[Var(\hat\beta_1) \le Var\Big( \sum_{i = 1}^n b_i Y_i \Big) \: \forall b_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n b_i Y_i \Big) = \beta_1\]</span>
</div>

</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2">
<h2><span class="header-section-number">2.3</span> Maximum Likelihood Estimation</h2>
<p>In this section, we add an assumption to an random errors <span class="math inline">\(\epsilon_i\)</span>.</p>
<p><span class="math display">\[\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\]</span></p>

<div class="example">
<p><span id="exm:gmle" class="example"><strong>Example 2.1  (Gaussian Likelihood)  </strong></span>Note that <span class="math inline">\(Y_i \stackrel{indep}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)\)</span>. Then the likelihood function is</p>
<p><span class="math display">\[L(\beta_0, \beta_1, \sigma^2) = \prod_{i = 1}^n\bigg( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \bigg(- \frac{(Y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2} \bigg) \bigg)\]</span></p>
<p>and so the log-likelihood function can be computed as</p>
<span class="math display">\[l(\beta_0, \beta_1, \sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2\]</span>
</div>

<div id="likelihood-equations" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Likelihood equations</h3>

<div class="definition">
<span id="def:mledef" class="definition"><strong>Definition 2.2  (Maximum Likelihood Estimator)  </strong></span><span class="math display">\[(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) := \arg\sup L(\beta_0, \beta_1, \sigma^2)\]</span>
</div>

<p>Since <span class="math inline">\(l(\cdot) = \ln L(\cdot)\)</span> is monotone,</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> <span class="math display">\[(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) = \arg\sup l(\beta_0, \beta_1, \sigma^2)\]</span>
</div>

<p>We can find the maximum of this <em>quadratic</em> function by making first derivative.</p>
<p><span class="math display">\[\begin{equation} \label{eq:mlbeta0}
  \frac{\partial l}{\partial \beta_0} = \frac{1}{\sigma^2} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i) = 0
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation} \label{eq:mlbeta1}
  \frac{\partial l}{\partial \beta_1} = \frac{1}{\sigma^2} \sum_{i = 1}^n x_i (Y_i - \beta_0 - \beta_1 x_i) = 0
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation} \label{eq:mlsig}
  \frac{\partial l}{\partial \sigma^2} = - \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = 0
\end{equation}\]</span></p>
<p>Denote that Equations <span class="math inline">\(\eqref{eq:mlbeta0}\)</span> and <span class="math inline">\(\eqref{eq:mlbeta1}\)</span> given <span class="math inline">\(\hat\sigma^2\)</span> are equivalent to the normal equations. Thus,</p>
<p><span class="math display">\[\hat\beta_0^{MLE} = \hat\beta_0^{LSE}, \quad \hat\beta_1^{MLE} = \hat\beta_1^{LSE}\]</span></p>
<p>From <span class="math inline">\(\eqref{eq:mlsig}\)</span>,</p>
<p><span class="math display">\[\hat\sigma^{2MLE} = \frac{1}{n}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2 = \frac{n - 2}{n} \hat\sigma^{2LSE}\]</span></p>
<p>Recall that <span class="math inline">\(\hat\sigma^{2LSE}\)</span> is an unbiased, i.e.Â this <em>MLE is not an unbiased estimator</em>. Since <span class="math inline">\(\hat\sigma^{2MLE} \approx \hat\sigma^{2LSE}\)</span> for large <span class="math inline">\(n\)</span>, howerver, it is <em>asymptotically unbiased</em>.</p>

<div class="theorem">
<p><span id="thm:rclb" class="theorem"><strong>Theorem 2.2  (Rao-Cramer Lower Bound, univariate case)  </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta)\)</span>. If <span class="math inline">\(\hat\theta\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[Var(\hat\theta) \ge \frac{1}{I_n(\theta)}\]</span></p>
where <span class="math inline">\(I_n(\theta) = -E\bigg(\frac{\partial^2 l(\theta)}{\partial \theta^2} \bigg)\)</span>
</div>

<p>To apply this theorem @(thm:rclb) in the simple linear regression setting, i.e.Â <span class="math inline">\((\beta_0, \beta_1)\)</span>, we need to look at the <em>bivariate case</em>.</p>

<div class="theorem">
<p><span id="thm:rclb2" class="theorem"><strong>Theorem 2.3  (Rao-Cramer Lower Bound, bivariate case)  </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta1, \theta_2)\)</span> and let <span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \theta_2)^T\)</span>. If each <span class="math inline">\(\hat\theta_1\)</span>, <span class="math inline">\(\hat\theta_2\)</span> is an unbiased estimator of <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, then</p>
<p><span class="math display">\[
Var(\boldsymbol{\theta}) := \begin{bmatrix}
Var(\hat\theta_1) &amp; Cov(\hat\theta_1, \hat\theta_2) \\
Cov(\hat\theta_1, \hat\theta_2) &amp; Var(\hat\theta_2)
\end{bmatrix} \ge I_n^{-1}(\theta_1, \theta_2)
\]</span></p>
<p>where</p>
<span class="math display">\[
I_n(\theta_1, \theta_2) = - \begin{bmatrix}
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1^2} \bigg) &amp; E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) \\
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) &amp; E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_2^2} \bigg)
\end{bmatrix}
\]</span>
</div>

</div>
</div>
<div id="residuals" class="section level2">
<h2><span class="header-section-number">2.4</span> Residuals</h2>

<div class="definition">
<span id="def:res" class="definition"><strong>Definition 2.3  (Residuals)  </strong></span><span class="math display">\[e_i := Y_i - \hat{Y_i}\]</span>
</div>

<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">yhat =</span> <span class="kw">predict</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x))) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_linerange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> y, <span class="dt">ymax =</span> yhat), <span class="dt">col =</span> <span class="kw">I</span>(<span class="st">&quot;red&quot;</span>), <span class="dt">alpha =</span> <span class="fl">.7</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Number of Cases&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Delivery Time&quot;</span>
  )</code></pre>
<div class="figure" style="text-align: center"><span id="fig:regplot"></span>
<img src="regression-analysis_files/figure-html/regplot-1.png" alt="Fit and residuals\label{fig:regplot}" width="70%" />
<p class="caption">
Figure 2.3: Fit and residuals
</p>
</div>
<p>See <span class="math inline">\(\text{Figure }\ref{fig:regplot}\)</span>. Each red line is <span class="math inline">\(e_i\)</span>. As we can see, <span class="math inline">\(e_i\)</span> represents the difference between <em>observed</em> response and <em>predicted</em> response. A large <span class="math inline">\(\lvert e_i \rvert\)</span> indicates a large prediction error. You can call this <span class="math inline">\(e_i\)</span> for each <span class="math inline">\(Y_i\)</span> by <code>lm()$residuals</code> or <code>residuals()</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">delv_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> delv)
delv_fit<span class="op">$</span>residuals</code></pre>
<pre><code>     1      2      3      4      5      6      7      8      9     10 
-1.874  1.651  2.181  2.855 -2.628 -0.444  0.327 -0.724 10.634  7.298 
    11     12     13     14     15     16     17     18     19     20 
 2.191 -4.082  1.475  3.372  1.094  3.918 -1.028  0.446 -0.349 -5.216 
    21     22     23     24     25 
-7.182 -7.581 -4.156 -0.900 -1.275 </code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["regression-analysis.pdf", "regression-analysis.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
