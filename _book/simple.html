<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 2 Simple Linear Regression | R Lab for Regression Analysis</title>
  <meta name="description" content="This aims at covering materials of regression analysis with R programming.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 2 Simple Linear Regression | R Lab for Regression Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="cover.png" />
  <meta property="og:description" content="This aims at covering materials of regression analysis with R programming." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Simple Linear Regression | R Lab for Regression Analysis" />
  
  <meta name="twitter:description" content="This aims at covering materials of regression analysis with R programming." />
  <meta name="twitter:image" content="cover.png" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="linear-regression-analysis.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Lab for Regression Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="linear-regression-analysis.html"><a href="linear-regression-analysis.html"><i class="fa fa-check"></i><b>1</b> Linear Regression Analysis</a><ul>
<li class="chapter" data-level="1.1" data-path="linear-regression-analysis.html"><a href="linear-regression-analysis.html#relation"><i class="fa fa-check"></i><b>1.1</b> Relation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="simple.html"><a href="simple.html#model"><i class="fa fa-check"></i><b>2.1</b> Model</a></li>
<li class="chapter" data-level="2.2" data-path="simple.html"><a href="simple.html#least-squares-estimation"><i class="fa fa-check"></i><b>2.2</b> Least Squares Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="simple.html"><a href="simple.html#normal-equations"><i class="fa fa-check"></i><b>2.2.1</b> Normal equations</a></li>
<li class="chapter" data-level="2.2.2" data-path="simple.html"><a href="simple.html#prediction-and-mean-response"><i class="fa fa-check"></i><b>2.2.2</b> Prediction and Mean response</a></li>
<li class="chapter" data-level="2.2.3" data-path="simple.html"><a href="simple.html#lseprop"><i class="fa fa-check"></i><b>2.2.3</b> Properties of LSE</a></li>
<li class="chapter" data-level="2.2.4" data-path="simple.html"><a href="simple.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>2.2.4</b> Gauss-Markov Theorem</a></li>
<li class="chapter" data-level="2.2.5" data-path="simple.html"><a href="simple.html#estimation-of-sigma2"><i class="fa fa-check"></i><b>2.2.5</b> Estimation of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="simple.html"><a href="simple.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.3</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="2.3.1" data-path="simple.html"><a href="simple.html#likelihood-equations"><i class="fa fa-check"></i><b>2.3.1</b> Likelihood equations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="simple.html"><a href="simple.html#residuals"><i class="fa fa-check"></i><b>2.4</b> Residuals</a><ul>
<li class="chapter" data-level="2.4.1" data-path="simple.html"><a href="simple.html#prediction-error"><i class="fa fa-check"></i><b>2.4.1</b> Prediction error</a></li>
<li class="chapter" data-level="2.4.2" data-path="simple.html"><a href="simple.html#residuals-and-the-variance"><i class="fa fa-check"></i><b>2.4.2</b> Residuals and the variance</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="simple.html"><a href="simple.html#decomposition-of-total-variability"><i class="fa fa-check"></i><b>2.5</b> Decomposition of Total Variability</a></li>
<li class="chapter" data-level="2.6" data-path="simple.html"><a href="simple.html#geometric-interpretations"><i class="fa fa-check"></i><b>2.6</b> Geometric Interpretations</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/ygeunkim/regression-analysis" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R Lab for Regression Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Simple Linear Regression</h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<div id="model" class="section level2">
<h2><span class="header-section-number">2.1</span> Model</h2>
<pre class="sourceCode r"><code class="sourceCode r">delv &lt;-<span class="st"> </span>MPV<span class="op">::</span>p2<span class="fl">.9</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tbl_df</span>()</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Number of Cases&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Delivery Time&quot;</span>
  )</code></pre>
<div class="figure" style="text-align: center"><span id="fig:delivery"></span>
<img src="regression-analysis_files/figure-html/delivery-1.png" alt="The Delivery Time Data\label{fig:delivery}" width="70%" />
<p class="caption">
Figure 2.1: The Delivery Time Data
</p>
</div>
<p>Given data <span class="math inline">\((x_1, Y_1), \ldots, (x_n, Y_n)\)</span>, we try to fit linear model</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]</span></p>
<p>Here <span class="math inline">\(\epsilon_i\)</span> is a error term, which is a random variable.</p>
<p><span class="math display">\[\epsilon \stackrel{iid}{\sim} (0, \sigma^2)\]</span></p>
<p>It gives the problem of estimating three parameters <span class="math inline">\((\beta_0, \beta_1, \sigma^2)\)</span>. Before estimating these, we set some assumptions.</p>
<ol style="list-style-type: decimal">
<li>linear relationship</li>
<li><span class="math inline">\(\epsilon_i\)</span>s are independent</li>
<li><span class="math inline">\(\epsilon_i\)</span>s are identically destributed, i.e. <em>constant variance</em></li>
<li>In some setting, <span class="math inline">\(\epsilon_i \sim N\)</span></li>
</ol>
</div>
<div id="least-squares-estimation" class="section level2">
<h2><span class="header-section-number">2.2</span> Least Squares Estimation</h2>
<div class="figure" style="text-align: center"><span id="fig:lsefig"></span>
<img src="regression-analysis_files/figure-html/lsefig-1.png" alt="Idea of the least square estimation\label{fig:lsefig}" width="70%" />
<p class="caption">
Figure 2.2: Idea of the least square estimation
</p>
</div>
<p>We try to find <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the sum of squares of the vertical distances, i.e.</p>
<p><span class="math display">\[\begin{equation} \label{eq:ssq}
  (\beta_0, \beta_1) = \arg\min \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2
\end{equation}\]</span></p>
<div id="normal-equations" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Normal equations</h3>
<p>Denote that Equation <span class="math inline">\(\eqref{eq:ssq}\)</span> is quadratic. Then we can find its minimum by find the zero point of the first derivative. Set</p>
<p><span class="math display">\[Q(\beta_0, \beta_1) := \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2\]</span></p>
<p>Then we have</p>
<p><span class="math display">\[\begin{equation} \label{eq:normbeta0}
  \frac{\partial Q}{\partial \beta_0} = -2 \sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i) = 0
\end{equation}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{equation} \label{eq:normbeta1}
  \frac{\partial Q}{\partial \beta_1} = -2 \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)x_i = 0
\end{equation}\]</span></p>
<p>From <span class="math inline">\(\eqref{eq:normbeta0}\)</span>,</p>
<p><span class="math display">\[\sum_{i = 1}^n Y_i - n \hat\beta_0 - \hat\beta_1 \sum_{i = 1}^n x_i = 0\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x}\]</span></p>
<p><span class="math inline">\(\eqref{eq:normbeta1}\)</span> gives</p>
<p><span class="math display">\[\sum_{i = 1}^n x_i (Y_i - \overline{Y} + \hat\beta_1\overline{x} - \hat\beta_1 x_i) = \sum_{i = 1}^n x_i(Y_i - \overline{Y}) - \hat\beta_1\sum_{i = 1}^n x_i (x_i - \overline{x}) = 0\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[\hat\beta_1 = \frac{\sum\limits_{i = 1}^nx_i(Y_i - \overline{Y})}{\sum\limits_{i = 1}^n x_i (x_i - \overline{x})}\]</span></p>

<div class="remark">
<p> <span class="remark"><em>Remark. </em></span> <span class="math display">\[\hat\beta_1 = \frac{S_{XY}}{S_{XX}}\]</span></p>
where <span class="math inline">\(S_{XX} := \sum\limits_{i = 1}^n (x_i - \overline{x})^2\)</span> and <span class="math inline">\(S_{XY} := \sum\limits_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y})\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that <span class="math inline">\(\overline{x}^2 = \frac{1}{n^2}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2\)</span>. Then we have</p>
<p><span class="math display">\[\begin{equation} \label{eq:sxx}
  \begin{split}
    S_{XX} &amp; = \sum_{i = 1}^n (x_i - \overline{x})^2 \\
    &amp; = \sum_{i = 1}^n x_i^2 - 2\sum_{i = 1}^n x_i \overline{x} + \sum_{i = 1}^n\overline{x}^2 \\
    &amp; = \sum_{i = 1}^n x_i^2 - \frac{2}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 + \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 \\
    &amp; = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2
  \end{split}
\end{equation}\]</span></p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \hat\beta_1 &amp; = \frac{\sum x_i(Y_i - \overline{Y})}{\sum x_i (x_i - \overline{x})} \\
    &amp; = \frac{\sum x_i (Y_i - \overline{Y}) - \overline{x}\sum (Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \qquad \because \sum (Y_i - \overline{Y}) = 0 \\
    &amp; = \frac{\sum (x_i - \overline{x})(Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \\
    &amp; = \frac{S_{XY}}{S_{XX}}
  \end{split}
\end{equation*}\]</span>
</div>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> delv)</code></pre>
<pre><code>
Call:
lm(formula = y ~ x, data = delv)

Coefficients:
(Intercept)            x  
       3.32         2.18  </code></pre>
</div>
<div id="prediction-and-mean-response" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Prediction and Mean response</h3>
<blockquote>
<p>“Essentially, all models are wrong, but some are useful.”</p>
<p>—George Box</p>
</blockquote>
<p>Recall that we have assumed the <strong>linear assumption</strong> between the predictor and the response variables, i.e. the true model. Estimating <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> is same as estimating the <em>assumed true model</em>.</p>

<div class="definition">
<span id="def:eyx" class="definition"><strong>Definition 2.1  (Mean response)  </strong></span><span class="math display">\[E(Y \mid X = x) = \beta_0 + \beta_1 x\]</span>
</div>

<p>We can estimate this mean resonse by</p>
<p><span class="math display">\[\begin{equation} \label{eq:meanres}
  \widehat{E(Y \mid x)} = \hat\beta_0 + \hat\beta_1 x
\end{equation}\]</span></p>
<p>However, in practice, the model might not be true, which is included in <span class="math inline">\(\epsilon\)</span> term.</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]</span></p>
<p>Our real problem is predicting individual <span class="math inline">\(Y\)</span>, not the mean. The <em>prediction</em> of response can be done by</p>
<p><span class="math display">\[\begin{equation} \label{eq:indpred}
  \hat{Y_i}  = \hat\beta_0 + \hat\beta_1 x_i
\end{equation}\]</span></p>
<p>Observe that the values of Equation <span class="math inline">\(\eqref{eq:meanres}\)</span> and <span class="math inline">\(\eqref{eq:indpred}\)</span> are same. However, due to the <strong>error term in the prediction</strong>, it has larger standard error.</p>
</div>
<div id="lseprop" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Properties of LSE</h3>
<p>Parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> have some properties related to the expectation and variance. We can notice that these lse’s are <strong>unbiased linear estimator</strong>. In fact, these are the <em>best unbiased linear estimator</em>. This will be covered in the Gauss-Markov theorem.</p>

<div class="lemma">
<p><span id="lem:sxy" class="lemma"><strong>Lemma 2.1  </strong></span><span class="math display">\[S_{XX} = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 = \sum_{i = 1}^n x_i(x_i - \overline{x})\]</span></p>
<span class="math display">\[S_{XY} = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg) = \sum_{i = 1}^n Y_i(x_i - \overline{x})\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> We already proven the first part of <span class="math inline">\(S_{XX}\)</span>. See the Equation <span class="math inline">\(\eqref{eq:sxx}\)</span>. The second part is tivial. Since <span class="math inline">\(\sum (x_i - \overline{x}) = 0\)</span>,</p>
<p><span class="math display">\[S_{XX} = \sum_{i = 1}^n (x_i - \overline{x})^2 = \sum_{i = 1}^n (x_i - \overline{x})x_i\]</span></p>
<p>For the first part of <span class="math inline">\(S_{XY}\)</span>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    S_{XY} &amp; = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    &amp; = \sum_{i = 1}^n x_i Y_i - \overline{x} \sum_{i = 1}^n Y_i - \overline{Y} \sum_{i = 1}^n x_i + n \overline{x} \overline{Y} \\
    &amp; = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg)
  \end{split}
\end{equation*}\]</span></p>
<p>Second part of <span class="math inline">\(S_{XY}\)</span> also can be proven from the definition.</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    S_{XY} &amp; = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    &amp; = \sum_{i = 1}^n Y_i (x_i - \overline{x}) - \overline{Y} \sum_{i = 1}^n (x_i - \overline{x}) \\
    &amp; = \sum_{i = 1}^n Y_i (x_i - \overline{x}) \qquad \because \sum_{i = 1}^n (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}\]</span>
</div>


<div class="lemma">
<p><span id="lem:linbet" class="lemma"><strong>Lemma 2.2  (Linearity)  </strong></span>Each coefficient is a linear estimator.</p>
<p><span class="math display">\[\hat\beta_1 = \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i\]</span></p>
<span class="math display">\[\hat\beta_0 = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})}{S_{XX}} \bigg) Y_i\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From lemma <a href="simple.html#lem:sxy">2.1</a>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    \hat\beta_1 &amp; = \frac{S_{XY}}{S_{XX}} \\
    &amp; = \frac{1}{S_{XX}}\sum_{i = 1}^n (x_i - \overline{x}) Y_i
  \end{split}
\end{equation*}\]</span></p>
<p>It gives that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \hat\beta_0 &amp; = \overline{Y} - \hat\beta_1 \overline{x} \\
    &amp; = \frac{1}{n}\sum_{i = 1}^n Y_i - \overline{x} \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i \\
    &amp; = \sum_{i = 1}^n\bigg(\frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)Y_i
  \end{split}
\end{equation*}\]</span>
</div>


<div class="proposition">
<p><span id="prp:ue" class="proposition"><strong>Proposition 2.1  (Unbiasedness)  </strong></span>Both coefficients are unbiased.</p>
<p><span class="math inline">\(\text{(a)}\: E\hat\beta_1 = \beta_1\)</span></p>
<span class="math inline">\(\text{(b)}\: E\hat\beta_0 = \beta_0\)</span>
</div>

<p>From the model, <span class="math inline">\(Y_1, \ldots, Y_n \stackrel{indep}{\sim} (\beta_0 + \beta_1 x_i, \sigma^2)\)</span>.</p>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From lemma <a href="simple.html#lem:sxy">2.1</a>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    E\hat\beta_1 &amp; = \sum_{i = 1}^n \bigg[ \frac{(x_i - \overline{x})}{S_{XX}} E(Y_i) \bigg] \\
    &amp; = \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}(\beta_0 + \beta_1 x_i) \\
    &amp; = \frac{\beta_1 \sum (x_i - \overline{x})x_i}{\sum (x_i - \overline{x})x_i} \qquad \because \sum (x_i - \overline{x}) = 0 \\
    &amp; = \beta_1
  \end{split}
\end{equation*}\]</span></p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    E\hat\beta_0 &amp; = E(\overline{Y} - \hat\beta_1 \overline{x}) \\
    &amp; = E(\overline{Y}) - \overline{x}E(\hat\beta_1) \\
    &amp; = E(\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) - \beta_1 \overline{x} \\
    &amp; = \beta_0 + \beta_1 \overline{x} - \beta_1 \overline{x} \\
    &amp; = \beta_0
  \end{split}
\end{equation*}\]</span>
</div>


<div class="proposition">
<p><span id="prp:vb" class="proposition"><strong>Proposition 2.2  (Variances)  </strong></span>Variances and covariance of coefficients</p>
<p><span class="math inline">\(\text{(a)}\: Var\hat\beta_1 = \frac{\sigma^2}{S_{XX}}\)</span></p>
<p><span class="math inline">\(\text{(b)}\: Var\hat\beta_0 = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2\)</span></p>
<span class="math inline">\(\text{(c)}\: Cov(\hat\beta_0, \hat\beta_1) = - \frac{\overline{x}}{S_{XX}} \sigma^2\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Proving is just arithmetic.</p>
<ol style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Var\hat\beta_1 &amp; = \frac{1}{S_{XX}^2}\sum_{i = 1}^n \bigg[ (x_i - \overline{x})^2 Var(Y_i) \bigg] + \frac{1}{S_{XX}^2} \sum_{j \neq k}^n \bigg[ (x_j - \overline{x})(x_k - \overline{x}) Cov(Y_j, Y_k) \bigg] \\
    &amp; = \frac{\sigma^2}{S_{XX}} \qquad \because Cov(Y_j, Y_k) = 0 \: \text{if} \: j \neq k
  \end{split}
\end{equation*}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Var\hat\beta_0 &amp; = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)^2Var(Y_i) + \sum_{j \neq k} \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\bigg( \frac{1}{n} - \frac{(x_k - \overline{x})\overline{x}}{S_{XX}} \bigg) Cov(Y_j, Y_k) \\
    &amp; = \frac{\sigma^2}{n} - 2 \sigma^2 \frac{\overline{x}}{S_{XX}} \sum_{i = 1}^n (x_i - \overline{x}) + \frac{\sigma^2 \overline{x}^2 \sum (x_i - \overline{x})^2}{S_{XX}^2} \\
    &amp; = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \sigma^2 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li></li>
</ol>
<span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, \hat\beta_1) &amp; = Cov(\overline{Y} - \hat\beta_1 \overline{x}, \hat\beta_1) \\
    &amp; = - \overline{x} Var\hat\beta_1 \\
    &amp; = - \frac{\overline{x}}{S_{XX}} \sigma^2
  \end{split}
\end{equation*}\]</span>
</div>

</div>
<div id="gauss-markov-theorem" class="section level3">
<h3><span class="header-section-number">2.2.4</span> Gauss-Markov Theorem</h3>
<p>Chapter <a href="simple.html#lseprop">2.2.3</a> shows that the <span class="math inline">\(\beta_0^{LSE}\)</span> and <span class="math inline">\(\beta_1^{LSE}\)</span> are the <strong>linear unbiased estimators</strong>. Are these good? Good compared to <em>what estimators</em>? Here we consider <em>linear unbiased estimator</em>. If variances in the proposition <a href="simple.html#prp:vb">2.2</a> are lower than any parameters in this parameter family, <span class="math inline">\(\beta_0^{LSE}\)</span> and <span class="math inline">\(\beta_1^{LSE}\)</span> are the <strong>best linear unbiased estimators</strong>.</p>

<div class="theorem">
<p><span id="thm:gmt" class="theorem"><strong>Theorem 2.1  (Gauss Markov Theorem)  </strong></span><span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are BLUE, i.e. the best linear unbiased estimator.</p>
<p><span class="math display">\[Var(\hat\beta_0) \le Var\Big( \sum_{i = 1}^n a_i Y_i \Big) \: \forall a_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n a_i Y_i \Big) = \beta_0\]</span></p>
<span class="math display">\[Var(\hat\beta_1) \le Var\Big( \sum_{i = 1}^n b_i Y_i \Big) \: \forall b_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n b_i Y_i \Big) = \beta_1\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof</em> (Bestness of beta1). </span> Consider <span class="math inline">\(\Theta := \bigg\{ \sum\limits_{i = 1}^n b_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n b_i Y_i \Big) = \beta_1 \bigg\}\)</span>.</p>
<p>Claim: <span class="math inline">\(Var( \sum b_i Y_i) - Var(\hat\beta_1) \ge 0\)</span></p>
<p>Let <span class="math inline">\(\sum b_i Y_i \in \Theta\)</span>. Then <span class="math inline">\(E(\sum b_i Y_i) = \beta_1\)</span>.</p>
<p>Since <span class="math inline">\(E(Y_i) = \beta_0 + \beta_1 x_i\)</span>,</p>
<p><span class="math display">\[\beta_0 \sum b_i + \beta_1 \sum b_i x_i = \beta_1\]</span></p>
<p>It gives</p>
<p><span class="math display">\[\begin{equation} \label{eq:ule}
  \begin{cases}
    \sum b_i = 0 \\
    \sum b_i x_i = 1
  \end{cases}
\end{equation}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum b_i Y_i - \hat\beta_1\Big) &amp; = Var\Big( \sum b_i Y_i - \sum \frac{(x_i - \bar{x})}{S_{XX}} Y_i \Big) \\
    &amp; \stackrel{indep}{=} \sum \bigg( b_i - \frac{(x_i - \bar{x})}{S_{XX}} \bigg)^2 \sigma^2 \\
    &amp; = \sum \bigg( b_i^2 - \frac{2b_i (x_i - \bar{x})}{S_{XX}} + \frac{(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \sigma^2 \\
    &amp; = \sum b_i^2 \sigma^2 - \frac{2 \sigma^2}{S_{XX}} \sum b_i x_i + \frac{2 \bar{x} \sigma^2}{S_{XX}} \sum b_i + \sigma^2 \frac{\sum (x_i - \bar{x})^2}{S_{XX}^2} \\
    &amp; = \sum b_i^2 \sigma^2 - \frac{\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule} \:\text{and}\: S_{XX} = \sum (x_i - \bar{x})^2 \\
    &amp; = Var(\sum b_i Y_i) - Var(\hat\beta_1)
  \end{split}
\end{equation*}\]</span></p>
<p>Hence,</p>
<span class="math display">\[Var(\sum b_i Y_i) \ge Var(\hat\beta_1)\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof</em> (Bestness of beta0). </span> Consider <span class="math inline">\(\Theta := \bigg\{ \sum\limits_{i = 1}^n a_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n a_i Y_i \Big) = \beta_0 \bigg\}\)</span>.</p>
<p>Claim: <span class="math inline">\(Var( \sum a_i Y_i) - Var(\hat\beta_0) \ge 0\)</span></p>
<p>Let <span class="math inline">\(\sum a_i Y_i \in \Theta\)</span>. Then <span class="math inline">\(E(\sum a_i Y_i) = \beta_0\)</span>.</p>
<p>Since <span class="math inline">\(E(Y_i) = \beta_0 + \beta_1 x_i\)</span>,</p>
<p><span class="math display">\[\beta_0 \sum a_i + \beta_1 \sum a_i x_i = \beta_0\]</span></p>
<p>It gives</p>
<p><span class="math display">\[\begin{equation} \label{eq:ule0}
  \begin{cases}
    \sum a_i = 1 \\
    \sum a_i x_i = 0
  \end{cases}
\end{equation}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum a_iY_i - \hat\beta_0 \Big) &amp; = Var\bigg[\sum a_iY_i - \sum\Big( \frac{1}{n} - \frac{(x_k - \bar{x})\bar{x}}{S_{XX}} \Big) Y_k \bigg] \\
    &amp; = \sum \bigg(a_i - \frac{1}{n} +  \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \bigg)^2\sigma^2 \\
    &amp; = \sum \bigg[ a_i^2 - 2a_i\Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big) + \Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big)^2 \bigg]\sigma^2 \\
        &amp; = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n}\sum a_i + \frac{2\bar{x}\sigma^2\sum a_ix_i}{S_{XX}} - \frac{2\bar{x}^2\sigma^2\sum a_i}{S_{XX}} \\
        &amp; \qquad + \sigma^2\bigg( \frac{1}{n} - \frac{2\bar{x}}{nS_{XX}} \sum(x_i - \bar{x}) + \frac{\bar{x}^2\sum(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \\
        &amp; = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n} - \frac{2\bar{x}^2\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule0} \\
        &amp; \qquad + \bigg(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \qquad \because \sum(x_i - \bar{x}) = 0 \: \text{and} \: S_{XX} := \sum (x_i - \bar{x})^2 \\
        &amp; = \sum a_i^2\sigma^2 - \bigg( \frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \\
        &amp; = Var\Big( \sum a_i Y_i \Big) - Var\hat\beta_0
  \end{split}
\end{equation*}\]</span></p>
<p>Hence,</p>
<span class="math display">\[Var(\sum a_i Y_i) \ge Var(\hat\beta_0)\]</span>
</div>


<div class="example">
<span id="exm:usingnormal" class="example"><strong>Example 2.1  </strong></span>Show that <span class="math inline">\(\sum (Y_i - \hat{Y_i}) = 0\)</span>, <span class="math inline">\(\sum x_i (Y_i - \hat{Y_i}) = 0\)</span>, and <span class="math inline">\(\sum \hat{Y_i} (Y_i - \hat{Y_i}) = 0\)</span>.
</div>


<div class="solution">
<p> <span class="solution"><em>Solution. </em></span> Consider the two normal equations <span class="math inline">\(\eqref{eq:normbeta0}\)</span> and <span class="math inline">\(\eqref{eq:normbeta1}\)</span>. Note that <span class="math inline">\(\hat{Y_i} = \hat\beta_0 + \hat\beta_1 x_i\)</span>.</p>
<p>From the Equation <span class="math inline">\(\eqref{eq:normbeta0}\)</span>, we have <span class="math inline">\(\sum (Y_i - \hat{Y_i}) = 0\)</span>.</p>
<p>From the Equation <span class="math inline">\(\eqref{eq:normbeta1}\)</span>, we have <span class="math inline">\(\sum x_i (Y_i - \hat{Y_i}) = 0\)</span>.</p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    \sum \hat{Y_i} (Y_i - \hat{Y_i}) &amp; = \sum (\hat\beta_0 + \hat\beta_1 x_i) (Y_i - \hat{Y_i}) \\
    &amp; = \hat\beta_0 \sum (Y_i - \hat{Y_i}) + \hat\beta_1 \sum x_i (Y_i - \hat{Y_i}) \\
    &amp; = 0
  \end{split}
\end{equation*}\]</span>
</div>

</div>
<div id="estimation-of-sigma2" class="section level3">
<h3><span class="header-section-number">2.2.5</span> Estimation of <span class="math inline">\(\sigma^2\)</span></h3>
<p>There is the last parameter, <span class="math inline">\(\sigma^2 = Var(Y_i)\)</span>. In the <em>least squares estimation literary</em>, we estimate <span class="math inline">\(\sigma^2\)</span> by</p>
<p><span class="math display">\[\begin{equation} \label{eq:siglse}
  \hat\sigma^2 = \frac{1}{n - 2} \sum_{i = 1}^n (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2
\end{equation}\]</span></p>
<p>Why <span class="math inline">\(n - 2\)</span>? This makes the estimator unbiased.</p>

<div class="proposition">
<span id="prp:sigex" class="proposition"><strong>Proposition 2.3  (Unbiasedness)  </strong></span><span class="math display">\[E(\hat\sigma^2) = \sigma^2\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Note that</p>
<p><span class="math display">\[(Y_i - \hat\beta_0 - \hat\beta_1 x_i) = (Y_i - \overline{Y}) - \hat\beta_1(x_i - \overline{x})\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) &amp; = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2 \bigg] \\
    &amp; = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \overline{Y})^2 + \hat\beta_1^2 \sum (x_i - \overline{x})^2 -2\hat\beta_1 \sum (Y_i - \overline{Y})(x_i - \overline{x}) \bigg] \\
    &amp; = \frac{1}{n - 2} E ( S_{YY} + \hat\beta_1^2 S_{XX} - 2 \hat\beta_1 S_{XY}) \\
    &amp; = \frac{1}{n - 2} E ( S_{YY} - \hat\beta_1^2 S_{XX}) \qquad \because S_{XY} = \hat\beta_1 S_{XX} \\
    &amp; = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{XX} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big)
  \end{split}
\end{equation*}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    ES_{YY} &amp; = E\Big[ \sum (Y_i - \overline{Y})^2 \Big] \\
    &amp; = E \Big[ \sum \Big( (\beta_0 + \beta_1 x_i + \epsilon_i) - (\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) \Big)^2 \Big] \\
    &amp; = E \Big[ \sum \Big( \beta_1 (x_i - \overline{x}) + (\epsilon_i - \overline{\epsilon}) \Big)^2 \Big] \\
    &amp; = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) + 2\beta_1 \sum (x_i - \overline{x}) E(\epsilon_i - \overline{\epsilon}) \\
    &amp; = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big)
  \end{split}
\end{equation*}\]</span></p>
<p>Since <span class="math inline">\(E(\bar\epsilon) = 0\)</span> and <span class="math inline">\(Var(\bar\epsilon) = \frac{\sigma^2}{n}\)</span>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) &amp; = E \Big( \sum (\epsilon_i^2 + \bar\epsilon^2 - 2\epsilon_i \bar\epsilon) \Big) \\
    &amp; = \sum E(\epsilon_i^2) - n E(\bar\epsilon^2) \qquad \because \sum \epsilon = n \bar\epsilon \\
    &amp; = \sum (Var(\epsilon_i) + E(\epsilon_i)^2) - n(Var(\bar\epsilon) + E(\bar\epsilon)^2) \\
    &amp; = n\sigma^2 - \sigma^2 \\
    &amp; = (n - 1)\sigma^2
  \end{split}
\end{equation*}\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[ES_{YY} = \beta_1^2 S_{XX} + (n - 1)\sigma^2\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    E\hat\beta_1^2 &amp; = Var\hat\beta_1 + E(\hat\beta_1)^2 \\
    &amp; = \frac{\sigma^2}{S_{XX}} + \beta_1^2
  \end{split}
\end{equation*}\]</span></p>
<p>It follows that</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) &amp; = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{YY} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big) \\
    &amp; = \frac{1}{n - 2} \bigg( \Big(\beta_1^2 S_{XX} + (n - 1)\sigma^2 \Big) - S_{XX}\Big(\frac{\sigma^2}{S_{XX}} + \beta_1^2 \Big) \bigg) \\
    &amp; = \frac{1}{n - 2}((n - 2)\sigma^2) \\
    &amp; = \sigma^2
  \end{split}
\end{equation*}\]</span>
</div>

</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2">
<h2><span class="header-section-number">2.3</span> Maximum Likelihood Estimation</h2>
<p>In this section, we add an assumption to an random errors <span class="math inline">\(\epsilon_i\)</span>.</p>
<p><span class="math display">\[\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\]</span></p>

<div class="example">
<p><span id="exm:gmle" class="example"><strong>Example 2.2  (Gaussian Likelihood)  </strong></span>Note that <span class="math inline">\(Y_i \stackrel{indep}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)\)</span>. Then the likelihood function is</p>
<p><span class="math display">\[L(\beta_0, \beta_1, \sigma^2) = \prod_{i = 1}^n\bigg( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \bigg(- \frac{(Y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2} \bigg) \bigg)\]</span></p>
<p>and so the log-likelihood function can be computed as</p>
<span class="math display">\[l(\beta_0, \beta_1, \sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2\]</span>
</div>

<div id="likelihood-equations" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Likelihood equations</h3>

<div class="definition">
<span id="def:mledef" class="definition"><strong>Definition 2.2  (Maximum Likelihood Estimator)  </strong></span><span class="math display">\[(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) := \arg\sup L(\beta_0, \beta_1, \sigma^2)\]</span>
</div>

<p>Since <span class="math inline">\(l(\cdot) = \ln L(\cdot)\)</span> is monotone,</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> <span class="math display">\[(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) = \arg\sup l(\beta_0, \beta_1, \sigma^2)\]</span>
</div>

<p>We can find the maximum of this <em>quadratic</em> function by making first derivative.</p>
<p><span class="math display">\[\begin{equation} \label{eq:mlbeta0}
  \frac{\partial l}{\partial \beta_0} = \frac{1}{\sigma^2} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i) = 0
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation} \label{eq:mlbeta1}
  \frac{\partial l}{\partial \beta_1} = \frac{1}{\sigma^2} \sum_{i = 1}^n x_i (Y_i - \beta_0 - \beta_1 x_i) = 0
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation} \label{eq:mlsig}
  \frac{\partial l}{\partial \sigma^2} = - \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = 0
\end{equation}\]</span></p>
<p>Denote that Equations <span class="math inline">\(\eqref{eq:mlbeta0}\)</span> and <span class="math inline">\(\eqref{eq:mlbeta1}\)</span> given <span class="math inline">\(\hat\sigma^2\)</span> are equivalent to the normal equations. Thus,</p>
<p><span class="math display">\[\hat\beta_0^{MLE} = \hat\beta_0^{LSE}, \quad \hat\beta_1^{MLE} = \hat\beta_1^{LSE}\]</span></p>
<p>From <span class="math inline">\(\eqref{eq:mlsig}\)</span>,</p>
<p><span class="math display">\[\hat\sigma^{2MLE} = \frac{1}{n}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2 = \frac{n - 2}{n} \hat\sigma^{2LSE}\]</span></p>
<p>Recall that <span class="math inline">\(\hat\sigma^{2LSE}\)</span> is an unbiased, i.e. this <em>MLE is not an unbiased estimator</em>. Since <span class="math inline">\(\hat\sigma^{2MLE} \approx \hat\sigma^{2LSE}\)</span> for large <span class="math inline">\(n\)</span>, howerver, it is <em>asymptotically unbiased</em>.</p>

<div class="theorem">
<p><span id="thm:rclb" class="theorem"><strong>Theorem 2.2  (Rao-Cramer Lower Bound, univariate case)  </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta)\)</span>. If <span class="math inline">\(\hat\theta\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[Var(\hat\theta) \ge \frac{1}{I_n(\theta)}\]</span></p>
where <span class="math inline">\(I_n(\theta) = -E\bigg(\frac{\partial^2 l(\theta)}{\partial \theta^2} \bigg)\)</span>
</div>

<p>To apply this theorem <a href="simple.html#thm:rclb">2.2</a> in the simple linear regression setting, i.e. <span class="math inline">\((\beta_0, \beta_1)\)</span>, we need to look at the <em>bivariate case</em>.</p>

<div class="theorem">
<p><span id="thm:rclb2" class="theorem"><strong>Theorem 2.3  (Rao-Cramer Lower Bound, bivariate case)  </strong></span>Let <span class="math inline">\(X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta1, \theta_2)\)</span> and let <span class="math inline">\(\boldsymbol{\theta} = (\theta_1, \theta_2)^T\)</span>. If each <span class="math inline">\(\hat\theta_1\)</span>, <span class="math inline">\(\hat\theta_2\)</span> is an unbiased estimator of <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, then</p>
<p><span class="math display">\[
Var(\boldsymbol{\theta}) := \begin{bmatrix}
Var(\hat\theta_1) &amp; Cov(\hat\theta_1, \hat\theta_2) \\
Cov(\hat\theta_1, \hat\theta_2) &amp; Var(\hat\theta_2)
\end{bmatrix} \ge I_n^{-1}(\theta_1, \theta_2)
\]</span></p>
<p>where</p>
<span class="math display">\[
I_n(\theta_1, \theta_2) = - \begin{bmatrix}
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1^2} \bigg) &amp; E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) \\
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) &amp; E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_2^2} \bigg)
\end{bmatrix}
\]</span>
</div>

<p>Assume that <span class="math inline">\(\sigma^2\)</span> is <strong>known</strong>. From the Equations <span class="math inline">\(\eqref{eq:mlbeta0}\)</span> and <span class="math inline">\(\eqref{eq:mlbeta1}\)</span>,</p>
<p><span class="math display">\[
\begin{cases}
  \frac{\partial^2 l}{\partial \beta_0^2} = - \frac{n}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_1^2} = - \frac{\sum x_i^2}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_0 \partial \beta_1} = - \frac{\sum x_i}{\sigma^2}
\end{cases}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
I_n(\beta_0, \beta_1) = \begin{bmatrix}
  \frac{n}{\sigma^2} &amp; \frac{\sum x_i}{\sigma^2} \\
  \frac{\sum x_i}{\sigma^2} &amp; \frac{\sum x_i^2}{\sigma^2}
\end{bmatrix}
\]</span></p>
<p>Applying gaussian elimination,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} &amp; \frac{\sum x_i}{\sigma^2} &amp; 1 &amp; 0 \\
      \frac{\sum x_i}{\sigma^2} &amp; \frac{\sum x_i^2}{\sigma^2} &amp; 0 &amp; 1
    \end{array}
    \right] &amp; \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} &amp; \frac{\sum x_i}{\sigma^2} &amp; 1 &amp; 0 \\
      \frac{\sum x_i}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) &amp; \frac{\sum x_i^2}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) &amp; 0 &amp; \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    &amp; \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} &amp; \frac{\sum x_i}{\sigma^2} &amp; 1 &amp; 0 \\
      0 &amp; \frac{\sum x_i^2 - \overline{x}\sum x_i}{\sigma^2\overline{x}} = \frac{S_{XX}}{\sigma^2\overline{x}} &amp; -1 &amp; \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    &amp; \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 &amp; \overline{x} &amp; \frac{\sigma^2}{n} &amp; 0 \\
      0 &amp; 1 &amp; -\frac{\overline{x}}{S_{XX}}\sigma^2 &amp; \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right] \\
    &amp; \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 &amp; 0 &amp; \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 &amp; -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
      0 &amp; 1 &amp; -\frac{\overline{x}}{S_{XX}}\sigma^2 &amp; \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right]
  \end{split}
\end{equation*}\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
I_n^{-1}(\beta_0, \beta_1) = \begin{bmatrix}
  \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 &amp; -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
  -\frac{\overline{x}}{S_{XX}}\sigma^2 &amp; \frac{\sigma^2}{S_{XX}}
\end{bmatrix} = \begin{bmatrix}
  Var(\hat\beta_0) &amp; Cov(\hat\beta_0, \hat\beta_1) \\
  Cov(\hat\beta_0, \hat\beta_1) &amp; Var(\hat\beta_1)
\end{bmatrix}
\]</span></p>
<p>Since <span class="math inline">\(Var(\boldsymbol{\hat\beta}) - I^{-1} = 0\)</span> is non-negative definite, each <span class="math inline">\(Var(\hat\beta_0) = \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2\)</span> and <span class="math inline">\(Var(\hat\beta_1) = \frac{\sigma^2}{S_{XX}}\)</span> is a theoretical bound.</p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> This says that <span class="math inline">\(\hat\beta_0^{LSE} = \hat\beta_0^{MLE}\)</span> and <span class="math inline">\(\hat\beta_1^{LSE} = \hat\beta_1^{MLE}\)</span> have the smallest variance among all unbiased estimator.
</div>

<p>This result is <em>stronger than Gauss-Markov theorem</em> <a href="simple.html#thm:gmt">2.1</a>, where the LSE has the smalleset variance among all <em>linear unbiased</em> estimators. It can be simply obtained from the <em>Lehmann-Scheffe Theorem</em>: If some unbiased estimator is a function of complete sufficient statistic, then this estimator is the unique MVUE <span class="citation">(Hogg, McKean, and Craig <a href="#ref-Hogg:2018aa" role="doc-biblioref">2018</a>)</span>.</p>

<div class="remark">
 <span class="remark"><em>Remark</em> (Lehmann and Scheffe for regression coefficients). </span> <span class="math inline">\(u\Big(\sum Y_i, S_{XY} \Big)\)</span> is CSS in this regression problem, i.e. known <span class="math inline">\(\sigma^2\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> From the example <a href="simple.html#exm:gmle">2.2</a>,</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    L(\beta_0, \beta_1) &amp; = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum(Y_i - \beta_0 - \beta_1 x_i)^2 \bigg] \\
    &amp; = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum \Big(Y_i^2 - (\beta_0 + \beta_1 x_i)Y_i + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg] \\
    &amp; = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \Big( -\beta_0 \sum Y_i - \beta_1 \sum x_i Y_i \Big) \bigg] \exp\bigg[-\frac{1}{2\sigma^2} \Big( \sum Y_i^2 + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg]
  \end{split}
\end{equation*}\]</span></p>
<p>By the Factorization theorem, both <span class="math inline">\(\sum Y_i\)</span> and <span class="math inline">\(\sum x_i Y_i\)</span> are sufficient statistics. Since <span class="math inline">\(S_{XY}\)</span> is one-to-one function of <span class="math inline">\(\sum x_i Y_i\)</span>, it is also a sufficient statistic.</p>
<p>Denote that the normal distribution is in exponential family.</p>
Hence, <span class="math inline">\((\sum Y_i, S_{XY})\)</span> are CSS.
</div>

</div>
</div>
<div id="residuals" class="section level2">
<h2><span class="header-section-number">2.4</span> Residuals</h2>

<div class="definition">
<span id="def:res" class="definition"><strong>Definition 2.3  (Residuals)  </strong></span><span class="math display">\[e_i := Y_i - \hat{Y_i}\]</span>
</div>

<div id="prediction-error" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Prediction error</h3>
<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">yhat =</span> <span class="kw">predict</span>(<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x))) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_linerange</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> y, <span class="dt">ymax =</span> yhat), <span class="dt">col =</span> <span class="kw">I</span>(<span class="st">&quot;red&quot;</span>), <span class="dt">alpha =</span> <span class="fl">.7</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Number of Cases&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Delivery Time&quot;</span>
  )</code></pre>
<div class="figure" style="text-align: center"><span id="fig:regplot"></span>
<img src="regression-analysis_files/figure-html/regplot-1.png" alt="Fit and residuals\label{fig:regplot}" width="70%" />
<p class="caption">
Figure 2.3: Fit and residuals
</p>
</div>
<p>See <span class="math inline">\(\text{Figure }\ref{fig:regplot}\)</span>. Each red line is <span class="math inline">\(e_i\)</span>. As we can see, <span class="math inline">\(e_i\)</span> represents the difference between <em>observed</em> response and <em>predicted</em> response. A large <span class="math inline">\(\lvert e_i \rvert\)</span> indicates a large prediction error. You can call this <span class="math inline">\(e_i\)</span> for each <span class="math inline">\(Y_i\)</span> by <code>lm()$residuals</code> or <code>residuals()</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">delv_fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> delv)
delv_fit<span class="op">$</span>residuals</code></pre>
<pre><code>     1      2      3      4      5      6      7      8      9     10 
-1.874  1.651  2.181  2.855 -2.628 -0.444  0.327 -0.724 10.634  7.298 
    11     12     13     14     15     16     17     18     19     20 
 2.191 -4.082  1.475  3.372  1.094  3.918 -1.028  0.446 -0.349 -5.216 
    21     22     23     24     25 
-7.182 -7.581 -4.156 -0.900 -1.275 </code></pre>
<p><span class="math inline">\(\sum e_i^2\)</span>, which has been minimized in the procedure of LSE, can be used to see <em>overall size of prediction errors</em>.</p>

<div class="definition">
<span id="def:sse" class="definition"><strong>Definition 2.4  (Error Sums of Squares)  </strong></span><span class="math display">\[SSE := \sum_{i = 1}^n e_i^2\]</span>
</div>

</div>
<div id="residuals-and-the-variance" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Residuals and the variance</h3>
<p><span class="math inline">\(e_i\)</span> is a random quantity, which contains the information for <span class="math inline">\(\epsilon_i\)</span>. <span class="math inline">\(\sum e_i^2\)</span> can give information about <span class="math inline">\(\sigma^2 = Var(\epsilon_i)\)</span>. For this, it is expected that <span class="math inline">\(e_i\)</span> and <span class="math inline">\(\epsilon_i\)</span> have similar feature.</p>

<div class="lemma">
<p><span id="lem:yandbet" class="lemma"><strong>Lemma 2.3  </strong></span>Covriance between Y and each coefficient</p>
<p><span class="math inline">\(\text{(a)}\: Cov(\hat\beta_0, Y_i) = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2\)</span></p>
<span class="math inline">\(\text{(b)}\: Cov(\hat\beta_1, Y_i) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> (a)</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, Y_i) &amp; = Cov(\sum a_i Y_i, Y_i) \\
    &amp; = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2
  \end{split}
\end{equation*}\]</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li></li>
</ol>
<span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(\hat\beta_1, Y_i) &amp; = Cov(\sum b_i Y_i, Y_i) \\
    &amp; = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2
  \end{split}
\end{equation*}\]</span>
</div>


<div class="proposition">
<p><span id="prp:resprop" class="proposition"><strong>Proposition 2.4  (Properties of residuals)  </strong></span>Mean and variance of the residual</p>
<p><span class="math inline">\(\text{(a)}\: E(e_i) = 0\)</span></p>
<p><span class="math inline">\(\text{(b)}\: Var(e_i) \neq \sigma^2\)</span></p>
<span class="math inline">\(\text{(c)}\: \forall i \neq j : Cov(e_i, e_j) \neq 0\)</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> (a) Recall that this is the assumption of the regression model.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Lemma <a href="simple.html#lem:yandbet">2.3</a> implies that</li>
</ol>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(\overline{Y}, \hat\beta_1) &amp; = Cov(\frac{1}{n}\sum Y_i, \hat\beta_1) \\
    &amp; = \frac{1}{n} \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    &amp; = 0 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{equation} \label{eq:predvar}
  \begin{split}
    Var(\hat{Y_i}) &amp; = Var(\hat\beta_0 + \hat\beta_1 x_i) \\
    &amp; = Var \bigg[ \overline{Y} + (x_i - \overline{x}) \hat\beta_1 \bigg] \qquad \because \hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x} \\
    &amp; = Var(\overline{Y}) + (x_i - \overline{x})^2 Var(\hat\beta_1) + 2(x_i - \overline{x}) Cov(\overline{Y}, \hat\beta_1) \\
    &amp; = \frac{\sigma^2}{n} + (x_i - \overline{x})^2\frac{\sigma^2}{S_{XX}} + 0 \\
    &amp; = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
\end{equation}\]</span></p>
<p>From the same lemma <a href="simple.html#lem:yandbet">2.3</a>,</p>
<p><span class="math display">\[\begin{equation} \label{eq:yyhat}
  \begin{split}
    Cov(Y_i, \hat{Y_i}) &amp; = Cov(Y_i, \overline{Y} + (x_i - \overline{x}) \hat\beta_1) \\
    &amp; = Cov(Y_i, \overline{Y}) + (x_i - \overline{x}) Cov(Y_i, \hat\beta_1) \\
    &amp; = \frac{\sigma^2}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}}\sigma^2 \qquad \because Cov(Y_i, \hat\beta_1) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    &amp; = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
\end{equation}\]</span></p>
<p>These <span class="math inline">\(\eqref{eq:predvar}\)</span> and <span class="math inline">\(\eqref{eq:yyhat}\)</span> give that</p>
<p><span class="math display">\[\begin{equation*}
  \begin{split}
    Var(e_i) &amp; = Var(Y_i) + Var(\hat{Y_i}) -2Cov(Y_i, \hat{Y_i}) \\
    &amp; = \sigma^2 + \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 - 2 \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    &amp; = \bigg(1 - \frac{1}{n} - \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    &amp; \neq \sigma^2
  \end{split}
\end{equation*}\]</span></p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Let <span class="math inline">\(i \neq j\)</span>. Then</li>
</ol>
<span class="math display">\[\begin{equation*}
  \begin{split}
    Cov(e_i, e_j) &amp; = Cov\Big( Y_i - (\hat\beta_0 + \hat\beta_1 x_i), Y_j - (\hat\beta_0 + \hat\beta_1 x_j) \Big) \\
    &amp; = Cov(Y_i, Y_j) - Cov\Big(Y_i, (\hat\beta_0 + \hat\beta_1 x_j) \Big) - Cov((\hat\beta_0 + \hat\beta_1 x_i), Y_j) + Cov((\hat\beta_0 + \hat\beta_1 x_i), (\hat\beta_0 + \hat\beta_1 x_j)) \\
    &amp; = 0 - \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_j}{S_{XX}}\sigma^2 \\
    &amp; \qquad - \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_i}{S_{XX}}\sigma^2 \\
    &amp; \qquad + \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    &amp; = - \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    &amp; \neq 0
  \end{split}
\end{equation*}\]</span>
</div>

</div>
</div>
<div id="decomposition-of-total-variability" class="section level2">
<h2><span class="header-section-number">2.5</span> Decomposition of Total Variability</h2>

<div class="definition">
<span id="def:unsst" class="definition"><strong>Definition 2.5  (Uncorrected Total Sum of Squares)  </strong></span><span class="math display">\[SST_{uncor} := \sum_{i = 1}^n Y_i^2\]</span>
</div>


<div class="definition">
<span id="def:sst" class="definition"><strong>Definition 2.6  (Corrected Total Sum of Squares)  </strong></span><span class="math display">\[SST := \sum_{i = 1}^n (Y_i - \overline{Y})^2\]</span>
</div>

<p>What does this total sum of squares mean? To know this, we should know <span class="math inline">\(\overline{Y}\)</span> first.</p>
<pre class="sourceCode r"><code class="sourceCode r">delv <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Number of Cases&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Delivery Time&quot;</span>
  )</code></pre>
<div class="figure" style="text-align: center"><span id="fig:ybarpred"></span>
<img src="regression-analysis_files/figure-html/ybarpred-1.png" alt="Regression without predictor\label{fig:ybarpred}" width="70%" />
<p class="caption">
Figure 2.4: Regression without predictor
</p>
</div>
<p>See <span class="math inline">\(\text{Figure }\ref{fig:ybarpred}\)</span>. The line represents the closest line when we use only intercept term for the regression model. In other words, <em>if we use no information for the response</em>, i.e. no predictor variables, we will get just average of the response variable. Consider</p>
<p><span class="math display">\[Y_i = \beta_0 + \epsilon_i\]</span></p>
<p>Then we can get only one normal equation</p>
<p><span class="math display">\[\sum (Y_i - \hat\beta_0) = 0\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[\hat\beta_0 = \frac{1}{n} \sum_{i = 1}^n Y_i \equiv \overline{Y}\]</span></p>
<p>From this fact, <span class="math inline">\(SST\)</span> implies</p>

<div class="proposition">
<p><span id="prp:decom" class="proposition"><strong>Proposition 2.5  (Decomposition of SST)  </strong></span><span class="math display">\[SST = SSR + SSE\]</span></p>
where <span class="math inline">\(SSR = \sum (\hat{Y_i} - \overline{Y})^2\)</span> and <span class="math inline">\(SSE = \sum (Y_i - \hat{Y_i})\)</span>
</div>

</div>
<div id="geometric-interpretations" class="section level2">
<h2><span class="header-section-number">2.6</span> Geometric Interpretations</h2>

<div id="refs" class="references">
<div>
<p>Hogg, Robert V., Joseph W. McKean, and Allen Thornton Craig. 2018. <em>Introduction to Mathematical Statistics</em>. 8th ed. Pearson College Division.</p>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Hogg:2018aa">
<p>Hogg, Robert V., Joseph W. McKean, and Allen Thornton Craig. 2018. <em>Introduction to Mathematical Statistics</em>. 8th ed. Pearson College Division.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression-analysis.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["regression-analysis.pdf", "regression-analysis.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
