<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 2 Multiple Linear Regression | R Lab for Regression Analysis</title>
  <meta name="description" content="This aims at covering materials of regression analysis with R programming.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 2 Multiple Linear Regression | R Lab for Regression Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="cover.png" />
  <meta property="og:description" content="This aims at covering materials of regression analysis with R programming." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Multiple Linear Regression | R Lab for Regression Analysis" />
  
  <meta name="twitter:description" content="This aims at covering materials of regression analysis with R programming." />
  <meta name="twitter:image" content="cover.png" />




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="simple.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Lab for Regression Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#linear-regression-analysis"><i class="fa fa-check"></i>Linear Regression Analysis</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#relation"><i class="fa fa-check"></i>Relation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="simple.html"><a href="simple.html#model"><i class="fa fa-check"></i><b>1.1</b> Model</a></li>
<li class="chapter" data-level="1.2" data-path="simple.html"><a href="simple.html#least-squares-estimation"><i class="fa fa-check"></i><b>1.2</b> Least Squares Estimation</a><ul>
<li class="chapter" data-level="1.2.1" data-path="simple.html"><a href="simple.html#normal-equations"><i class="fa fa-check"></i><b>1.2.1</b> Normal equations</a></li>
<li class="chapter" data-level="1.2.2" data-path="simple.html"><a href="simple.html#prediction-and-mean-response"><i class="fa fa-check"></i><b>1.2.2</b> Prediction and Mean response</a></li>
<li class="chapter" data-level="1.2.3" data-path="simple.html"><a href="simple.html#lseprop"><i class="fa fa-check"></i><b>1.2.3</b> Properties of LSE</a></li>
<li class="chapter" data-level="1.2.4" data-path="simple.html"><a href="simple.html#gauss-markov-theorem"><i class="fa fa-check"></i><b>1.2.4</b> Gauss-Markov Theorem</a></li>
<li class="chapter" data-level="1.2.5" data-path="simple.html"><a href="simple.html#estimation-of-sigma2"><i class="fa fa-check"></i><b>1.2.5</b> Estimation of <span class="math inline">\(\sigma^2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="simple.html"><a href="simple.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>1.3</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="1.3.1" data-path="simple.html"><a href="simple.html#likelihood-equations"><i class="fa fa-check"></i><b>1.3.1</b> Likelihood equations</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="simple.html"><a href="simple.html#residuals"><i class="fa fa-check"></i><b>1.4</b> Residuals</a><ul>
<li class="chapter" data-level="1.4.1" data-path="simple.html"><a href="simple.html#prediction-error"><i class="fa fa-check"></i><b>1.4.1</b> Prediction error</a></li>
<li class="chapter" data-level="1.4.2" data-path="simple.html"><a href="simple.html#residuals-and-the-variance"><i class="fa fa-check"></i><b>1.4.2</b> Residuals and the variance</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="simple.html"><a href="simple.html#decomposition-of-total-variability"><i class="fa fa-check"></i><b>1.5</b> Decomposition of Total Variability</a><ul>
<li class="chapter" data-level="1.5.1" data-path="simple.html"><a href="simple.html#total-sum-of-squares"><i class="fa fa-check"></i><b>1.5.1</b> Total sum of squares</a></li>
<li class="chapter" data-level="1.5.2" data-path="simple.html"><a href="simple.html#regression-sum-of-squares"><i class="fa fa-check"></i><b>1.5.2</b> Regression sum of squares</a></li>
<li class="chapter" data-level="1.5.3" data-path="simple.html"><a href="simple.html#residual-sum-of-squares"><i class="fa fa-check"></i><b>1.5.3</b> Residual sum of squares</a></li>
<li class="chapter" data-level="1.5.4" data-path="simple.html"><a href="simple.html#decompsst"><i class="fa fa-check"></i><b>1.5.4</b> Decomposition of total sum of squares</a></li>
<li class="chapter" data-level="1.5.5" data-path="simple.html"><a href="simple.html#coefficient-of-determination"><i class="fa fa-check"></i><b>1.5.5</b> Coefficient of determination</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="simple.html"><a href="simple.html#matnot"><i class="fa fa-check"></i><b>1.6</b> Geometric Interpretations</a><ul>
<li class="chapter" data-level="1.6.1" data-path="simple.html"><a href="simple.html#fundamental-subspaces"><i class="fa fa-check"></i><b>1.6.1</b> Fundamental subspaces</a></li>
<li class="chapter" data-level="1.6.2" data-path="simple.html"><a href="simple.html#simple-linear-regression"><i class="fa fa-check"></i><b>1.6.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="1.6.3" data-path="simple.html"><a href="simple.html#solproj"><i class="fa fa-check"></i><b>1.6.3</b> Projection mapping</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="simple.html"><a href="simple.html#simpledist"><i class="fa fa-check"></i><b>1.7</b> Distributions</a><ul>
<li class="chapter" data-level="1.7.1" data-path="simple.html"><a href="simple.html#mean-response-and-response"><i class="fa fa-check"></i><b>1.7.1</b> Mean response and response</a></li>
<li class="chapter" data-level="1.7.2" data-path="simple.html"><a href="simple.html#simplebdist"><i class="fa fa-check"></i><b>1.7.2</b> Regression coefficients</a></li>
<li class="chapter" data-level="1.7.3" data-path="simple.html"><a href="simple.html#mean-response"><i class="fa fa-check"></i><b>1.7.3</b> Mean response</a></li>
<li class="chapter" data-level="1.7.4" data-path="simple.html"><a href="simple.html#response"><i class="fa fa-check"></i><b>1.7.4</b> Response</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="simple.html"><a href="simple.html#statistical-inference"><i class="fa fa-check"></i><b>1.8</b> Statistical Inference</a><ul>
<li class="chapter" data-level="1.8.1" data-path="simple.html"><a href="simple.html#confidence-interval"><i class="fa fa-check"></i><b>1.8.1</b> Confidence interval</a></li>
<li class="chapter" data-level="1.8.2" data-path="simple.html"><a href="simple.html#prediction-interval"><i class="fa fa-check"></i><b>1.8.2</b> Prediction interval</a></li>
<li class="chapter" data-level="1.8.3" data-path="simple.html"><a href="simple.html#hypothesis-testing"><i class="fa fa-check"></i><b>1.8.3</b> Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="simple.html"><a href="simple.html#analysis-of-variance"><i class="fa fa-check"></i><b>1.9</b> Analysis of Variance</a><ul>
<li class="chapter" data-level="1.9.1" data-path="simple.html"><a href="simple.html#useful-distributions"><i class="fa fa-check"></i><b>1.9.1</b> Useful distributions</a></li>
<li class="chapter" data-level="1.9.2" data-path="simple.html"><a href="simple.html#quadratic-form"><i class="fa fa-check"></i><b>1.9.2</b> Quadratic form</a></li>
<li class="chapter" data-level="1.9.3" data-path="simple.html"><a href="simple.html#anova-for-testing-significance-of-regression"><i class="fa fa-check"></i><b>1.9.3</b> ANOVA for testing significance of regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple.html"><a href="multiple.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple.html"><a href="multiple.html#model-1"><i class="fa fa-check"></i><b>2.1</b> Model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple.html"><a href="multiple.html#least-square-estimation"><i class="fa fa-check"></i><b>2.2</b> Least Square Estimation</a><ul>
<li class="chapter" data-level="2.2.1" data-path="multiple.html"><a href="multiple.html#normal-equation"><i class="fa fa-check"></i><b>2.2.1</b> Normal equation</a></li>
<li class="chapter" data-level="2.2.2" data-path="multiple.html"><a href="multiple.html#orthogonal-decomposition"><i class="fa fa-check"></i><b>2.2.2</b> Orthogonal decomposition</a></li>
<li class="chapter" data-level="2.2.3" data-path="multiple.html"><a href="multiple.html#gram-schmidt-qr-factorization"><i class="fa fa-check"></i><b>2.2.3</b> Gram-Schmidt QR factorization</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/ygeunkim/regression-analysis" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R Lab for Regression Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Multiple Linear Regression</h1>
<div id="model-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Model</h2>
<pre class="sourceCode r"><code class="sourceCode r">(cem &lt;-<span class="st"> </span>MPV<span class="op">::</span>cement <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">tbl_df</span>())</code></pre>
<pre><code># A tibble: 13 x 5
       y    x1    x2    x3    x4
   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
 1  78.5     7    26     6    60
 2  74.3     1    29    15    52
 3 104.     11    56     8    20
 4  87.6    11    31     8    47
 5  95.9     7    52     6    33
 6 109.     11    55     9    22
 7 103.      3    71    17     6
 8  72.5     1    31    22    44
 9  93.1     2    54    18    22
10 116.     21    47     4    26
11  83.8     1    40    23    34
12 113.     11    66     9    12
13 109.     10    68     8    12</code></pre>
<p>Above is a data set about cement and concerning four ingredients from the <span class="citation">Montgomery, Peck, and Vining (<a href="#ref-Montgomery:2015aa" role="doc-biblioref">2015</a>)</span> textbook.</p>
<ul>
<li><code>y</code>: heat evolved in calories per gram of cement</li>
<li><code>x1</code>: tricalcium aluminate</li>
<li><code>x2</code>: tricalcium silicate</li>
<li><code>x3</code>: tetracalcium alumino ferrite</li>
<li><code>x4</code>: dicalcium silicate</li>
</ul>
<p>Given data <span class="math inline">\((x_{11}, x_{12}, \ldots, x_{1p}, Y_1), \ldots, (x_{n1}, x_{n2}, \ldots, x_{np}, Y_n)\)</span> (<span class="math inline">\(p = 4\)</span>), we try to fit linear regression model</p>
<p><span class="math display">\[Y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} + \epsilon_{i}\]</span></p>
<p>with</p>
<p><span class="math display">\[\epsilon_i \iid (0, \sigma^2)\]</span></p>
<p>Compared to simple linear regression problem <a href="simple.html#simple">1</a>, we have more parameters for coefficients</p>
<p><span class="math display">\[(\beta_0, \beta_1, \ldots, \beta_p, \sigma^2)\]</span></p>
<p>Each <span class="math inline">\(\beta_j\)</span> is a change of <span class="math inline">\(Y\)</span> when each predictor variable <span class="math inline">\(x_j\)</span> increases in 1 unit while the others fixed. In this part, we use <em>matrix notation</em>. Extending our former matrix work <a href="simple.html#matnot">1.6</a>,</p>
<p><span class="math display">\[
\underset{\substack{\\ \\ \\ \\ \huge \mathbf{Y}}}{\begin{bmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
\end{bmatrix}} = \underset{\substack{\\ \\ \\ \\ \huge X}}{\begin{bmatrix}
  1 &amp; x_{11} &amp; \cdots &amp; x_{1p} \\
  1 &amp; x_{21} &amp; \cdots &amp; x_{2p} \\
  \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
  1 &amp; x_{n1} &amp; \cdots &amp; x_{np}
\end{bmatrix}} \underset{\substack{\\ \\ \\ \\ \huge \boldsymbol\beta}}{\begin{bmatrix}
  \beta_0 \\
  \vdots \\
  \beta_p
\end{bmatrix}} + \underset{\substack{\\ \\ \\ \\ \huge \boldsymbol\epsilon}}{\begin{bmatrix}
  \epsilon_1 \\
  \epsilon_2 \\
  \vdots \\
  \epsilon_n
\end{bmatrix}}
\]</span></p>
<p>where <span class="math inline">\(\epsilon_i\)</span> are i.i.d., and</p>
<p><span class="math display">\[E\boldsymbol\epsilon = \mathbf{0}\]</span></p>
<p><span class="math display">\[Var\boldsymbol\epsilon = \sigma^2 I\]</span></p>
</div>
<div id="least-square-estimation" class="section level2">
<h2><span class="header-section-number">2.2</span> Least Square Estimation</h2>
<p>Write <span class="math inline">\(\boldsymbol\beta \equiv (\beta_1, \ldots, \beta_p)^T \in \R^{p + 1}\)</span>. Extend Equation <a href="simple.html#eq:qmatrix">(1.14)</a>.</p>
<p><span class="math display" id="eq:qmultiple">\[\begin{equation}
  \begin{split}
    \boldsymbol{\hat\beta} &amp; = \argmin_{\boldsymbol\beta \in \R^{p + 1}} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_{i1} - \cdots - \beta_p x_{ip})^2 \\
    &amp; = \argmin_{\boldsymbol\beta \in \R^{p + 1}} \lVert \mathbf{Y} - \beta_0 \mathbf{1} - \beta_1 \mathbf{x}_1 - \cdots - \beta_p \mathbf{x}_p \rVert^2 \\
    &amp; = \argmin_{\boldsymbol\beta \in \R^{p + 1}} \lVert \mathbf{Y} - X\boldsymbol\beta \rVert^2
  \end{split}
  \tag{2.1}
\end{equation}\]</span></p>
<p>As discussed, the solution <span class="math inline">\(\boldsymbol{\hat\beta}\)</span> is related to the projection. <span class="math inline">\(X\boldsymbol{\hat\beta}\)</span> is a projection of <span class="math inline">\(\mathbf{Y}\)</span> onto <span class="math inline">\(Col(X)\)</span>.</p>
<div id="normal-equation" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Normal equation</h3>
<p>Now recap the section <a href="simple.html#solproj">1.6.3</a>. Fundamental subspaces theorem <a href="simple.html#thm:fundsub">1.4</a> implies that</p>
<p><span class="math display">\[\mathbf{Y} - X\boldsymbol{\hat\beta} \in Col(X)^{\perp} = N(X^T)\]</span></p>
<p>From the second part of subset, i.e. <span class="math inline">\(N(X^T)\)</span>, we now have <em>Normal equation</em></p>
<p><span class="math display" id="eq:multeq">\[\begin{equation}
  X^T(\mathbf{Y} - X\boldsymbol{\hat\beta}) = \mathbf{0}
  \tag{2.2}
\end{equation}\]</span></p>
<p>This is equivalent to</p>
<p><span class="math display">\[X^T\mathbf{Y} = X^TX\boldsymbol{\hat\beta}\]</span></p>
<p>Hence, if <span class="math inline">\(X^T X\)</span> is invertible, the equation gives unique solution</p>
<p><span class="math display">\[\boldsymbol{\hat\beta} = (X^TX)^{-1}X^T \mathbf{Y}\]</span></p>
<p>Our first question is when <span class="math inline">\(X^T X\)</span> is invertible, and Theorem <a href="simple.html#thm:fullrank">1.8</a> have said that it is when the model matrix <span class="math inline">\(X\)</span> is full rank.</p>

<div class="lemma">
<p><span id="lem:modelnnd" class="lemma"><strong>Lemma 2.1  </strong></span>Let <span class="math inline">\(X \in \R^{n \times (p + 1)}\)</span> be any model matrix. Then <span class="math inline">\(X^T X\)</span> is always non-negative definite.</p>
<span class="math display">\[\forall \mathbf{v} \in \R^{p + 1} : \mathbf{v}^T(X^T X)\mathbf{v} \ge 0\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Let <span class="math inline">\(\mathbf{v} \in \R^{p + 1}\)</span>. Then</p>
<span class="math display">\[\mathbf{v}^T(X^T X)\mathbf{v} = (X\mathbf{v})^T (X\mathbf{v}) = \lVert X\mathbf{v} \rVert^2 \ge 0\]</span>
</div>

<p>This lemma can also prove our Theorem <a href="simple.html#thm:fullrank">1.8</a>.</p>

<div class="theorem">
<p><span id="thm:fullrank2" class="theorem"><strong>Theorem 2.1  </strong></span>Let <span class="math inline">\(\mathbf{Y} = X\boldsymbol\beta\)</span> inconsistent and let <span class="math inline">\(X \in \R^{n \times (p + 1)}\)</span> with <span class="math inline">\(n &gt; p + 1\)</span>.</p>
If <span class="math inline">\(rank(X) = p + 1\)</span>, i.e. full rank, then <span class="math inline">\(X^T X\)</span> is invertible.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Let <span class="math inline">\(\mathbf{c} \in \R^{(p + 1)}\)</span></p>
<p>Suppose that <span class="math inline">\(X^T X\)</span> is positive definite.</p>
<span class="math display">\[\begin{equation*}
  \begin{split}
    &amp; \Leftrightarrow \mathbf{c}^TX^T X \mathbf{c} = 0 \quad \text{implies} \quad \mathbf{c} = \mathbf{0} \\
    &amp; \Leftrightarrow X\mathbf{c} = \mathbf{0} \quad \text{implies} \quad \mathbf{c} = \mathbf{0} \\
    &amp; \Leftrightarrow \text{columns of}\: X \:\text{linearly independent} \\
    &amp; \Leftrightarrow rank(X) = p + 1
  \end{split}
\end{equation*}\]</span>
</div>

</div>
<div id="orthogonal-decomposition" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Orthogonal decomposition</h3>
<p>Let’s briefly look at orthogonalization process.</p>

<div class="theorem">
<p><span id="thm:gs" class="theorem"><strong>Theorem 2.2  (Gram-Schmidt Process)  </strong></span>Let <span class="math inline">\(\{ \mathbf{a}_1, \ldots, \mathbf{a}_m \}\)</span> be a basis for the inner product space <span class="math inline">\(V\)</span>. Let</p>
<p><span class="math display">\[\mathbf{u}_1 = \bigg( \frac{1}{\lVert \mathbf{a}_1 \rVert} \bigg) \mathbf{a}_1\]</span></p>
<p>and define next <span class="math inline">\(\mathbf{u}_2, \ldots, \mathbf{u}_m\)</span> recursively by</p>
<p><span class="math display">\[\mathbf{u}_{k + 1} = \frac{1}{\lVert \mathbf{a}_{k + 1} - \mathbf{p}_k \rVert}(\mathbf{a}_{k + 1} - \mathbf{p}_k)\]</span></p>
<p>for <span class="math inline">\(k = 1, \ldots, m - 1\)</span>, where</p>
<p><span class="math display">\[\mathbf{p}_k = &lt;\mathbf{a}_{k + 1}, \mathbf{u}_1&gt; \mathbf{u}_1 + &lt;\mathbf{a}_{k + 1}, \mathbf{u}_2 &gt;\mathbf{u}_2 + \cdots + &lt; \mathbf{a}_{k + 1}, \mathbf{u} &gt; \mathbf{u}_k\]</span></p>
<p>is the projection of <span class="math inline">\(\mathbf{a}_{k + 1}\)</span> onto <span class="math inline">\(sp(\{ \mathbf{u}_1, \ldots, \mathbf{u}_k \})\)</span>.</p>
Hence, we get <span class="math inline">\(\{ \mathbf{u}_1, \ldots, \mathbf{u}_m \}\)</span> is an orthonormal basis for <span class="math inline">\(V\)</span>.
</div>

<p>This <em>orthonormal basis</em> gives some useful facts with least squares problem <span class="citation">(Leon <a href="#ref-Leon:2014aa" role="doc-biblioref">2014</a>)</span>.</p>

<div class="theorem">
<p><span id="thm:orthonormal" class="theorem"><strong>Theorem 2.3  </strong></span>Let <span class="math inline">\(Col(X)\)</span> be a subspace of <span class="math inline">\(\R^n\)</span>, let <span class="math inline">\(\mathbf{Y} \in \R^n\)</span>, and let <span class="math inline">\(\{ \mathbf{u}_0, \ldots, \mathbf{u}_{p} \}\)</span> be an orthonormal basis for <span class="math inline">\(Col(X)\)</span>. If</p>
<p><span class="math display">\[\mathbf{\hat{Y}} = \sum_{j = 0}^p \hat\beta_j \mathbf{u}_j\]</span></p>
<p>where</p>
<p><span class="math display">\[\hat\beta_j = \Pi(\mathbf{Y} \mid R(\mathbf{u}_j)) \quad \text{for each} \: i\]</span></p>
then <span class="math inline">\(\mathbf{\hat{Y}} - \mathbf{Y} \in Col(X)^{\perp}\)</span>.
</div>


<div class="theorem">
<p><span id="thm:orthonormalproj" class="theorem"><strong>Theorem 2.4  </strong></span>Under the hypothesis of Theorem <a href="multiple.html#thm:orthonormal">2.3</a>, <span class="math inline">\(\mathbf{\hat{Y}} \in Col(X)\)</span> is the closest to <span class="math inline">\(\mathbf{Y}\)</span> amongst its any element <span class="math inline">\(\mathbf{p}\)</span>, i.e.</p>
<p><span class="math display">\[\Vert \mathbf{p} - \mathbf{Y} \Vert &gt; \Vert \mathbf{\hat{Y}} - \mathbf{Y} \Vert\]</span></p>
for any <span class="math inline">\(\mathbf{p} \neq \mathbf{\hat{Y}}\)</span> in <span class="math inline">\(Col(X)\)</span>
</div>

<p>In other words, projection of <span class="math inline">\(\mathbf{Y}\)</span> onto <span class="math inline">\(Col(X)\)</span>, <span class="math inline">\(\mathbf{\hat{Y}}\)</span> can be <em>represented as sum of projections of</em> <span class="math inline">\(\mathbf{Y}\)</span> <em>onto each (orthogonal) individual variable</em>. Before looking at individual basis, consider two-block space.</p>
<p>Write</p>
<p><span class="math display">\[
X = \left[\begin{array}{c|ccc}
  1 &amp; x_{11} &amp; \cdots &amp; x_{1p} \\
  1 &amp; x_{21} &amp; \cdots &amp; x_{2p} \\
  \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
  1 &amp; x_{n1} &amp; \cdots &amp; x_{np}
\end{array}\right] = [\mathbf{1}, \mathbb{X}_A]
\]</span></p>
<p>Consider <span class="math inline">\(R(X)\)</span>, <span class="math inline">\(R(\mathbf{1})\)</span>, and <span class="math inline">\(R(\mathbb{X}_A)\)</span>.</p>
<p>To decompose subspace <span class="math inline">\(R(X)\)</span>, we try to orthogonalize <span class="math inline">\(\mathbf{1}\)</span> and <span class="math inline">\(\mathbb{X}_A\)</span> applying G-S process <a href="multiple.html#thm:gs">2.2</a>.</p>
<p><span class="math display">\[\mathbf{1} \perp \mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A\]</span></p>
<p>Theorem <a href="simple.html#thm:dsum">1.6</a> implies that</p>
<p><span class="math display">\[R(X) = R(\mathbf{1}) \oplus R(\mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A)\]</span></p>

<div class="theorem">
<p><span id="thm:orthdecomp" class="theorem"><strong>Theorem 2.5  (Orthogonal decomposition)  </strong></span>Let <span class="math inline">\(X = [\mathbf{1}, \mathbb{X}_A]\)</span>. Then</p>
<ol style="list-style-type: lower-roman">
<li></li>
</ol>
<p><span class="math display">\[R(X) = R(\mathbf{1}) \oplus R(\mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A)\]</span></p>
<ol start="2" style="list-style-type: lower-roman">
<li></li>
</ol>
<span class="math display">\[\Pi(\cdot \mid R(X)) = \Pi(\cdot \mid R(\mathbf{1})) + \Pi(\cdot \mid R(\mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A))\]</span>
</div>

<p>Write</p>
<p><span class="math display">\[\mathbb{X}_{A, \perp} := \mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A\]</span></p>
<p>Note that</p>
<p><span class="math display">\[\Pi_{\mathbf{1}} = \mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T = \frac{1}{n}\mathbf{1}\mathbf{1}^T\]</span></p>
<p>Then</p>
<p><span class="math display" id="eq:blockfit">\[\begin{equation}
  \begin{split}
    X\boldsymbol{\hat\beta} &amp; = \hat\beta_0\mathbf{1} + \mathbb{X}_A\boldsymbol{\hat\beta}_A \\
    &amp; = \hat\beta_0\mathbf{1} + (\mathbb{X}_{A, \perp} + \Pi_{\mathbf{1}}\mathbb{X}_A)\boldsymbol{\hat\beta}_A \\
    &amp; = \Big(\hat\beta_0 + \frac{1}{n}\mathbf{1}^T\mathbb{X}_A\boldsymbol{\hat\beta}_A \Big)\mathbf{1} + \mathbb{X}_{A,\perp}\boldsymbol{\hat\beta}_A \qquad \because \hat\beta_0 + \frac{1}{n}\mathbf{1}^T\mathbb{X}_A\boldsymbol{\hat\beta}_A \in \R
  \end{split}
  \tag{2.3}
\end{equation}\]</span></p>
<p>From (ii) of Theorem <a href="multiple.html#thm:orthdecomp">2.5</a>,</p>
<p><span class="math display" id="eq:decompfit">\[\begin{equation}
  \begin{split}
    \Pi(\mathbf{Y} \mid R(X)) &amp; = \Pi(\mathbf{Y} \mid R(\mathbf{1})) + \Pi(\mathbf{Y} \mid R(\mathbb{X}_{A,\perp})) \\
    &amp; = \overline{Y}\mathbf{1} + \mathbb{X}_{A,\perp}(\mathbb{X}_{A,\perp}^T\mathbb{X}_{A,\perp})^{-1}\mathbb{X}_{A,\perp}^T\mathbf{Y}
  \end{split}
  \tag{2.4}
\end{equation}\]</span></p>
<p>Equations <a href="multiple.html#eq:blockfit">(2.3)</a> and <a href="multiple.html#eq:decompfit">(2.4)</a> give that</p>
<p><span class="math display">\[
\begin{cases}
  \hat\beta_0 = \overline{Y} - \frac{1}{n}\mathbf{1}\mathbb{X}_A\boldsymbol{\hat\beta}_A \\
  \boldsymbol{\hat\beta}_A = (\mathbb{X}_{A,\perp}^T\mathbb{X}_{A,\perp})^{-1}\mathbb{X}_{A,\perp}^T\mathbf{Y}
\end{cases}
\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:illdecomp"></span>
<img src="images/multiple-orthogonal.png" alt="Orthogonal decomposition of the column space and LSE" width="70%" />
<p class="caption">
Figure 2.1: Orthogonal decomposition of the column space and LSE
</p>
</div>
<p>See Figure <a href="multiple.html#fig:illdecomp">2.1</a>. Two are orthogonal, so sum of projections onto them become LSE. In fact, <em>each projection indicate each regression coefficient</em>. When we do not have orthogonal basis, however, each projection is nothing.</p>
<div class="figure" style="text-align: center"><span id="fig:illdecomp2"></span>
<img src="images/multiple-nonorth.png" alt="Non-orthongality" width="70%" />
<p class="caption">
Figure 2.2: Non-orthongality
</p>
</div>
<p>In this situation, we have to do orthogonalization.</p>
<p><span class="math display">\[\tilde{\mathbb{X}}_A = \Pi_{\mathbf{1}}\mathbb{X}_A + (\mathbb{X}_A - \Pi_{\mathbf{1}}\mathbb{X}_A)\]</span></p>
</div>
<div id="gram-schmidt-qr-factorization" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Gram-Schmidt QR factorization</h3>

<div id="refs" class="references">
<div>
<p>Hogg, Robert V., Joseph W. McKean, and Allen Thornton Craig. 2018. <em>Introduction to Mathematical Statistics</em>. 8th ed. Pearson College Division.</p>
</div>
<div>
<p>Johnson, Richard Arnold, and Dean W. Wichern. 2013. <em>Applied Multivariate Statistical Analysis</em>.</p>
</div>
<div>
<p>Leon, Steve. 2014. <em>Linear Algebra with Applications</em>. Pearson Higher Ed.</p>
</div>
<div>
<p>Montgomery, Douglas C., Elizabeth A. Peck, and G. Geoffrey Vining. 2015. <em>Introduction to Linear Regression Analysis</em>. John Wiley &amp; Sons.</p>
</div>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Montgomery:2015aa">
<p>Montgomery, Douglas C., Elizabeth A. Peck, and G. Geoffrey Vining. 2015. <em>Introduction to Linear Regression Analysis</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-Leon:2014aa">
<p>Leon, Steve. 2014. <em>Linear Algebra with Applications</em>. Pearson Higher Ed.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["regression-analysis.pdf", "regression-analysis.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
