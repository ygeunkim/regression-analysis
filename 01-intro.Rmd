# Simple Linear Regression {#simple}

## Model

```{r}
delv <- MPV::p2.9 %>% tbl_df()
```

```{r delivery, fig.cap="The Delivery Time Data"}
delv %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```


Given data $(x_1, Y_1), \ldots, (x_n, Y_n)$, we try to fit linear model

$$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

Here $\epsilon_i$ is a error term, which is a random variable.

$$\epsilon \stackrel{iid}{\sim} (0, \sigma^2)$$

It gives the problem of estimating three parameters $(\beta_0, \beta_1, \sigma^2)$. Before estimating these, we set some assumptions.

1. linear relationship
2. $\epsilon_i$s are independent
3. $\epsilon_i$s are identically destributed, i.e. *constant variance*
4. In some setting, $\epsilon_i \sim N$

## Least Squares Estimation

```{r lsefig, echo=FALSE, fig.cap="Idea of the least square estimation"}
delv %>% 
  mutate(yhat = predict(lm(y ~ x))) %>% 
  ggplot(aes(x = x)) +
  geom_abline(intercept = 10, slope = 1.5, col = I("grey40"), alpha = .7) +
  geom_abline(intercept = 5, slope = 2, col = I("grey40"), alpha = .7) +
  geom_line(aes(y = yhat), col = gg_hcl(2)[2], size = 1, alpha = .7) +
  geom_point(aes(y = y)) +
  geom_linerange(aes(ymin = y, ymax = yhat), col = gg_hcl(2)[1]) +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```

We try to find $\beta_0$ and $\beta_1$ that minimize the sum of squares of the vertical distances, i.e.

\begin{equation}
  (\beta_0, \beta_1) = \arg\min \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2
  (\#eq:ssq)
\end{equation}

### Normal equations

Denote that Equation \@ref(eq:ssq) is quadratic. Then we can find its minimum by find the zero point of the first derivative. Set

$$Q(\beta_0, \beta_1) := \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2$$

Then we have

\begin{equation}
  \frac{\partial Q}{\partial \beta_0} = -2 \sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i) = 0
  (\#eq:normbeta0)
\end{equation}

and

\begin{equation}
  \frac{\partial Q}{\partial \beta_1} = -2 \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)x_i = 0
  (\#eq:normbeta1)
\end{equation}

From Equation \@ref(eq:normbeta0),

$$\sum_{i = 1}^n Y_i - n \hat\beta_0 - \hat\beta_1 \sum_{i = 1}^n x_i = 0$$

Thus,

$$\hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x}$$

Equation \@ref(eq:normbeta1) gives

$$\sum_{i = 1}^n x_i (Y_i - \overline{Y} + \hat\beta_1\overline{x} - \hat\beta_1 x_i) = \sum_{i = 1}^n x_i(Y_i - \overline{Y}) - \hat\beta_1\sum_{i = 1}^n x_i (x_i - \overline{x}) = 0$$

Thus,

$$\hat\beta_1 = \frac{\sum\limits_{i = 1}^nx_i(Y_i - \overline{Y})}{\sum\limits_{i = 1}^n x_i (x_i - \overline{x})}$$

```{remark}
$$\hat\beta_1 = \frac{S_{XY}}{S_{XX}}$$

where $S_{XX} := \sum\limits_{i = 1}^n (x_i - \overline{x})^2$ and $S_{XY} := \sum\limits_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y})$
```

```{proof}
Note that $\overline{x}^2 = \frac{1}{n^2}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2$. Then we have

\begin{equation}
  \begin{split}
    S_{XX} & = \sum_{i = 1}^n (x_i - \overline{x})^2 \\
    & = \sum_{i = 1}^n x_i^2 - 2\sum_{i = 1}^n x_i \overline{x} + \sum_{i = 1}^n\overline{x}^2 \\
    & = \sum_{i = 1}^n x_i^2 - \frac{2}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 + \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 \\
    & = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2
  \end{split}
  (\#eq:sxx)
\end{equation}

It follows that

\begin{equation*}
  \begin{split}
    \hat\beta_1 & = \frac{\sum x_i(Y_i - \overline{Y})}{\sum x_i (x_i - \overline{x})} \\
    & = \frac{\sum x_i (Y_i - \overline{Y}) - \overline{x}\sum (Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \qquad \because \sum (Y_i - \overline{Y}) = 0 \\
    & = \frac{\sum (x_i - \overline{x})(Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \\
    & = \frac{S_{XY}}{S_{XX}}
  \end{split}
\end{equation*}
```


```{r}
lm(y ~ x, data = delv)
```


### Prediction and Mean response


> "Essentially, all models are wrong, but some are useful."
>
> ---George Box


Recall that we have assumed the **linear assumption** between the predictor and the response variables, i.e. the true model. Estimating $\beta_0$ and $\beta_1$ is same as estimating the *assumed true model*.

```{definition, eyx, name = "Mean response"}
$$E(Y \mid X = x) = \beta_0 + \beta_1 x$$
```

We can estimate this mean resonse by

\begin{equation}
  \widehat{E(Y \mid x)} = \hat\beta_0 + \hat\beta_1 x
  (\#eq:meanres)
\end{equation}

However, in practice, the model might not be true, which is included in $\epsilon$ term.

$$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

Our real problem is predicting individual $Y$, not the mean. The *prediction* of response can be done by

\begin{equation}
  \hat{Y_i}  = \hat\beta_0 + \hat\beta_1 x_i
  (\#eq:indpred)
\end{equation}

Observe that the values of Equations \@ref(eq:meanres) and \@ref(eq:indpred) are same. However, due to the **error term in the prediction**, it has larger standard error.


### Properties of LSE {#lseprop}

Parameters $\beta_0$ and $\beta_1$ have some properties related to the expectation and variance. We can notice that these lse's are **unbiased linear estimator**. In fact, these are the *best unbiased linear estimator*. This will be covered in the Gauss-Markov theorem.


```{lemma, sxy}
$$S_{XX} = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 = \sum_{i = 1}^n x_i(x_i - \overline{x})$$

$$S_{XY} = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg) = \sum_{i = 1}^n Y_i(x_i - \overline{x})$$
```

```{proof}
We already proven the first part of $S_{XX}$. See the Equation \@ref(eq:sxx). The second part is tivial. Since $\sum (x_i - \overline{x}) = 0$,

$$S_{XX} = \sum_{i = 1}^n (x_i - \overline{x})^2 = \sum_{i = 1}^n (x_i - \overline{x})x_i$$

For the first part of $S_{XY}$,

\begin{equation*}
  \begin{split}
    S_{XY} & = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    & = \sum_{i = 1}^n x_i Y_i - \overline{x} \sum_{i = 1}^n Y_i - \overline{Y} \sum_{i = 1}^n x_i + n \overline{x} \overline{Y} \\
    & = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg)
  \end{split}
\end{equation*}

Second part of $S_{XY}$ also can be proven from the definition.

\begin{equation*}
  \begin{split}
    S_{XY} & = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    & = \sum_{i = 1}^n Y_i (x_i - \overline{x}) - \overline{Y} \sum_{i = 1}^n (x_i - \overline{x}) \\
    & = \sum_{i = 1}^n Y_i (x_i - \overline{x}) \qquad \because \sum_{i = 1}^n (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}
```

```{lemma, linbet, name = "Linearity"}
Each coefficient is a linear estimator.

$$\hat\beta_1 = \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i$$

$$\hat\beta_0 = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})}{S_{XX}} \bigg) Y_i$$
```

```{proof}
From lemma \@ref(lem:sxy),

\begin{equation*}
  \begin{split}
    \hat\beta_1 & = \frac{S_{XY}}{S_{XX}} \\
    & = \frac{1}{S_{XX}}\sum_{i = 1}^n (x_i - \overline{x}) Y_i
  \end{split}
\end{equation*}

It gives that

\begin{equation*}
  \begin{split}
    \hat\beta_0 & = \overline{Y} - \hat\beta_1 \overline{x} \\
    & = \frac{1}{n}\sum_{i = 1}^n Y_i - \overline{x} \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i \\
    & = \sum_{i = 1}^n\bigg(\frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)Y_i
  \end{split}
\end{equation*}
```

```{proposition, ue, name = "Unbiasedness"}
Both coefficients are unbiased.

$\text{(a)}\: E\hat\beta_1 = \beta_1$

$\text{(b)}\: E\hat\beta_0 = \beta_0$
```

From the model, $Y_1, \ldots, Y_n \stackrel{indep}{\sim} (\beta_0 + \beta_1 x_i, \sigma^2)$.

```{proof}
From lemma \@ref(lem:sxy),

\begin{equation*}
  \begin{split}
    E\hat\beta_1 & = \sum_{i = 1}^n \bigg[ \frac{(x_i - \overline{x})}{S_{XX}} E(Y_i) \bigg] \\
    & = \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}(\beta_0 + \beta_1 x_i) \\
    & = \frac{\beta_1 \sum (x_i - \overline{x})x_i}{\sum (x_i - \overline{x})x_i} \qquad \because \sum (x_i - \overline{x}) = 0 \\
    & = \beta_1
  \end{split}
\end{equation*}

It follows that

\begin{equation*}
  \begin{split}
    E\hat\beta_0 & = E(\overline{Y} - \hat\beta_1 \overline{x}) \\
    & = E(\overline{Y}) - \overline{x}E(\hat\beta_1) \\
    & = E(\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) - \beta_1 \overline{x} \\
    & = \beta_0 + \beta_1 \overline{x} - \beta_1 \overline{x} \\
    & = \beta_0
  \end{split}
\end{equation*}
```

```{proposition, vb, name = "Variances"}
Variances and covariance of coefficients

$\text{(a)}\: Var\hat\beta_1 = \frac{\sigma^2}{S_{XX}}$

$\text{(b)}\: Var\hat\beta_0 = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2$

$\text{(c)}\: Cov(\hat\beta_0, \hat\beta_1) = - \frac{\overline{x}}{S_{XX}} \sigma^2$
```

```{proof}
Proving is just arithmetic.

(a)

\begin{equation*}
  \begin{split}
    Var\hat\beta_1 & = \frac{1}{S_{XX}^2}\sum_{i = 1}^n \bigg[ (x_i - \overline{x})^2 Var(Y_i) \bigg] + \frac{1}{S_{XX}^2} \sum_{j \neq k}^n \bigg[ (x_j - \overline{x})(x_k - \overline{x}) Cov(Y_j, Y_k) \bigg] \\
    & = \frac{\sigma^2}{S_{XX}} \qquad \because Cov(Y_j, Y_k) = 0 \: \text{if} \: j \neq k
  \end{split}
\end{equation*}

(b)

\begin{equation*}
  \begin{split}
    Var\hat\beta_0 & = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)^2Var(Y_i) + \sum_{j \neq k} \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\bigg( \frac{1}{n} - \frac{(x_k - \overline{x})\overline{x}}{S_{XX}} \bigg) Cov(Y_j, Y_k) \\
    & = \frac{\sigma^2}{n} - 2 \sigma^2 \frac{\overline{x}}{S_{XX}} \sum_{i = 1}^n (x_i - \overline{x}) + \frac{\sigma^2 \overline{x}^2 \sum (x_i - \overline{x})^2}{S_{XX}^2} \\
    & = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \sigma^2 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}

(c)

\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, \hat\beta_1) & = Cov(\overline{Y} - \hat\beta_1 \overline{x}, \hat\beta_1) \\
    & = - \overline{x} Var\hat\beta_1 \\
    & = - \frac{\overline{x}}{S_{XX}} \sigma^2
  \end{split}
\end{equation*}
```


### Gauss-Markov Theorem

Chapter \@ref(lseprop) shows that the $\beta_0^{LSE}$ and $\beta_1^{LSE}$ are the **linear unbiased estimators**. Are these good? Good compared to _what estimators_? Here we consider *linear unbiased estimator*. If variances in the proposition \@ref(prp:vb) are lower than any parameters in this parameter family, $\beta_0^{LSE}$ and $\beta_1^{LSE}$ are the **best linear unbiased estimators**.

```{theorem, gmt, name = "Gauss Markov Theorem"}
$\hat\beta_0$ and $\hat\beta_1$ are BLUE, i.e. the best linear unbiased estimator.

$$Var(\hat\beta_0) \le Var\Big( \sum_{i = 1}^n a_i Y_i \Big) \: \forall a_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n a_i Y_i \Big) = \beta_0$$

$$Var(\hat\beta_1) \le Var\Big( \sum_{i = 1}^n b_i Y_i \Big) \: \forall b_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n b_i Y_i \Big) = \beta_1$$
```

```{proof, name = "Bestness of $\\beta_1$"}
Consider $\Theta := \bigg\{ \sum\limits_{i = 1}^n b_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n b_i Y_i \Big) = \beta_1 \bigg\}$.

Claim: $Var( \sum b_i Y_i) - Var(\hat\beta_1) \ge 0$

Let $\sum b_i Y_i \in \Theta$. Then $E(\sum b_i Y_i) = \beta_1$.

Since $E(Y_i) = \beta_0 + \beta_1 x_i$,

$$\beta_0 \sum b_i + \beta_1 \sum b_i x_i = \beta_1$$

It gives

\begin{equation} \label{eq:ule}
  \begin{cases}
    \sum b_i = 0 \\
    \sum b_i x_i = 1
  \end{cases}
\end{equation}

Then

\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum b_i Y_i - \hat\beta_1\Big) & = Var\Big( \sum b_i Y_i - \sum \frac{(x_i - \bar{x})}{S_{XX}} Y_i \Big) \\
    & \stackrel{indep}{=} \sum \bigg( b_i - \frac{(x_i - \bar{x})}{S_{XX}} \bigg)^2 \sigma^2 \\
    & = \sum \bigg( b_i^2 - \frac{2b_i (x_i - \bar{x})}{S_{XX}} + \frac{(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \sigma^2 \\
    & = \sum b_i^2 \sigma^2 - \frac{2 \sigma^2}{S_{XX}} \sum b_i x_i + \frac{2 \bar{x} \sigma^2}{S_{XX}} \sum b_i + \sigma^2 \frac{\sum (x_i - \bar{x})^2}{S_{XX}^2} \\
    & = \sum b_i^2 \sigma^2 - \frac{\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule} \:\text{and}\: S_{XX} = \sum (x_i - \bar{x})^2 \\
    & = Var(\sum b_i Y_i) - Var(\hat\beta_1)
  \end{split}
\end{equation*}

Hence,

$$Var(\sum b_i Y_i) \ge Var(\hat\beta_1)$$
```


```{proof, name = "Bestness of $\\beta_0$"}
Consider $\Theta := \bigg\{ \sum\limits_{i = 1}^n a_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n a_i Y_i \Big) = \beta_0 \bigg\}$.

Claim: $Var( \sum a_i Y_i) - Var(\hat\beta_0) \ge 0$

Let $\sum a_i Y_i \in \Theta$. Then $E(\sum a_i Y_i) = \beta_0$.

Since $E(Y_i) = \beta_0 + \beta_1 x_i$,

$$\beta_0 \sum a_i + \beta_1 \sum a_i x_i = \beta_0$$

It gives

\begin{equation} \label{eq:ule0}
  \begin{cases}
    \sum a_i = 1 \\
    \sum a_i x_i = 0
  \end{cases}
\end{equation}

Then

\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum a_iY_i - \hat\beta_0 \Big) & = Var\bigg[\sum a_iY_i - \sum\Big( \frac{1}{n} - \frac{(x_k - \bar{x})\bar{x}}{S_{XX}} \Big) Y_k \bigg] \\
    & = \sum \bigg(a_i - \frac{1}{n} +  \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \bigg)^2\sigma^2 \\
    & = \sum \bigg[ a_i^2 - 2a_i\Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big) + \Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big)^2 \bigg]\sigma^2 \\
		& = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n}\sum a_i + \frac{2\bar{x}\sigma^2\sum a_ix_i}{S_{XX}} - \frac{2\bar{x}^2\sigma^2\sum a_i}{S_{XX}} \\
		& \qquad + \sigma^2\bigg( \frac{1}{n} - \frac{2\bar{x}}{nS_{XX}} \sum(x_i - \bar{x}) + \frac{\bar{x}^2\sum(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \\
		& = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n} - \frac{2\bar{x}^2\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule0} \\
		& \qquad + \bigg(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \qquad \because \sum(x_i - \bar{x}) = 0 \: \text{and} \: S_{XX} := \sum (x_i - \bar{x})^2 \\
		& = \sum a_i^2\sigma^2 - \bigg( \frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \\
		& = Var\Big( \sum a_i Y_i \Big) - Var\hat\beta_0
  \end{split}
\end{equation*}

Hence,

$$Var(\sum a_i Y_i) \ge Var(\hat\beta_0)$$
```


```{example, usingnormal}
Show that $\sum (Y_i - \hat{Y_i}) = 0$, $\sum x_i (Y_i - \hat{Y_i}) = 0$, and $\sum \hat{Y_i} (Y_i - \hat{Y_i}) = 0$.
```

```{solution}
Consider the two normal equations \@ref(eq:normbeta0) and \@ref(eq:normbeta1). Note that $\hat{Y_i} = \hat\beta_0 + \hat\beta_1 x_i$.

From the Equation \@ref(eq:normbeta0), we have $\sum (Y_i - \hat{Y_i}) = 0$.

From the Equation \@ref(eq:normbeta1), we have $\sum x_i (Y_i - \hat{Y_i}) = 0$.

It follows that

\begin{equation*}
  \begin{split}
    \sum \hat{Y_i} (Y_i - \hat{Y_i}) & = \sum (\hat\beta_0 + \hat\beta_1 x_i) (Y_i - \hat{Y_i}) \\
    & = \hat\beta_0 \sum (Y_i - \hat{Y_i}) + \hat\beta_1 \sum x_i (Y_i - \hat{Y_i}) \\
    & = 0
  \end{split}
\end{equation*}
```


### Estimation of $\sigma^2$

There is the last parameter, $\sigma^2 = Var(Y_i)$. In the *least squares estimation literary*, we estimate $\sigma^2$ by

\begin{equation}
  \hat\sigma^2 = \frac{1}{n - 2} \sum_{i = 1}^n (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2
  (\#eq:siglse)
\end{equation}

Why $n - 2$? This makes the estimator unbiased.

```{proposition, sigex, name = "Unbiasedness"}
$$E(\hat\sigma^2) = \sigma^2$$
```

```{proof}
Note that

$$(Y_i - \hat\beta_0 - \hat\beta_1 x_i) = (Y_i - \overline{Y}) - \hat\beta_1(x_i - \overline{x})$$

Then

\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) & = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2 \bigg] \\
    & = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \overline{Y})^2 + \hat\beta_1^2 \sum (x_i - \overline{x})^2 -2\hat\beta_1 \sum (Y_i - \overline{Y})(x_i - \overline{x}) \bigg] \\
    & = \frac{1}{n - 2} E ( S_{YY} + \hat\beta_1^2 S_{XX} - 2 \hat\beta_1 S_{XY}) \\
    & = \frac{1}{n - 2} E ( S_{YY} - \hat\beta_1^2 S_{XX}) \qquad \because S_{XY} = \hat\beta_1 S_{XX} \\
    & = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{XX} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big)
  \end{split}
\end{equation*}

(a)

\begin{equation*}
  \begin{split}
    ES_{YY} & = E\Big[ \sum (Y_i - \overline{Y})^2 \Big] \\
    & = E \Big[ \sum \Big( (\beta_0 + \beta_1 x_i + \epsilon_i) - (\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) \Big)^2 \Big] \\
    & = E \Big[ \sum \Big( \beta_1 (x_i - \overline{x}) + (\epsilon_i - \overline{\epsilon}) \Big)^2 \Big] \\
    & = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) + 2\beta_1 \sum (x_i - \overline{x}) E(\epsilon_i - \overline{\epsilon}) \\
    & = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big)
  \end{split}
\end{equation*}

Since $E(\bar\epsilon) = 0$ and $Var(\bar\epsilon) = \frac{\sigma^2}{n}$,

\begin{equation*}
  \begin{split}
    E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) & = E \Big( \sum (\epsilon_i^2 + \bar\epsilon^2 - 2\epsilon_i \bar\epsilon) \Big) \\
    & = \sum E(\epsilon_i^2) - n E(\bar\epsilon^2) \qquad \because \sum \epsilon = n \bar\epsilon \\
    & = \sum (Var(\epsilon_i) + E(\epsilon_i)^2) - n(Var(\bar\epsilon) + E(\bar\epsilon)^2) \\
    & = n\sigma^2 - \sigma^2 \\
    & = (n - 1)\sigma^2
  \end{split}
\end{equation*}

Thus,

$$ES_{YY} = \beta_1^2 S_{XX} + (n - 1)\sigma^2$$

(b)

\begin{equation*}
  \begin{split}
    E\hat\beta_1^2 & = Var\hat\beta_1 + E(\hat\beta_1)^2 \\
    & = \frac{\sigma^2}{S_{XX}} + \beta_1^2
  \end{split}
\end{equation*}

It follows that

\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) & = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{YY} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big) \\
    & = \frac{1}{n - 2} \bigg( \Big(\beta_1^2 S_{XX} + (n - 1)\sigma^2 \Big) - S_{XX}\Big(\frac{\sigma^2}{S_{XX}} + \beta_1^2 \Big) \bigg) \\
    & = \frac{1}{n - 2}((n - 2)\sigma^2) \\
    & = \sigma^2
  \end{split}
\end{equation*}
```


## Maximum Likelihood Estimation

In this section, we add an assumption to an random errors $\epsilon_i$.

$$\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)$$

```{example, gmle, name = "Gaussian Likelihood"}
Note that $Y_i \stackrel{indep}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)$. Then the likelihood function is

$$L(\beta_0, \beta_1, \sigma^2) = \prod_{i = 1}^n\bigg( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \bigg(- \frac{(Y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2} \bigg) \bigg)$$

and so the log-likelihood function can be computed as

$$l(\beta_0, \beta_1, \sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2$$
```


### Likelihood equations

```{definition, mledef, name = "Maximum Likelihood Estimator"}
$$(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) := \arg\sup L(\beta_0, \beta_1, \sigma^2)$$
```

Since $l(\cdot) = \ln L(\cdot)$ is monotone,

```{remark, mlemax}
$$(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) = \arg\sup l(\beta_0, \beta_1, \sigma^2)$$
```

We can find the maximum of this *quadratic* function by making first derivative.

\begin{equation}
  \frac{\partial l}{\partial \beta_0} = \frac{1}{\sigma^2} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i) = 0
  (\#eq:mlbeta0)
\end{equation}

\begin{equation}
  \frac{\partial l}{\partial \beta_1} = \frac{1}{\sigma^2} \sum_{i = 1}^n x_i (Y_i - \beta_0 - \beta_1 x_i) = 0
  (\#eq:mlbeta1)
\end{equation}

\begin{equation}
  \frac{\partial l}{\partial \sigma^2} = - \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = 0
  (\#eq:mlsig)
\end{equation}

Denote that Equations \@ref(eq:mlbeta0) and \@ref(eq:mlbeta1) given $\hat\sigma^2$ are equivalent to the normal equations. Thus,

$$\hat\beta_0^{MLE} = \hat\beta_0^{LSE}, \quad \hat\beta_1^{MLE} = \hat\beta_1^{LSE}$$

From Equation \@ref(eq:mlsig),

$$\hat\sigma^{2MLE} = \frac{1}{n}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2 = \frac{n - 2}{n} \hat\sigma^{2LSE}$$

While $\hat\sigma^{2LSE}$ is an unbiased, above *MLE is not an unbiased estimator*. Since $\hat\sigma^{2MLE} \approx \hat\sigma^{2LSE}$ for large $n$, howerver, it is *asymptotically unbiased*.

```{theorem, rclb, name = "Rao-Cramer Lower Bound, univariate case"}
Let $X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta)$. If $\hat\theta$ is an unbiased estimator of $\theta$,

$$Var(\hat\theta) \ge \frac{1}{I_n(\theta)}$$

where $I_n(\theta) = -E\bigg(\frac{\partial^2 l(\theta)}{\partial \theta^2} \bigg)$
```

To apply this theorem \@ref(thm:rclb) in the simple linear regression setting, i.e. $(\beta_0, \beta_1)$, we need to look at the *bivariate case*.

```{theorem, rclb2, name = "Rao-Cramer Lower Bound, bivariate case"}
Let $X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta1, \theta_2)$ and let $\boldsymbol{\theta} = (\theta_1, \theta_2)^T$. If each $\hat\theta_1$, $\hat\theta_2$ is an unbiased estimator of $\theta_1$ and $\theta_2$, then

$$
Var(\boldsymbol{\theta}) := \begin{bmatrix}
Var(\hat\theta_1) & Cov(\hat\theta_1, \hat\theta_2) \\
Cov(\hat\theta_1, \hat\theta_2) & Var(\hat\theta_2)
\end{bmatrix} \ge I_n^{-1}(\theta_1, \theta_2)
$$

where

$$
I_n(\theta_1, \theta_2) = - \begin{bmatrix}
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1^2} \bigg) & E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) \\
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) & E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_2^2} \bigg)
\end{bmatrix}
$$
```

Assume that $\sigma^2$ is **known**. From the Equations \@ref(eq:mlbeta0) and \@ref(eq:mlbeta1),

$$
\begin{cases}
  \frac{\partial^2 l}{\partial \beta_0^2} = - \frac{n}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_1^2} = - \frac{\sum x_i^2}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_0 \partial \beta_1} = - \frac{\sum x_i}{\sigma^2}
\end{cases}
$$

Thus,

$$
I_n(\beta_0, \beta_1) = \begin{bmatrix}
  \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} \\
  \frac{\sum x_i}{\sigma^2} & \frac{\sum x_i^2}{\sigma^2}
\end{bmatrix}
$$

Applying gaussian elimination,

\begin{equation*}
  \begin{split}
    \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} & 1 & 0 \\
      \frac{\sum x_i}{\sigma^2} & \frac{\sum x_i^2}{\sigma^2} & 0 & 1
    \end{array}
    \right] & \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} & 1 & 0 \\
      \frac{\sum x_i}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) & \frac{\sum x_i^2}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) & 0 & \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    & \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} & 1 & 0 \\
      0 & \frac{\sum x_i^2 - \overline{x}\sum x_i}{\sigma^2\overline{x}} = \frac{S_{XX}}{\sigma^2\overline{x}} & -1 & \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    & \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 & \overline{x} & \frac{\sigma^2}{n} & 0 \\
      0 & 1 & -\frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right] \\
    & \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 & 0 & \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 & -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
      0 & 1 & -\frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right]
  \end{split}
\end{equation*}

Hence,

$$
I_n^{-1}(\beta_0, \beta_1) = \begin{bmatrix}
  \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 & -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
  -\frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
\end{bmatrix} = \begin{bmatrix}
  Var(\hat\beta_0) & Cov(\hat\beta_0, \hat\beta_1) \\
  Cov(\hat\beta_0, \hat\beta_1) & Var(\hat\beta_1)
\end{bmatrix}
$$

Since $Var(\boldsymbol{\hat\beta}) - I^{-1} = 0$ is non-negative definite, each $Var(\hat\beta_0) = \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2$ and $Var(\hat\beta_1) = \frac{\sigma^2}{S_{XX}}$ is a theoretical bound.

```{remark, lsemle}
This says that $\hat\beta_0^{LSE} = \hat\beta_0^{MLE}$ and $\hat\beta_1^{LSE} = \hat\beta_1^{MLE}$ have the smallest variance among all unbiased estimator.
```

This result is *stronger than Gauss-Markov theorem* \@ref(thm:gmt), where the LSE has the smalleset variance among all *linear unbiased* estimators. It can be simply obtained from the *Lehmann-Scheffe Theorem*: If some unbiased estimator is a function of complete sufficient statistic, then this estimator is the unique MVUE [@Hogg:2018aa].

```{remark, regcss, name = "Lehmann and Scheffe for regression coefficients"}
$u\Big(\sum Y_i, S_{XY} \Big)$ is CSS in this regression problem, i.e. known $\sigma^2$.
```

```{proof}
From the example \@ref(exm:gmle),

\begin{equation*}
  \begin{split}
    L(\beta_0, \beta_1) & = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum(Y_i - \beta_0 - \beta_1 x_i)^2 \bigg] \\
    & = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum \Big(Y_i^2 - (\beta_0 + \beta_1 x_i)Y_i + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg] \\
    & = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \Big( -\beta_0 \sum Y_i - \beta_1 \sum x_i Y_i \Big) \bigg] \exp\bigg[-\frac{1}{2\sigma^2} \Big( \sum Y_i^2 + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg]
  \end{split}
\end{equation*}

By the Factorization theorem, both $\sum Y_i$ and $\sum x_i Y_i$ are sufficient statistics. Since $S_{XY}$ is one-to-one function of $\sum x_i Y_i$, it is also a sufficient statistic.

Denote that the normal distribution is in exponential family.

Hence, $(\sum Y_i, S_{XY})$ are CSS.
```


## Residuals

```{definition, res, name = "Residuals"}
$$e_i := Y_i - \hat{Y_i}$$
```

### Prediction error

```{r regplot, fig.cap="Fit and residuals"}
delv %>% 
  mutate(yhat = predict(lm(y ~ x))) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point() +
  geom_linerange(aes(ymin = y, ymax = yhat), col = I("red"), alpha = .7) +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```

See Figure \@ref(fig:regplot). Each red line is $e_i$. As we can see, $e_i$ represents the difference between *observed* response and *predicted* response. A large $\lvert e_i \rvert$ indicates a large prediction error. You can call this $e_i$ for each $Y_i$ by `lm()$residuals` or `residuals()`.

```{r}
delv_fit <- lm(y ~ x, data = delv)
delv_fit$residuals
```

$\sum e_i^2$, which has been minimized in the procedure of LSE, can be used to see *overall size of prediction errors*.

```{definition, sse, name = "Residual Sum of Squares"}
$$SSE := \sum_{i = 1}^n e_i^2$$
```

### Residuals and the variance

$e_i$ is a random quantity, which contains the information for $\epsilon_i$. $\sum e_i^2$ can give information about $\sigma^2 = Var(\epsilon_i)$. For this, it is expected that $e_i$ and $\epsilon_i$ have similar feature.


```{lemma, yandbet}
Covriance between Y and each coefficient

$\text{(a)}\: Cov(\hat\beta_0, Y_i) = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2$

$\text{(b)}\: Cov(\hat\beta_1, Y_i) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2$
```

```{proof}
(a)

\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, Y_i) & = Cov(\sum a_i Y_i, Y_i) \\
    & = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2
  \end{split}
\end{equation*}

(b)

\begin{equation*}
  \begin{split}
    Cov(\hat\beta_1, Y_i) & = Cov(\sum b_i Y_i, Y_i) \\
    & = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2
  \end{split}
\end{equation*}
```


```{proposition, resprop, name = "Properties of residuals"}
Mean and variance of the residual

$\text{(a)}\: E(e_i) = 0$

$\text{(b)}\: Var(e_i) \neq \sigma^2$

$\text{(c)}\: \forall i \neq j : Cov(e_i, e_j) \neq 0$
```

```{proof}
(a) Recall that this is the assumption of the regression model.

(b) Lemma \@ref(lem:yandbet) implies that

\begin{equation*}
  \begin{split}
    Cov(\overline{Y}, \hat\beta_1) & = Cov(\frac{1}{n}\sum Y_i, \hat\beta_1) \\
    & = \frac{1}{n} \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    & = 0 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}

Then

\begin{equation}
  \begin{split}
    Var(\hat{Y_i}) & = Var(\hat\beta_0 + \hat\beta_1 x_i) \\
    & = Var \bigg[ \overline{Y} + (x_i - \overline{x}) \hat\beta_1 \bigg] \qquad \because \hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x} \\
    & = Var(\overline{Y}) + (x_i - \overline{x})^2 Var(\hat\beta_1) + 2(x_i - \overline{x}) Cov(\overline{Y}, \hat\beta_1) \\
    & = \frac{\sigma^2}{n} + (x_i - \overline{x})^2\frac{\sigma^2}{S_{XX}} + 0 \\
    & = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
  (\#eq:predvar)
\end{equation}

From the same lemma \@ref(lem:yandbet),
    
\begin{equation}
  \begin{split}
    Cov(Y_i, \hat{Y_i}) & = Cov(Y_i, \overline{Y} + (x_i - \overline{x}) \hat\beta_1) \\
    & = Cov(Y_i, \overline{Y}) + (x_i - \overline{x}) Cov(Y_i, \hat\beta_1) \\
    & = \frac{\sigma^2}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}}\sigma^2 \qquad \because Cov(Y_i, \hat\beta_1) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    & = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
  (\#eq:yyhat)
\end{equation}

These Equations \@ref(eq:predvar) and \@ref(eq:yyhat) give that

\begin{equation}
  \begin{split}
    Var(e_i) & = Var(Y_i) + Var(\hat{Y_i}) -2Cov(Y_i, \hat{Y_i}) \\
    & = \sigma^2 + \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 - 2 \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    & = \bigg(1 - \frac{1}{n} - \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    & \neq \sigma^2
  \end{split}
  (\#eq:residvar)
\end{equation}

(c) Let $i \neq j$. Then

\begin{equation*}
  \begin{split}
    Cov(e_i, e_j) & = Cov\Big( Y_i - (\hat\beta_0 + \hat\beta_1 x_i), Y_j - (\hat\beta_0 + \hat\beta_1 x_j) \Big) \\
    & = Cov(Y_i, Y_j) - Cov\Big(Y_i, (\hat\beta_0 + \hat\beta_1 x_j) \Big) - Cov((\hat\beta_0 + \hat\beta_1 x_i), Y_j) + Cov((\hat\beta_0 + \hat\beta_1 x_i), (\hat\beta_0 + \hat\beta_1 x_j)) \\
    & = 0 - \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_j}{S_{XX}}\sigma^2 \\
    & \qquad - \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_i}{S_{XX}}\sigma^2 \\
    & \qquad + \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    & = - \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    & = - \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})(x_j - \overline{x})}{S_{XX}} \bigg)\sigma^2 \\
    & \neq 0
  \end{split}
\end{equation*}
```


## Decomposition of Total Variability

### Total sum of squares

```{definition, unsst, name = "Uncorrected Total Sum of Squares"}
$$SST_{uncor} := \sum_{i = 1}^n Y_i^2$$
```

```{definition, sst, name = "Corrected Total Sum of Squares"}
$$SST := \sum_{i = 1}^n (Y_i - \overline{Y})^2$$
```

What does this total sum of squares mean? To know this, we should know $\overline{Y}$ first.

```{r ybarpred, fig.cap="Regression without predictor"}
delv %>% 
  ggplot(aes(x = x, y = y)) +
  geom_smooth(method = "lm", formula = y ~ 1, se = FALSE) +
  geom_point() +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```

See Figure \@ref(fig:ybarpred). The line represents the closest line when we use only intercept term for the regression model. In other words, *if we use no information for the response*, i.e. no predictor variables, we will get just average of the response variable. Consider

$$Y_i = \beta_0 + \epsilon_i$$

Then we can get only one normal equation

$$\sum (Y_i - \hat\beta_0) = 0$$

Hence,

$$\hat\beta_0 = \frac{1}{n} \sum_{i = 1}^n Y_i \equiv \overline{Y}$$

From this fact, $SST$ implies **total variance**.

### Regression sum of squares

```{definition, ssr, name = "Regression Sum of Squares"}
$$SSR := \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2$$
```

This $SSR$ compares $\hat{Y_i}$ versus $\overline{Y}$, computing the sum of squares for difference between predicted values from *regression model* and *model not using predictors*.

### Residual sum of squares

Now consider the *residual sum of squares* $SSE$ in the definition \@ref(def:sse). As mentioned, this is related to the *prediction errors*, which the regression model could not explain the data.

### Decomposition of total sum of squares {#decompsst}

$SST$ can be decomposed by construction of sum of squares.

```{proposition, decom, name = "Decomposition of SST"}
$$SST = SSR + SSE$$

where $SST = \sum (Y_i - \overline{Y})^2$, $SSR = \sum (\hat{Y_i} - \overline{Y})^2$, and $SSE = \sum (Y_i - \hat{Y_i})$
```

```{proof}
From the Example \@ref(exm:usingnormal),

\begin{equation*}
  \begin{split}
    \sum_{i = 1}^n (Y_i - \overline{Y})^2 & = \sum_{i = 1}^n (Y_i - \hat{Y_i} + \hat{Y_i} - \overline{Y})^2 \\
    & = \sum_{i = 1}^n (Y_i - \hat{Y_i})^2 + 2 \sum_{i = 1}^n (Y_i - \hat{Y_i})(\hat{Y_i} - \overline{Y}) + \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2 \\
    & = \sum_{i = 1}^n (Y_i - \hat{Y_i})^2 + \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2 \qquad \because \sum (Y_i - \hat{Y_i}) = 0 \: \text{and} \: \sum (Y_i - \hat{Y_i})\hat{Y_i} = 0
  \end{split}
\end{equation*}
```

This represents each $SSR$ and $SSE$ divides total variability as following.

$$\overset{SST}{\text{total variability}} = \overset{SSR}{\text{explained by regression}} + \overset{SSE}{\text{left unexplained by regression}}$$

Denote that the total variability $SST$ is *constant given data set*. If our model is good, $SSR$ grows and $SSE$ flattens. Thus the larger $SSR$ is, the better. The lower $SSE$ is, the better.

### Coefficient of determination

We have discussed in the previous section \@ref(decompsst) that $SSR$ and $SSE$ splits the total variability into *explained part and not-explained part by our regression model*. Our first interest is whether the model works well for the data well, so we can think about the *proportion of explained part to the total variance*. The following measure $R^2$ computes this kind of value.

```{definition, rsq, name = "Coefficient of Determination"}
$$R^2 := \frac{SSR}{SST} = 1 - \frac{1 - SSE}{SST}$$
```

By construction,

$$0 \le R^2 \le 1$$

As $R^2$ goes to $0$, the model goes wrong. As $R^2$ is close to $1$, large proportion of variability has been explained. So we prefer large values rather than small.

```{proposition, rsqlin}
$R^2$ shows the strength of linear relation between two variables $x$ and $Y$ in the simple linear regression.

$$R^2 = \hat\rho_{XY}$$

where $\hat\rho_{XY} := \frac{\sum (X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum (X_i - \overline{X})^2} \sqrt{\sum (Y_i - \overline{Y})^2}}$ is the sample correlation coefficients
```

```{proof}
Note that $\hat{Y_i} - \overline{Y} = \hat\beta_1 (x_i - \overline{x}) = \frac{S_{XY}}{S_{XX}} (x_i - \overline{x})$. Then

\begin{equation*}
  \begin{split}
    \sum (\hat{Y_i} - \overline{Y})^2 & = \frac{S_{XY}^2}{S_{XX}^2} \sum (x_i - \overline{x})^2 \\
    & = \frac{S_{XY}^2}{S_{XX}}
  \end{split}
\end{equation*}

It follows that

\begin{equation*}
  \begin{split}
    R^2 & = \frac{\sum (\hat{Y_i} - \overline{Y})^2}{\sum (Y_i - \overline{Y})^2} \\
    & = \frac{S_{XY}^2}{S_{XX}S_{YY}} \\
    & =: \hat\rho_{XY}^2
  \end{split}
\end{equation*}
```

In this relation, we can know that $R^2$ statistic performs as a measure of the linear relationship in the simple linear regression setting.


## Geometric Interpretations {#matnot}

### Fundamental subspaces

These linear algebra concepts might be more useful for *multiple linear regression*, but let's briefly recap [@Leon:2014aa].

```{definition, subspace, name = "Fundamental Subspaces"}
Let $X \in \mathbb{R}^{n \times (p + 1)}$.

Then the Null space is defined by

$$N(X) := \{ \mathbf{b} \in \mathbb{R}^n \mid X\mathbf{b} = \mathbf{0} \}$$

The Row space is defined by

$$Row(X) := sp(\{\mathbf{r}_1, \ldots, \mathbf{r}_{p + 1} \}) \quad \text{where} \: X^T = [\mathbf{r}_1^T, \ldots, \mathbf{r}_{n}^T]$$

The Column space is defined by

$$Col(X) := sp(\{\mathbf{c}_1, \ldots, \mathbf{c}_{n} \}) \quad \text{where} \: X = [\mathbf{c}_1, \ldots, \mathbf{c}_{p + 1}]$$

The Range of $X$ is defined by

$$R(X) := \{ \mathbf{y} \in \mathbb{R}^n \mid \mathbf{y} = X\mathbf{b} \quad \text{for some} \: \mathbf{b} \in \mathbb{R}^{p + 1} \}$$
```

These spaces have some constructional relationship.

```{theorem, fundsub, name = "Fundamental Subspaces Theorem"}
Let $X \in \mathbb{R}^{n \times (p + 1)}$. Then

$$N(X) = R(X^T)^{\perp} = Col(X^T)^{\perp} = Row(X)^{\perp}$$

Transposed matrix also satisfy this.

$$N(X^T) = R(X)^{\perp} = Col(X)^{\perp}$$
```

```{proof}
Let $\mathbf{a} \in N(X)$. Then $X\mathbf{a} = \mathbf{0}$.

Let $\mathbf{y} \in R(X^T)$. Then $X^T \mathbf{b} = \mathbf{y}$ for some $\mathbf{b} \in \mathbb{R}^{p + 1}$.

Choose $\mathbf{b} \in \mathbb{R}^{p + 1}$ such that $X^T \mathbf{b} = \mathbf{y}$. Then

\begin{equation*}
  \begin{split}
    \mathbf{0} & = X\mathbf{a} \\
    & = \mathbf{b}^T X\mathbf{a} \\
    & = \mathbf{y}^T \mathbf{a}
  \end{split}
\end{equation*}

Hence,

$$N(X) \perp R(X^T)$$

Since

$$X^T \mathbf{b} = \mathbf{c}_1 \mathbf{b} + \cdots + \mathbf{c}_{p + 1} \mathbf{b}$$

it is trivial that $R(X) = Col(X)$ and $R(X^T) = Col(X^T)$.

If $\mathbf{a} \in N(X)$, then

$$
X\mathbf{a} = \begin{bmatrix}
  \mathbf{r}_1 \\
  \mathbf{r}_2 \\
  \cdots \\
  \mathbf{r}_n
\end{bmatrix} \begin{bmatrix}
  a_1 \\
  \cdots \\
  a_{p + 1}
\end{bmatrix} = \begin{bmatrix}
  0 \\
  0 \\
  \cdots \\
  0
\end{bmatrix}
$$

Thus,

$$\forall i :  \mathbf{a}^T \mathbf{r}_i = 0$$

and so

$$N(X) \subseteq Row(X)^{\perp}$$

Conversely, if $\mathbf{a} \in Row(X)^{\perp}$, then $\forall i :  \mathbf{a}^T \mathbf{r}_i = 0$. This implies that $X\mathbf{a} = \mathbf{0}$. Thus,

$$Row(X)^{\perp} \subseteq N(X)$$

and so

$$N(X) = Row(X)^{\perp}$$
```

$N(X^T) = R(X)^{\perp}$ part in Theorem \@ref(thm:fundsub) will give the geometric insight to *least squares solution*.

```{theorem, perpbasis}
Let $S$ be a subspace of $\mathbb{R}^n$. Then

$$dim S + dim S^{\perp} = n$$

If $\{ \mathbf{x}_1, \ldots, \mathbf{x}_r \}$ is a basis for $S$ and $\{ \mathbf{x}_{r + 1}, \ldots, \mathbf{x}_n \}$ is a basis for $S^{\perp}$, then $\{ \mathbf{x}_1, \ldots, \mathbf{x}_r, \mathbf{x}_{r + 1}, \ldots, \mathbf{x}_n \}$ is a basis for $\mathbb{R}^n$.
```

```{theorem, dsum}
Let $S$ be a subspace of $\R^n$. Then

$$\R^n = S \oplus S^{\perp}$$
```


### Simple linear regression

```{theorem, projection}
Let $S$ be a subspace of $\mathbb{R}^n$. For each $\mathbf{y} \in \mathbf{R}^n$, there exists a unique $\mathbf{p} \in S$ that is closest to $\mathbf{y}$, i.e.

$$\Vert \mathbf{y} - \mathbf{p}  \Vert > \Vert \mathbf{y} - \mathbf{\hat{y}} \Vert$$

for any $\mathbf{p} \neq \mathbf{\hat{y}}$. Furthermore, a given vector $\mathbf{p} \in S$ will be the closest to a given vector $\mathbf{y} \in \mathbb{R}^n$ if and only if

$$\mathbf{y} - \mathbf{\hat{y}} \in S^{\perp}$$
```

Least square estimator $(\hat\beta_0, \hat\beta_1)^T$ minimizes

\begin{equation}
  \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = \Vert \mathbf{Y} - (\beta_0 \mathbf{1} + \beta_1 \mathbf{x}) \Vert^2
  (\#eq:qmatrix)
\end{equation}

with respect to $(\hat\beta_0, \hat\beta_1)^T \in \mathbb{R}^2$ (where $\mathbf{1} := (1, 1)^T$). Recall that the normal equation gives

$$\sum_{i = 1}^n(Y_i - \hat\beta_0 - \hat\beta_1 x_i) = \Big( \mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \Big)^T \mathbf{1} = 0$$

and

$$\sum_{i = 1}^n (Y_i - \hat\beta_0 - \hat\beta_1 x_i)x_i = \Big( \mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \Big)^T \mathbf{x} = 0$$

These two relation give

$$\mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \perp sp(\{ \mathbf{1}, \mathbf{x} \})^{\perp}$$

i.e. $\mathbf{\hat{Y}} = \hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}$ is the projection of $\mathbf{Y}$.

Theorem \@ref(thm:projection) can give the same result.

$$\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x} \in R([\mathbf{1}, \mathbf{x}])^{\perp} = sp(\{ \mathbf{1}, \mathbf{x} \})^{\perp}$$


```{r simpledraw, echo=FALSE, fig.cap="Geometric Illustration of Simple Linear Regression"}
tibble(
  z1 = seq(0, 5, length.out = 100),
  z2 = seq(0, 3, length.out = 100),
  y = seq(0, 2, length.out = 100)
) %>% 
  mutate(z2y = z2, yy = 5/2*y) %>%
  mutate(yhat = 1/2*y) %>%
  ggplot() +
  geom_line(aes(z1, 0), size = 1, arrow = arrow()) +
  geom_line(aes(z2, z2y), size = .3, arrow = arrow()) +
  geom_line(aes(y, yy), arrow = arrow()) +
  geom_line(aes(y, yhat), size = .5, arrow = arrow(), linetype = "dashed", col = I("red")) +
  geom_segment(aes(x = 2, y = 1, xend = 2, yend = 5), size = .4, linetype = "dashed", col = I("red")) +
  # geom_segment(aes(x = 1.5, y = 1.5, xend = 2, yend = 5), size = .2, linetype = "dashed", col = I("blue")) +
  # geom_segment(aes(x = 1.8, y = 0, xend = 2, yend = 5), size = .5, linetype = "dashed", col = I("blue")) +
  # geom_segment(aes(x = 1.5, y = 1.5, xend = 2, yend = 1), size = .25, linetype = "dashed", col = I("blue")) +
  # geom_segment(aes(x = 1.8, y = 0, xend = 2, yend = 1), size = .45, linetype = "dashed", col = I("blue")) +
  annotate("text", x = 5, y = .2, label = "bold(1)", parse = T) +
  annotate("text", x = 3, y = 3.2, label = "bold(x)[1]", parse = T) +
  annotate("text", x = 1.8, y = 5, label = "bold(y)", parse = T) +
  annotate("text", x = 2.7, y = 1, label = "hat(bold(y))^LSE == bar(italic(y))*bold(1) + hat(beta)[0]*bold(1) + hat(beta)[1]*bold(x)[1]", parse = T, col = I("red")) +
  # annotate("text", x = 1.4, y = 1.6, label = "hat(beta)[2]", parse = T, col = I("blue")) +
  # annotate("text", x = 2, y = .2, label = "hat(beta)[1]", parse = T, col = I("blue")) +
  annotate("text", x = 0, y = .3, label = "bar(italic(y))*bold(1)", parse = T) +
  annotate("text", x = 3.5, y = 2, label = "Col(X)", size = 5) +
  theme(axis.line = element_blank(), axis.text = element_blank(), axis.title = element_blank())
```

We can see the details from Figure \@ref(fig:simpledraw). In fact, decomposition of $SST$ and $R^2$ are also in here.

```{r simpledraw2, echo=FALSE, fig.cap="Geometric Illustration of Decomposing SST"}
tibble(
  z1 = seq(0, 5, length.out = 100),
  z2 = seq(0, 3, length.out = 100),
  y = seq(0, 2, length.out = 100)
) %>% 
  mutate(z2y = z2, yy = 5/2*y) %>%
  mutate(yhat = 1/2*y) %>%
  ggplot() +
  geom_line(aes(z1, 0), size = 1, arrow = arrow()) +
  geom_line(aes(z2, z2y), size = .3, arrow = arrow()) +
  geom_line(aes(y, yy), arrow = arrow()) +
  geom_line(aes(y, yhat), size = .5, arrow = arrow(), linetype = "dashed", col = I("red")) +
  geom_segment(aes(x = 2, y = 1, xend = 2, yend = 5), size = .4, linetype = "dashed", col = I("red")) +
  annotate("text", x = 5, y = .2, label = "bold(1)", parse = T) +
  annotate("text", x = 3, y = 3.2, label = "bold(x)[1]", parse = T) +
  annotate("text", x = 1.8, y = 5, label = "bold(y)", parse = T) +
  annotate("text", x = 2.7, y = 1, label = "hat(bold(y))^LSE == bar(italic(y))*bold(1) + hat(beta)[0]*bold(1) + hat(beta)[1]*bold(x)[1]", parse = T, col = I("red")) +
  annotate("text", x = 0, y = .3, label = "bar(italic(y))*bold(1)", parse = T) +
  annotate("text", x = 3.5, y = 2, label = "Col(X)", size = 5) +
  annotate("text", x = 1, y = 3, label = "sqrt(SST)", size = 5, parse = T) +
  annotate("text", x = 1.5, y = 1, label = "sqrt(SSR)", size = 5, parse = T, col = I("red")) +
  annotate("text", x = 2, y = 3, label = "sqrt(SSE)", size = 5, parse = T, col = I("red")) +
  theme(axis.line = element_blank(), axis.text = element_blank(), axis.title = element_blank())
```

See Figure \@ref(fig:simpledraw2).

$$
\begin{cases}
  SST = \lVert \mathbf{Y} - \overline{Y} \mathbf{1} \rVert^2 \\
  SSR = \lVert \mathbf{\hat{Y}} - \overline{Y} \mathbf{1} \rVert^2 \\
  SSE = \lVert \mathbf{Y} - \mathbf{\hat{Y}} \rVert^2
\end{cases}
$$

Pythagorean law implies that

$$SST = SSR + SSE$$

Also,

$$R^2 = \frac{SSR}{SST} = cos^2\theta = \hat\rho_{XY}^2$$

### Projection mapping {#solproj}

Look again Figure \@ref(fig:simpledraw). Let $X \equiv [\mathbf{1}, \mathbf{x}] \in \R^{n \times 2}$ and let $\boldsymbol\beta \equiv (\beta_0, \beta_1)^T$. By the fundamental subspaces theorem \@ref(thm:fundsub),

$$\mathbf{Y} - X\boldsymbol{\hat\beta} \in Col(X)^{\perp} = N(X^T)$$

Thus,

\begin{equation}
  X^T(\mathbf{Y} - X\boldsymbol{\hat\beta}) = \mathbf{0}
  (\#eq:projeq)
\end{equation}

This is the another representation of normal equation. Then we now have

\begin{equation*}
  \begin{split}
    & X^T\mathbf{Y} - X^TX\boldsymbol{\hat\beta} = \mathbf{0} \\
    & \Leftrightarrow X^T\mathbf{Y} = X^TX\boldsymbol{\hat\beta}
  \end{split}
\end{equation*}

If $X^TX$ is nonsingular,

$$\boldsymbol{\hat\beta} = (X^TX)^{-1}X^T \mathbf{Y}$$

It follows that

$$\mathbf{\hat{Y}} = X\boldsymbol{\hat\beta} = X(X^TX)^{-1}X^T \mathbf{Y}$$

Combining this equation and our figure, we can know that $X(X^TX)^{-1}X^T$ projects $\mathbf{Y}$ from $\R^n$ onto $Col(X) = R(X)$. This is called projection operator/mapping.


```{definition, projop, name = "Projection matrix"}
Projection operator or mapping from $\R^n$ to $W$ is written by

$$\Pi(\cdot \mid W) := X(X^TX)^{-1}X^T$$
```

As mentioned, $X^TX$ should be invertible to get the LSE solution.

```{theorem, fullrank}
Let $\mathbf{Y} = X\boldsymbol\beta$ inconsistent and let $X \in \R^{n \times (p + 1)}$ with $n > p + 1$.

If $rank(X) = p + 1$, i.e. full rank, then $X^T X$ is invertible.
```

```{proof}
Suppose that $(X^TX)\mathbf{b} = \mathbf{0}$. Then

$$X^T (X\mathbf{b}) = \mathbf{0}$$

By the fundamental subspaces theorem \@ref(thm:fundsub),

$$X\mathbf{b} \in N(X^T) = Col(X)^{\perp}$$

By construction,

$$X\mathbf{b} \in Col(X) = N(X^T)^{\perp}$$

Then

$$X\mathbf{b} \in N(X^T) \cap N(X^T)^{\perp} = \{ \mathbf{0} \}$$

It follows that

$$X\mathbf{b} = \mathbf{0}$$

If $rank(X) = n$, then the linear equation system has trivial solution $\mathbf{b} = \mathbf{0}$ and so does $X^T (X\mathbf{b}) = \mathbf{0}$. Hence, $X^T X$ is invertible.
```

Using projection matrix $\Pi_W$, we can re-express each sum of squares. Recall that when we only use $y_i$ for regression fitting, the result becomes its average. It is because $\mathbf{Y}$ vector has been projected onto $sp(\{ \mathbf{1} \})$ line.

```{remark}
$$\overline{Y}\mathbf{1} = \mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T\mathbf{Y} = \Pi_{\mathbf{1}}\mathbf{Y}$$

$$\mathbf{\hat{Y}} = X(X^TX)^{-1}X^T \mathbf{Y} = \Pi_X \mathbf{Y}$$
```

Intuitively, every projection matrix is idempotent and symmetric. Once projected, the result is same when projecting it again.

```{corollary, projss, name = "Sum of squares"}
$\Pi_{\mathbf{1}}$ and $\Pi_X$ can express each $SS$ as following.

(i)

$$SST = \mathbf{Y}^T (I - \Pi_{\mathbf{1}}) \mathbf{Y}$$

(ii)

$$SSR = \mathbf{Y}^T (\Pi_X - \Pi_{\mathbf{1}}) \mathbf{Y}$$

(iii)

$$SSE = \mathbf{Y}^T (I - \Pi_X) \mathbf{Y}$$
```


## Distributions {#simpledist}

### Mean response and response

We have already look at predicting each mean response and response from equation \@ref(eq:meanres) and \@ref(eq:indpred).

```{theorem, mux, name = "Estimation of the mean response"}
$$\hat\mu_x \equiv \widehat{E(Y \mid x)} = \hat\beta_0 + \hat\beta_1 x$$
```


```{theorem, yhatx, name = "(out of sample) Prediction of a response"}
$$\hat{Y_x}  = \hat\beta_0 + \hat\beta_1 x$$
```

Recall that predicting \@ref(thm:mux) targets at

$$\mu_x \equiv E(Y \mid x) = \beta_0 + \beta_1 x$$

which have been assumed to be true model. On the other hand, predicting \@ref(thm:yhatx) targets at

$$Y = \beta_0 + \beta_1 + \epsilon_x$$

The linearity is not true in reality. So the errors caused by modeling linear model are included in $\epsilon_x$. This error term makes difference between properties of \@ref(thm:mux) and \@ref(thm:yhatx).

To derive their distribution and see the difference, we additionaly assume Normality, i.e.

$$\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)$$

### Regression coefficients {#simplebdist}

Under Normality, we have

$$Y_i \stackrel{indep}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)$$

Then

$$
\mathbf{Y} = \begin{bmatrix}
  Y_1 \\
  Y_2 \\
  \vdots \\
  Y_n
\end{bmatrix} \sim MVN_n\Bigg( \boldsymbol\mu \equiv \begin{bmatrix}
  \beta_0 + \beta_1 x_1 \\
  \beta_0 + \beta_1 x_2 \\
  \vdots \\
  \beta_0 + \beta_1 x_n
\end{bmatrix}, \Sigma \equiv \sigma^2 I = \begin{bmatrix}
  \sigma^2 & 0 & \cdots & 0 \\
  0 & \sigma^2 & \cdots & 0 \\
  \vdots & \vdots & \vdots & \vdots \\
  0 & 0 & 0 & \sigma^2
\end{bmatrix} \Bigg)
$$

Write $\boldsymbol{\hat\beta} = (\hat\beta_0, \hat\beta_1)^T$. From Lemma \@ref(lem:linbet),

$$\hat\beta_0 = \mathbf{a}^T\mathbf{Y}$$

where $\mathbf{a} = (a_1, \ldots, a_n)^T \in \mathbb{R}^n$ with $a_i = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)$

and

$$\hat\beta_1 = \mathbf{b}^T\mathbf{Y}$$

where $\mathbf{b} = (b_1, \ldots, b_n)^T \in \mathbb{R}^n$ with $b_i = \frac{(x_i - \overline{x})}{S_{XX}}$.

Let

$$A^T = [ \mathbf{a}^T, \mathbf{b}^T ]$$

Then

$$
\boldsymbol{\hat\beta} = A\mathbf{Y}
$$

Linearity of the multivariate normal distribution, Proposition \@ref(prp:ue) and \@ref(prp:vb) imply that

\begin{equation}
  \boldsymbol{\hat\beta} = \begin{bmatrix}
    \hat\beta_0 \\ \hline
    \hat\beta_1
  \end{bmatrix} \sim MVN \bigg( A\boldsymbol\mu = \begin{bmatrix}
    \beta_0 \\ \hline
    \beta_1
  \end{bmatrix},
  A\Sigma A^T = \sigma^2 AA^T = \begin{bmatrix}
    \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 & - \frac{\overline{x}}{S_{XX}}\sigma^2 \\ \hline
    - \frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
  \end{bmatrix} \bigg)
  (\#eq:b01mvn)
\end{equation}

Since the joint random vector follows multivariate normal distribution, each *partitioned subset follow normal*. For this theorem, see @Johnson:2013aa. Hence, we finally get the following result.

```{theorem, b01dist, name = "Distributions of regression coefficients"}
Each regression coefficient follows Normal distribution.

$$\hat\beta_0 \sim N \bigg( \beta_0, \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 \bigg)$$

$$\hat\beta_1 \sim N \bigg( \beta_1, \frac{\sigma^2}{S_{XX}} \bigg)$$
```


### Mean response

In simple linear regression setting, we assume $\mu_x = E(Y \mid x) = \beta_0 + \beta_1 x$ is true.


```{r smoothline, fig.cap="Mean response and its standard deviation"}
delv %>% 
  ggplot(aes(x = x, y = y)) +
  geom_smooth(method = "lm") +
  geom_point(alpha = .7) +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```

For example, in the Figure \@ref(fig:smoothline), the blue line indicates $E(Y \mid X = x)$ for each point $x$. Without fitting using `lm()`, `geom_smooth(method = "lm")` let us visualize the fitted line. Since the default method is not the linear regression, the `method` option should be specified.

```{r}
delv %>% 
  mutate(eyx = predict(delv_fit, newdata = data.frame(x = x)))
```

We have already seen in section \@ref(simplebdist) that the estimators $\hat\beta_0$ and $\hat\beta_1$ are random variables. So $\hat\mu_x$ is. In fact, the ribbon of the line in Figure \@ref(fig:smoothline) represents upper and lower confidence limits on mean response. In the later section, we get to know that it is $+ t(n - 2)\widehat{SE}(\hat\mu_x)$ and $- t(n - 2) \widehat{SE}(\hat\mu_x)$. It can be drawn by default with the option of the `geom_smooth(se = TRUE)`.


```{theorem, mrdist, name = "Distribution of mean response estimator"}
$\hat\mu_x$ is also Normally distributed.

$$\hat\mu_x \sim N\bigg( \mu_x, \sigma^2\bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)$$
```

```{proof}
Since $\hat\mu_x = \hat\beta_0 + \hat\beta_1 x$ is the linear combination of $(\hat\beta_0, \hat\beta_1)^T$,

$$\hat\mu_x \sim N\Big(E(\hat\mu_x), Var(\hat\mu_x)\Big)$$

From Theorem \@ref(thm:b01dist),

$$E(\hat\mu_x) = E(\hat\beta_0) + E(\hat\beta_1)x = \beta_0 + \beta_1x \equiv \mu_x$$

and from Proposition \@ref(prp:vb)

\begin{equation*}
  \begin{split}
    Var(\hat\mu_x) & = Var(\hat\beta_0 + \hat\beta_1 x) \\
    & = Var(\hat\beta_0) + x^2Var(\hat\beta_1) + 2xCov(\hat\beta_0, \hat\beta_1) \\
    & = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 + \frac{x^2\sigma^2}{S_{XX}} - \frac{2\overline{x}x\sigma^2}{S_{XX}} \\
    & = \sigma^2\bigg(\frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)
  \end{split}
\end{equation*}
```

```{corollary, mrdiff}
$$\hat\mu_x - \mu_x \sim N\bigg( 0, \sigma^2\bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)$$
```

Denote that in both Theorem \@ref(thm:mrdist) and Corollary \@ref(cor:mrdiff), $\sigma^2$ is parameter. So to use $SE(\hat\mu_x) = \sqrt{Var(\hat\mu_x)}$ in practice we plug in its estimator, usually Equation \@ref(eq:siglse).

```{corollary, mrse, name = "Standard error of mean response estimator"}
$$\widehat{SE}(\hat\mu_x) = \hat\sigma^2\bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)$$

where $\hat\sigma^2 = MSE$
```

### Response

Our goal is to predict each response at each point, i.e. $Y_x = \beta_0 + \beta_1 x + \epsilon_x$. $\epsilon_x \sim N(0, \sigma^2)$ is independent of the given data ($\epsilon_1, \ldots, \epsilon_n$). In this sense, this prediction is called *out of sample prediction*. This setting makes difference between the *residuals, which are correlated to the data*. See Proposition \@ref(prp:resprop) for this. This is occurred because each $\hat\beta_0$ and $\hat\beta_1$ is linear combination of $Y_1, \ldots, Y_n$, not $Y_x$.

While $Cov(Y_i, \hat{Y_i}) > 0, i = 1, \ldots, n$ (See Equation \@ref(eq:yyhat)), in case of out-of-sample $Y_x$,

$$Cov(Y_x, \hat{Y_x}) = Cov(Y_x, \hat\beta_0 + \hat\beta_1 x) = 0$$

Hence, arithmetically, this *out of sample prediction becomes to have larger standard error*.


```{proposition, bepsmvn, name = "Joint distribution of coefficients and error term"}
$(\hat\beta_0, \hat\beta_1, \epsilon_x)^T$ is Normally distributed.
```

```{proof}
Want 1: $(\hat\beta_0, \hat\beta_1)^T \perp\!\!\!\perp \epsilon_x$

We have

\begin{equation}
  \begin{split}
    Cov((\hat\beta_0, \hat\beta_1)^T, \epsilon_x) & = \Big[Cov(\hat\beta_i, \epsilon_x) \Big]_{2 \times 1} \\
    & = \bigg[Cov\bigg(\sum_{i = 1}^n k_i Y_i, \epsilon_x \bigg) \bigg]_{2 \times 1} \qquad k_i = \text{each linear coefficient for}\: \hat\beta_0, \hat\beta_1 \\
    & = \mathbf{0}
  \end{split}
  (\#eq:betaepsind)
\end{equation}

From Equation \@ref(eq:b01mvn),

$$(\hat\beta_0, \hat\beta_1)^T \sim MVN$$

and from assumption,

$$\epsilon_x \sim N(0, \sigma^2)$$

It follows from Equation \@ref(eq:betaepsind) that (@Johnson:2013aa)

$$(\hat\beta_0, \hat\beta_1)^T \perp\!\!\!\perp \epsilon_x$$

Want 2: $(\hat\beta_0, \hat\beta_1, \epsilon_x)^T \sim MVN$

From independency, we have (@Johnson:2013aa)

$$
\begin{bmatrix}
  \hat\beta_0 \\
  \hat\beta_1 \\ \hline
  \epsilon_x
\end{bmatrix} \sim MVN_{2 + 1} \bigg( \begin{bmatrix}
  \beta_0 \\
  \beta_1 \\ \hline
  0
\end{bmatrix}, \left[
  \begin{array}{c|c}
    Cov(\boldsymbol{\hat\beta}) \in \mathbb{R}^{2 \times 2} & \mathbf{0} \in \mathbb{R}^2 \\ \hline
    \mathbf{0}^T \in \mathbb{R}^{2 \times 1} & \sigma^2
  \end{array}
\right] \bigg)
$$
```

This proposition gives clue to distribution of prediction error.

```{theorem, preddist, name = "Distribution of out-of-sample prediction error"}
Out of sample prediction error $\hat{Y_x} - Y_x$ is Normally distributed

$$\hat{Y_x} - Y_x \sim N\bigg( 0, \sigma^2 \bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)$$
```

```{proof}
Note that

\begin{equation*}
  \begin{split}
    \hat{Y_x} - Y_x & = (\hat\beta_0 + \hat\beta_1 x) - (\beta_0 + \beta_1 x + \epsilon_x) \\
    & = [1, x, -1] (\hat\beta_0, \hat\beta_1, \epsilon_x)^T - \beta_0 - \beta_1 x
  \end{split}
\end{equation*}

i.e. $\hat{Y_x} - Y_x$ is a linear combination of $(\hat\beta_0, \hat\beta_1, \epsilon_x)^T$. From prosition \@ref(prp:bepsmvn),

\begin{equation}
  \begin{split}
    \hat{Y_x} - Y_x & \sim MVN \Bigg( [1, x, -1]\begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    0
    \end{bmatrix} - \beta_0 - \beta_1 x,
    [1, x, -1]
    \left[
      \begin{array}{c|c}
        Cov(\boldsymbol{\hat\beta}) \in \mathbb{R}^{2 \times 2} & \mathbf{0} \in \mathbb{R}^2 \\ \hline
        \mathbf{0}^T \in \mathbb{R}^{2 \times 1} & \sigma^2
      \end{array}
    \right]
    \begin{bmatrix}
      1 \\
      x \\
      -1
    \end{bmatrix}
     \Bigg) \\
    & \stackrel{d}{=} MVN \bigg( 0, \sigma^2\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} - 2 \frac{\overline{x}x}{S_{XX}} + \frac{x^2}{S_{XX}} \bigg) + 1 \bigg) \\
    & \stackrel{d}{=} MVN \bigg( 0, \sigma^2\bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg) \bigg)
  \end{split}
  (\#eq:prederrmvn)
\end{equation}
```

Now we know the standard error of this out-of-sample prediction error.

$$SE(\hat{Y_x} - Y_x) = \sigma^2\bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)$$

We can see this standard error is *always larger than of mean response estimator* due to $1$ in the bracket, i.e. $\sigma^2$. As mentioned, this is due to $\epsilon$ term. When we estimate or predict the mean response the model have been assumed to be true. In this out-of-sample prediction setting, however, the model can be wrong. This assumption error is also included in $\epsilon$ term and it is called *irreducible error*, which cannot be reduced anymore.

```{remark}
$$SE(\hat\mu_x - \mu_x) < SE(\hat{Y_x} - Y_x)$$
```

It might be more clear if we see the inequality in the above remark. We know the fact that $\hat{Y_x}$ and $Y_x$ are uncorrelated in this out-of-sample setting. $Y_x$ is random variable, while $\mu_x$ is constant. Then we can re-express the inequality as

$$SE(\hat\mu_x) < SE(\hat{Y_x}) + SE(Y_x)$$

Actually, both $\hat\mu_x$ and $\hat{Y_x}$ are estimated as $\hat\beta_0 + \hat\beta_1 x$. Thus, $SE(Y_x) = \sigma^2$ makes out-of-sample more noisy.

To use standard error practically, we use $\hat\sigma^2$ as in corollary \@ref(cor:mrse).

```{corollary, predse, name = "Standard error of out-of-sample prediction error"}
$$\widehat{SE}(\hat{Y_x} - Y_x) = \hat\sigma^2\bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{XX}} \bigg)$$

where $\hat\sigma^2 = MSE$
```


## Statistical Inference

Based on each distribution of estimator in section \@ref(simpledist), we can construct various inferece for each

- $\beta_0$
- $\beta_1$
- $\mu_x$
- $Y_x$
- $\sigma^2$

We can get the standard error for each coefficient through `summary()` function.

```{r}
summary(delv_fit)
```

Or more state-or-art way, `broom:tidy()` function has a method for each model object to make tidy data: `tibble`.

```{r}
broom::tidy(delv_fit)
```


### Confidence interval

Consider standardization.

$$\frac{\hat\theta - \theta}{\sqrt{SE(\hat\theta)}}$$

Each $SE$ includes $\sigma^2$ as we have already seen. First think about **known** $\sigma^2$ setting. All three estimators follow Normal distribution, and $SE$ is constant by our the setting. Then we can construct each confidence interval as

$$\hat\theta \pm z_{\frac{\alpha}{2}} \sqrt{SE(\hat\theta)}$$


```{r estci, echo=FALSE, fig.cap="Confidence Interval when $\\sigma^2$ is known"}
tibble(x = seq(-5, 5, by = .01)) %>% 
  mutate(
    y = dnorm(x),
    test = ifelse(
      x >= qnorm(.05 / 2, lower.tail = FALSE) |
        x <= qnorm(.05 / 2, lower.tail = TRUE),
      "reject",
      "accept"
    )
  ) %>% 
  ggplot(aes(x, y)) +
  geom_path() +
  geom_ribbon(aes(ymin = 0, ymax = y, fill = test), alpha = .5) +
  scale_fill_manual(values = c("accept" = "red", "reject" = NA))
```


Now just plug in the results of section \@ref(simpledist). For each regression coefficient,

```{proposition, betaci, name = "Confidence intervals on $\\beta$"}
With known $\sigma^2$, $(1 - \alpha)100 \%$ confidence intervals on $\beta_0$ and $\beta_1$ are given as

$$\beta_0 : \quad \hat\beta_0 \pm z_{\frac{\alpha}{2}} \sqrt{\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \sigma^2}$$

$$\beta_1 : \quad \hat\beta_1 \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\sigma^2}{S_{XX}}}$$
```


```{proposition, mrci, name = "Confidence interval on $\\hat\\mu_x$"}
With known $\sigma^2$, $(1 - \alpha)100 \%$ confidence interval on $\hat\mu_x$ is given as

$$\mu_x : \quad \hat\mu_x \pm z_{\frac{\alpha}{2}} \sqrt{\sigma^2 \bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}$$
```

In practice, however, we do not know $\sigma^2$. In this case, we replace $\sigma^2$ with $\hat\sigma^2 = \frac{1}{n - 2}\sum\limits_{i = 1}^n (\hat{Y_i} - Y_i)^2 = MSE$. Then

$$\frac{\hat\theta - \theta}{\sqrt{\widehat{SE}}} = \frac{\frac{\hat\theta - \theta}{\sqrt{SE = \sigma^2(\cdot)}}}{\sqrt{\frac{\frac{SSE}{\sigma^2}}{n - 2}\bigg( \cdot \bigg)}} =  \frac{\frac{\hat\theta - \theta}{\sqrt{SE = \sigma^2}} \sim N(0, 1)}{\sqrt{\frac{\frac{SSE}{\sigma^2} \sim \chi^2(n - 2)}{n - 2}}} \sim t(n - 2)$$

Thus, we need to replace $z_{\frac{\alpha}{2}}$ with $t_{\frac{\alpha}{2}}(n - 2)$.

```{proposition, betaci2, name = "Confidence intervals on $\\beta$ when unknown $\\sigma^2$"}
With unknown $\sigma^2$, $(1 - \alpha)100 \%$ confidence intervals on $\beta_0$ and $\beta_1$ are given as

$$\beta_0 : \quad \hat\beta_0 \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \hat\sigma^2}$$

$$\beta_1 : \quad \hat\beta_1 \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\frac{\hat\sigma^2}{S_{XX}}}$$

where $\hat\sigma^2 = MSE$
```

Here we can estimate the intervals. Basically, `confint()` function gives this interval.

```{r}
confint(delv_fit, level = .95)
```


```{proposition, mrci2, name = "Confidence interval on $\\hat\\mu_x$ when unknown $\\sigma^2$"}
With unknown $\sigma^2$, $(1 - \alpha)100 \%$ confidence interval on $\hat\mu_x$ is given as

$$\mu_x : \quad \hat\mu_x \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\hat\sigma^2 \bigg( \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}$$

where $\hat\sigma^2 = MSE$
```

`predict()` provides options for this confidence interval. Specify `interval = "confidence"`. This argument has three option.

1. `"none"`: just compute fitted value, by default.
2. `"confidence"`: confidence interval of mean response
3. `"prediction"`: prediction interval of out-of-sample prediction

Default `level` is `0.95`.

```{r}
predict(delv_fit, interval = "confidence", level = .95) %>% tbl_df()
```

### Prediction interval

One proceeds in a similar way for out-of-sample $Y_x$.

```{proposition, predci, name = "Prediction interval on $\\hat{Y_x}$"}
With known $\sigma^2$, $(1 - \alpha)100 \%$ confidence interval on $\hat\mu_x$ is given as

$$Y_x : \quad \hat{Y_x} \pm z_{\frac{\alpha}{2}} \sqrt{\sigma^2 \bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}$$
```

Also, with unknown $\sigma^2$,

```{proposition, predci2, name = "Prediction interval on $\\hat{Y_x}$ when unknown $\\sigma^2$"}
With unknown $\sigma^2$, $(1 - \alpha)100 \%$ confidence interval on $\hat\mu_x$ is given as

$$Y_x : \quad \hat{Y_x} \pm t_{\frac{\alpha}{2}}(n - 2) \sqrt{\hat\sigma^2 \bigg( 1 + \frac{1}{n} + \frac{(x - \overline{x})^2}{S_{xx}} \bigg)}$$

where $\hat\sigma^2 = MSE$
```

Since this is out-of-sample setting, we should also give `newdata` option. Otherwise, we will get warning message. Denote that this argument only receive `data.frame` object with same element names.

```{r}
predict(delv_fit, newdata = data.frame(x = 31:35), interval = "prediction", level = .95)
```


### Hypothesis testing

Look again the output of `summary.lm()` and `broom::tidy.lm()`.

```{r}
summary(delv_fit)
```

We can see `t value` and `Pr(>|t|)`. At the same time, `statistic` and `p.value`. What are these values? These are the results of the following tests.

$$H_0 : \beta_0 = \alpha_0 \qquad \text{vs} \qquad H_1 : \beta_0 \neq \alpha_0$$

\begin{equation}
  T = \frac{\hat\beta_0 - \alpha_0}{\hat\sigma \sqrt{\bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{xx}} \bigg)}} \stackrel{H_0}{\sim} t(n - 2)
  (\#eq:b0test)
\end{equation}

For this test statistic \@ref(eq:b0test),

$$\text{reject}\: H_0 \quad \text{if} \: \lvert T \rvert > t_{\frac{\alpha}{2}}(n - 2)$$

```{r b0rjt, echo=FALSE, fig.cap="Rejection region for $\\beta_0$"}
tdf <- nrow(delv) - 2
tibble(x = seq(-5, 5, by = .01)) %>% 
  mutate(
    y = dt(x, df = tdf),
    test = ifelse(
      x >= qt(.05 / 2, df = tdf, lower.tail = FALSE) |
        x <= qt(.05 / 2, df = tdf, lower.tail = TRUE),
      "reject",
      "accept"
    ),
    ymax = ifelse(test == "reject", y, 0)
  ) %>% 
  ggplot(aes(x = x)) +
  geom_path(aes(y = y)) +
  geom_ribbon(aes(ymin = 0, ymax = ymax), alpha = .5, fill = gg_hcl(1))
```


More importantly, we test $\beta_1$ which means slope

$$H_0 : \beta_1 = \alpha_1 \qquad \text{vs} \qquad H_1 : \beta_1 \neq \alpha_1$$

\begin{equation}
  T = \frac{\hat\beta_0 - \alpha_0}{\hat\sigma \sqrt{\frac{1}{S_{xx}}}} \stackrel{H_0}{\sim} t(n - 2)
  (\#eq:b1test)
\end{equation}

For this test statistic \@ref(eq:b1test),

$$\text{reject}\: H_0 \quad \text{if} \: \lvert T \rvert > t_{\frac{\alpha}{2}}(n - 2)$$

Looking at these two statistics, we can intuitively know the meaning. As $\lvert \hat\beta_1 - \alpha_1 \rvert$ becomes larger, the data support $H_1$.


## Analysis of Variance

### Useful distributions

In linear regression setting, we usually assume $\epsilon_i \iid N(0, \sigma^2)$. There are some useful distributions around Normal.

```{proposition, chisq, name = "$\\chi^2$-distribution"}
Square of standard normal follows $\chi^2$-distribution.

If $Z \sim N(0, 1)$, then $Z^2 \sim \chi^2(1)$

If $Z_i \indep N(0, 1)$, then $Z_1^2 + \cdots + Z_n^2 \sim \chi^2(n)$
```

```{proposition, tdist, name = "t-distribution"}
Let $Z \sim N(0, 1) \ind V \sim \chi^2(m)$. Then

$$T = \frac{Z}{\sqrt{V / m}} \sim t(m)$$
```

```{proposition, fdist, name = "F-distribution"}
Let $V \sim \chi^2(m) \ind W \sim \chi^2(n)$. Then

$$F = \frac{V / m}{W / n} \sim F(m, n)$$
```

Also, there is *non-central analogue* of these three distributions, i.e. starting from $Z \sim N(\mu, 1)$.

```{proposition, nonchi, name = "Noncentral $\\chi^2$-distribution"}
Square of scaled normal follows non-central $\chi^2$-distribution.

If $Z_i \indep N(\mu_i, 1)$, then $Z_1^2 + \cdots + Z_n^2 \sim \chi^2(n, \sum\limits_{i = 1}^n \mu_i^2)$

$\sum\limits_{i = 1}^n \mu_i^2$ is called a non-central parameter.
```

```{proposition, nontdist, name = "Noncentral t-distribution"}
Let $X \sim N(\mu, 1) \ind V \sim \chi^2(m)$. Then

$$T = \frac{Z}{\sqrt{V / m}} \sim t(m, \mu)$$

$\mu$ is called a non-central parameter.
```

```{proposition, nonfdist, name = "Noncentral F-distribution"}
Let $V \sim \chi^2(m, \delta) \ind W \sim \chi^2(n)$. Then

$$F = \frac{V / m}{W / n} \sim F(m, n, \delta)$$

$\delta$ is called a non-central parameter.
```

### Quadratic form

Now we can determine the distributions of various quadratic forms. The reason we are taking care of this is ANOVA deals with sum of squares, i.e. quadratic form.

```{theorem, idem, name = "Idempotent and symmetric"}
Let $A \in \R^{k \times k}$ be idempotent and symmetric. Then

$\text{(a)}\: A^n$ is also idempotent

$\text{(b)}\: I - A$ is also idempotent

$\text{(c)}\:$ Every eigenvalue of $A$ is either $0$ or $1$ so that $tr(A) = rank(A)$
```

```{proof}
(a) and (b) are trivial.

$$(A^n)^2 = (A^2)^n = A^n$$

$$(I - A)^2 = I - 2A + A^2 = I - A$$

(c)

Fix $\lambda$ an eigenvalue of $A$. Let $\mathbf{v} \neq \mathbf{0}$ be the corresponding eigenvector.

By definition,

$$A\mathbf{v} = \lambda \mathbf{v}$$

Then

$$A^2\mathbf{v} = \lambda(A\mathbf{v}) = \lambda^2\mathbf{v}$$

and so $\lambda^2$ is eigenvalue of $A^2$.

Since $A^2 = A$,

$$\lambda = \lambda^2$$

Hence,

$$\lambda = 0 \:\text{or}\: 1$$

Note that for every matrix and its eigenvalues $\lambda_j$

$$tr(X) = \sum_{j = 1}^p \lambda_j, \quad rank(X) = \text{the number of non-zero}\: \lambda_j$$

Since $\lambda = 0, 1$ of A,

$$tr(A) = rank(A)$$
```

```{proposition, quadmvn, name = "Independence"}
Assume $\mathbf{Y} \sim MVN(\mathbf\mu, \Sigma)$. Then

(i) If $A$ and $B$ are symmetric,

$$Y^T AY \ind Y^T BY \Leftrightarrow A\Sigma B = 0$$

(ii) If $A$ is symmetric,

$$Y^T AY \ind BY \Leftrightarrow B\Sigma A = 0$$
```

```{theorem, quaddist, name = "Distribution of quadratic form"}
Assume that $\mathbf{Y} \sim MVN(\mathbf\mu, \Sigma)$ and that $A$ is symmetric and idempotent. Then

$$Y^T AT \sim \chi^2(K, \delta)$$

where $K = rank(A)$ and $\delta = \boldsymbol\mu^T A \boldsymbol\mu$. Furthermore,

$$
\begin{cases}
  E(Y^T AT) = K + \delta \\
  Var(Y^T AT) = 2(K + 2\delta)
\end{cases}
$$
```

```{corollary, mvnchi, name = "Inner product of standard normal vector"}
Let $\mathbf{Z} = (Z_1, \ldots, Z_n)^T \sim MVN(\mathbf{0}, I_n)$. Then

$$\mathbf{Z}^T\mathbf{Z} = \sum_{i = 1}^n Z_i^2 \sim \chi^2(n)$$
```

```{proof}
From Theorem \@ref(thm:quaddist) point of view,

$$\mathbf{Z}^T\mathbf{Z} = \mathbf{Z}^T I_n \mathbf{Z}$$

Thus,

$$K = rank(I_n) = n$$

$$\delta = \mathbf{0}$$
```

Using the above facts, we can now show distributions of sums of squares. First recall that

$$\mathbf{Y} \sim MVN(X\boldsymbol\beta, \sigma^2 I)$$

```{proposition, ssedist, name = "Distribution of SSE"}
$$\frac{SSE}{\sigma^2} \sim \chi^2(n - 2, 0)$$
```

```{proof}
From Corollary \@ref(cor:projss), write

$$\frac{SSE}{\sigma^2} = \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)^T (I - \Pi_X) \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)$$

Note that

$$\frac{\mathbf{Y}}{\sigma} \sim MVN(\frac{1}{\sigma}X\boldsymbol\beta, I)$$

Since $I - \Pi_X$ is idempotent and symmetric,

$$K = rank(I - \Pi_X) = tr(I - \Pi_X) = n - rank(\Pi_X) = n - 2$$

\begin{equation} \label{eq:delta1}
  \begin{split}
    \delta & = \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg)^T (I - \Pi_X) \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg) \\
    & = \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} - \frac{(\boldsymbol\beta^TX^T)X(X^TX)^{-1}X^T(X\boldsymbol\beta)}{\sigma^2} \\
    & = \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} - \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} \\
    & = 0
  \end{split}
\end{equation}

Hence,

$$\frac{SSE}{\sigma^2} \sim \chi^2(n - 2)$$
```

```{proposition, ssrdist, name = "Distribution of SSR"}
$$\frac{SSR}{\sigma^2} \sim \chi^2(1, \delta)$$

where $\delta = \sum\limits_{i = 1}^n (x_i - \overline{x})^2 \beta_1^2 = \frac{S_{xx}\beta_1^2}{\sigma^2}$
```

```{proof}
From Corollary \@ref(cor:projss), write

$$\frac{SSR}{\sigma^2} = \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)^T (\Pi_X - \Pi_{\mathbf{1}}) \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)$$

Note that $\Pi_X - \Pi_{\mathbf{1}}$ is symmetric idempotent. One proceeds in a similar way.

$$K = rank(\Pi_X - \Pi_{\mathbf{1}}) = tr(\Pi_X - \Pi_{\mathbf{1}}) = rank(\Pi_X) - rank(\Pi_{\mathbf{1}}) = 2 - 1 = 1$$

\begin{equation*}
  \begin{split}
    \delta & = \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg)^T (\Pi_X - \Pi_{\mathbf{1}}) \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg) \qquad \because \frac{\mathbf{Y}}{\sigma} \sim MVN(\frac{1}{\sigma}X\boldsymbol\beta, I) \\
    & = \frac{\boldsymbol\beta^TX^TX\boldsymbol\beta}{\sigma^2} - \frac{\boldsymbol\beta^TX^T\Pi_{\mathbf{1}}X\boldsymbol\beta}{\sigma^2} \\
    & = \frac{\boldsymbol\beta^T(X^TX - X^T \Pi_{\mathbf{1}}X)\boldsymbol\beta}{\sigma^2} \\
    & = \frac{\boldsymbol\beta^T \Big\{ X^T(I - \Pi_{\mathbf{1}})X \Big\} \boldsymbol\beta }{\sigma^2}
  \end{split}
\end{equation*}

Since $\mathbf{1} \in sp(\{ \mathbf{1} \})$,

$$\Pi_{\mathbf{1}} \mathbf{1} = \mathbf{1}$$

It gives that

$$\mathbf{1}^T(I - \Pi_{\mathbf{1}})\mathbf{1} = 0$$

If $\mathbf{x} \neq \mathbf{1}$, then we have

$$\mathbf{x}^T(I - \Pi_{\mathbf{1}})\mathbf{x} = \sum_{i = 1}^n (x_i - \overline{x})^2 = S_{xx}$$

Recall that

$$\overline{x}\mathbf{1} = \mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T\mathbf{x} = \Pi_{\mathbf{1}}\mathbf{x}$$

Then we have

$$\mathbf{1}^T(I - \Pi_{\mathbf{1}})\mathbf{x} = \sum x_i - n \overline{x} = 0$$

Similarly,

$$\mathbf{x}^T(I - \Pi_{\mathbf{1}})\mathbf{1} = n \overline{x} - \sum x_i = 0$$

Hence by partitioning $X = [\mathbf{1} \mid \mathbf{x}]$,

\begin{equation} \label{eq:delta2}
  \begin{split}
    \delta & = \frac{\boldsymbol\beta^T \Big\{ [\mathbf{1} \mid \mathbf{x}]^T(I - \Pi_{\mathbf{1}})[\mathbf{1} \mid \mathbf{x}] \Big\} \boldsymbol\beta }{\sigma^2} \\
    & = \frac{\boldsymbol\beta^T \begin{bmatrix} 0 & 0 \\ 0 & S_{xx} \end{bmatrix} \boldsymbol\beta}{\sigma^2} \\
    & = \frac{S_{xx}\beta_1^2}{\sigma^2}
  \end{split}
\end{equation}
```

```{proposition, ssind, name = "Independence"}
SSE and SSR are independent, i.e.

$$SSE \ind SSR$$
```

```{proof}
Note that both $SSE$ and $SSR$ are quadratic forms of $\mathbf{Y} \sim MVN(X\boldsymbol\beta, \sigma^2 I)$ and that each $I - \Pi_X$ and $\Pi_X - \Pi_{\mathbf{1}}$ is symmetric. Then from Proposition \@ref(prp:quadmvn),

Claim: $(I - \Pi_X)(\sigma^2I)(\Pi_X - \Pi_{\mathbf{1}}) = 0$, i.e. $(I - \Pi_X)(\Pi_X - \Pi_{\mathbf{1}}) = 0$

It is obvious that

$$\Pi_X\Pi_{\mathbf{1}} = \Pi_{\mathbf{1}}$$

Then

\begin{equation*}
  \begin{split}
    (I - \Pi_X)(\Pi_X - \Pi_{\mathbf{1}}) & = \Pi_X - \Pi_{\mathbf{1}} - \Pi_X^2 + \Pi_X\Pi_{\mathbf{1}} \\
    & = \Pi_X - \Pi_{\mathbf{1}} - \Pi_X + \Pi_{\mathbf{1}} \qquad \because \text{idempotent} \\
    & = 0
  \end{split}
\end{equation*}

This completes the proof.
```

```{proposition, ssbind, name = "Independence"}
SSE and $(\hat\beta_0, \hat\beta_1)$ are independent, i.e.

$$SSE \ind (\hat\beta_0, \hat\beta_1)^T$$
```

```{proof}
Note that

$$\boldsymbol{\hat\beta}= (\hat\beta_0, \hat\beta_1)^T = (X^TX)^{-1}X^T\mathbf{Y}$$

Since $I - \Pi_X$ of $SSE$ is symmetric, from Proposition \@ref(prp:quadmvn),

Claim: $((X^TX)^{-1}X^T)(\sigma^2I)(I - \Pi_X) = 0$, i.e. $((X^TX)^{-1}X^T)(I - \Pi_X) = 0$

Since $\Pi_X = X(X^TX)^{-1}X^T$,

\begin{equation*}
  \begin{split}
    ((X^TX)^{-1}X^T)(I - \Pi_X) & = (X^TX)^{-1}X^T - (X^TX)^{-1}X^TX(X^TX)^{-1}X^T \\
    & = (X^TX)^{-1}X^T - (X^TX)^{-1}X^T \\
    & = 0
  \end{split}
\end{equation*}

This completes the proof.
```

```{proposition, sstdist, name = "Distribution of SST"}
$$\frac{SST}{\sigma^2} \sim \chi^2(n - 1, \delta)$$

where $\delta = \sum\limits_{i = 1}^n (x_i - \overline{x})^2 \beta_1^2 = \frac{S_{xx}\beta_1^2}{\sigma^2}$
```

```{proof}
It proceedes in a similary way from Corollary \@ref(cor:projss)

$$\frac{SST}{\sigma^2} = \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)^T (I - \Pi_{\mathbf{1}}) \bigg(\frac{\mathbf{Y}}{\sigma}\bigg)$$

Since $I - \Pi_{\mathbf{1}}$ is symmetric idempotent,

$$K = rank(I - \Pi_{\mathbf{1}}) = tr(I - \Pi_{\mathbf{1}}) = n - rank(\Pi_{\mathbf{1}}) = n - 1$$

\begin{equation*}
  \begin{split}
    \delta & = \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg)^T (I - \Pi_{\mathbf{1}}) \bigg(\frac{X\boldsymbol\beta}{\sigma}\bigg) \\
    & = \frac{S_{xx}\beta_1^2}{\sigma^2} \qquad \because \eqref{eq:delta1} \:\text{and}\: \eqref{eq:delta2} 
  \end{split}
\end{equation*}
```


### ANOVA for testing significance of regression

Recall that

$$SST = SSR + SSE$$

- $SST$: the variation of a response itself
- $SSR$: the variation of a response *explained by the model*
- $SSE$: the variation of a response that *cannot be explained by the model*

As mentioned in section \@ref(decompsst), whether the model is useful or not can depend on the proportion of $SSR$ versus $SSE$ in constant $SST$. When $SSR$ is large compared to $SSE$, we can say that the model is good. On the other hand, when $SSR$ is not large, the model might be poor. This is what $R^2$ measures intuitively.

However, this direct comparison somtimes does not work in many times. Both $SSR$ and $SSE$ comes from different distribution, which have different degrees of freedom. So we *compare standardized versions*, i.e. divided by the degrees of freedom.

```{definition, dof, name = "Degrees of freedom"}
Degrees of freedom of each sum of squares is

$$df = \text{the number of deviation} - \text{the number of linear constraints}$$
```

```{corollary, dfss, name = "df of SS"}
$df$ of each sum of square is computed as

$\text{(a)}\: df(SST) = n - 1$

$\text{(b)}\: df(SSR) = 1$

$\text{(c)}\: df(SSE) = n - 2$
```

```{proof}
(a)

Since $\sum (Y_i - \overline{Y}) = 0$, we have $1$ linear constraints. Thus,

$$df(SST) = n - 1$$

(b)

Note that $\hat{Y_i} - \overline{Y} = \hat\beta_1(x_i - \overline{x})$

where $\sum (x_i - \overline{x}) = 0$.

Thus,

$$df(SSR) = n - (n - 1) = 1$$

(c)

From Example \@ref(exm:usingnormal), $\sum (Y_i - \hat{Y_i}) = 0$ and $\sum x_i (Y_i - \hat{Y_i}) = 0$.

Thus,

$$df(SSE) = n - 2$$
```

Dividing sum of squares in $df$, we can standardize it.

```{definition, ms, name = "Mean square"}
Mean square is a sum of square $SS$ divided by its degree of freedom $df$

$$MS := \frac{SS}{df}$$
```

Using the values of corollary \@ref(cor:dfss) we can define each mean square for $SSR$ and $SSE$.

```{definition, msr, name = "Regression mean square"}
$$MSR := \frac{SSR}{1} = SSR$$
```

From Proposition \@ref(prp:ssrdist), the following corollary can be drawn.

```{corollary, msrdist, name = "Distribution of MSR"}
Under $H_0: \beta_1 = 0$,

$$\frac{SSR}{\sigma^2} \stackrel{H_0}{\sim} \chi^2(1)$$
```

Now standardize residual sum of square.

```{definition, mse, name = "Residual mean square"}
$$MSE := \frac{SSE}{n - 2}$$
```

From Proposition \@ref(prp:ssrdist), we can construct same statistic. In fact, $\frac{SSE}{\sigma^2}$ follows $\chi^2(n - 2)$ whether or not $\beta_1$ is zero. Its $\delta = 0$.

```{corollary, msedist, name = "Distribution of MSE"}
$$\frac{SSE}{\sigma^2} \sim \chi^2(n - 2)$$
```

Finally, we can now use Proposition \@ref(prp:fdist) so that

$$
F \equiv \frac{MSR}{MSE} = \frac{\frac{SSE / \sigma^2 \sim \chi^2(1)}{1}}{\frac{SSR / \sigma^2 \hsim \chi^2(n - 2)}{n - 2}} \hsim F(1, n - 2)
$$

By construction, this test statistic is used for

$$H_0: \beta_1 = 0$$

which means that the predictor does not explain the response anything. In other words, we are testing that

\begin{equation}
  H_0: \text{Model is not useful at all} \qquad \text{vs} \qquad H_1: \text{Model can explain data}
  (\#eq:goodfit)
\end{equation}

```{remark, name = "F statistic on testing significance"}
Null hypothesis \@ref(eq:goodfit) can be tested with $F$-statistic.

$$F_0 = \frac{MSR}{MSE} = \frac{SSR / df(SSR)}{SSE / df(SSE)} \hsim F(df(SSR), df(SSE))$$
```

Then we reject $H_0$ if

$$F_0 > F_\alpha\bigg( df(SSR), df(SSE) \bigg)$$

```{r goodfitfig, echo=FALSE, fig.cap="Rejection region for significance testing"}
dfr <- 1
dfe <- nrow(delv) - 2
tibble(x = seq(0, 10, by = .01)) %>% 
  mutate(
    y = df(x, df1 = dfr, df2 = dfe),
    test = ifelse(
      x <= qf(.05, df1 = dfr, df2 = dfe, lower.tail = FALSE),
      "accept",
      "reject"
    )
  ) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_path() +
  geom_ribbon(aes(ymin = 0, ymax = y, fill = test), alpha = .5) +
  scale_fill_manual(values = c("accept" = NA, "reject" = "red")) +
  ylim(0, 1)
```

```{r}
summary(delv_fit)
```

This statistic is `F-statistic` included in `summary.lm()` output. This is saved as `$fstatistic`.

```{r}
summary(delv_fit)$fstatistic
```

We usually summarize these statistic in table form, so called *ANOVA table*.

|Source|SS|df|MS|F|p-value|  
|:----:|:--:|:--:|:--:|:--:|:--:|  
|Model| $SSR$ | $1$ | $MSR$ | $F_0$ | p-value |  
|Error| $SSE$ | $n - 2$ | $MSE$ | | |  
|Total| $SST$ | $n - 1$ | | | |  

To get this table, just use `anova()` for `lm` object.

```{r}
anova(delv_fit)
```

Since the last `Total` row is just sum of the model and error, the function does not give it. To use this table as `data.frame` more easily, just implement `broom::tidy` as before.

```{r}
anova(delv_fit) %>% 
  broom::tidy()
```

Denote that here *simple linear regression setting* $F$-statistic and $t$-statistic of Equation \@ref(eq:b1test) perform exactly same thing, $H_0 : \beta_1 = 0$. In fact, we know that

$$F(1, k) \stackrel{d}{=} T_k^2$$

```{remark}
In the simple linear regression setting, $F$-test for significance and $t$-test for no slope are equivalent, i.e. under $H_0 : \beta_1 = 0$

$$F_0 = \frac{\hat\beta_1 S_{xx}}{\sigma^2} = \bigg( \frac{\hat\beta_1}{\sigma / \sqrt{S_xx}} \bigg) = T_0^2$$
```

