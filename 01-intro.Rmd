# Simple Linear Regression {#simple}

```{r, echo=FALSE, results='asis'}
eqn_numbering()
```


## Model

```{r}
delv <- MPV::p2.9 %>% tbl_df()
```

```{r delivery, fig.cap=fig$cap("delivery", "The Delivery Time Data")}
delv %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```


Given data $(x_1, Y_1), \ldots, (x_n, Y_n)$, we try to fit linear model

$$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

Here $\epsilon_i$ is a error term, which is a random variable.

$$\epsilon \stackrel{iid}{\sim} (0, \sigma^2)$$

It gives the problem of estimating three parameters $(\beta_0, \beta_1, \sigma^2)$. Before estimating these, we set some assumptions.

1. linear relationship
2. $\epsilon_i$s are independent
3. $\epsilon_i$s are identically destributed, i.e. *constant variance*
4. In some setting, $\epsilon_i \sim N$

## Least Squares Estimation

```{r lsefig, echo=FALSE, fig.cap=fig$cap("lsefig", "Idea of the least square estimation")}
delv %>% 
  mutate(yhat = predict(lm(y ~ x))) %>% 
  ggplot(aes(x = x)) +
  geom_abline(intercept = 10, slope = 1.5, col = I("grey40"), alpha = .7) +
  geom_abline(intercept = 5, slope = 2, col = I("grey40"), alpha = .7) +
  geom_line(aes(y = yhat), col = gg_hcl(2)[2], size = 1, alpha = .7) +
  geom_point(aes(y = y)) +
  geom_linerange(aes(ymin = y, ymax = yhat), col = gg_hcl(2)[1]) +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```

We try to find $\beta_0$ and $\beta_1$ that minimize the sum of squares of the vertical distances, i.e.

\begin{equation} \label{eq:ssq}
  (\beta_0, \beta_1) = \arg\min \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2
\end{equation}

### Normal equations

Denote that Equation $\eqref{eq:ssq}$ is quadratic. Then we can find its minimum by find the zero point of the first derivative. Set

$$Q(\beta_0, \beta_1) := \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2$$

Then we have

\begin{equation} \label{eq:normbeta0}
  \frac{\partial Q}{\partial \beta_0} = -2 \sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i) = 0
\end{equation}

and

\begin{equation} \label{eq:normbeta1}
  \frac{\partial Q}{\partial \beta_1} = -2 \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)x_i = 0
\end{equation}

From $\eqref{eq:normbeta0}$,

$$\sum_{i = 1}^n Y_i - n \hat\beta_0 - \hat\beta_1 \sum_{i = 1}^n x_i = 0$$

Thus,

$$\hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x}$$

$\eqref{eq:normbeta1}$ gives

$$\sum_{i = 1}^n x_i (Y_i - \overline{Y} + \hat\beta_1\overline{x} - \hat\beta_1 x_i) = \sum_{i = 1}^n x_i(Y_i - \overline{Y}) - \hat\beta_1\sum_{i = 1}^n x_i (x_i - \overline{x}) = 0$$

Thus,

$$\hat\beta_1 = \frac{\sum\limits_{i = 1}^nx_i(Y_i - \overline{Y})}{\sum\limits_{i = 1}^n x_i (x_i - \overline{x})}$$

```{remark}
$$\hat\beta_1 = \frac{S_{XY}}{S_{XX}}$$

where $S_{XX} := \sum\limits_{i = 1}^n (x_i - \overline{x})^2$ and $S_{XY} := \sum\limits_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y})$
```

```{proof}
Note that $\overline{x}^2 = \frac{1}{n^2}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2$. Then we have

\begin{equation} \label{eq:sxx}
  \begin{split}
    S_{XX} & = \sum_{i = 1}^n (x_i - \overline{x})^2 \\
    & = \sum_{i = 1}^n x_i^2 - 2\sum_{i = 1}^n x_i \overline{x} + \sum_{i = 1}^n\overline{x}^2 \\
    & = \sum_{i = 1}^n x_i^2 - \frac{2}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 + \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 \\
    & = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2
  \end{split}
\end{equation}

It follows that

\begin{equation*}
  \begin{split}
    \hat\beta_1 & = \frac{\sum x_i(Y_i - \overline{Y})}{\sum x_i (x_i - \overline{x})} \\
    & = \frac{\sum x_i (Y_i - \overline{Y}) - \overline{x}\sum (Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \qquad \because \sum (Y_i - \overline{Y}) = 0 \\
    & = \frac{\sum (x_i - \overline{x})(Y_i - \overline{Y})}{\sum x_i^2 - \frac{1}{n} (\sum x_i)^2} \\
    & = \frac{S_{XY}}{S_{XX}}
  \end{split}
\end{equation*}
```


```{r}
lm(y ~ x, data = delv)
```


### Prediction and Mean response


> "Essentially, all models are wrong, but some are useful."
>
> ---George Box


Recall that we have assumed the **linear assumption** between the predictor and the response variables, i.e. the true model. Estimating $\beta_0$ and $\beta_1$ is same as estimating the *assumed true model*.

```{definition, eyx, name = "Mean response"}
$$E(Y \mid X = x) = \beta_0 + \beta_1 x$$
```

We can estimate this mean resonse by

\begin{equation} \label{eq:meanres}
  \widehat{E(Y \mid x)} = \hat\beta_0 + \hat\beta_1 x
\end{equation}

However, in practice, the model might not be true, which is included in $\epsilon$ term.

$$Y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

Our real problem is predicting individual $Y$, not the mean. The *prediction* of response can be done by

\begin{equation} \label{eq:indpred}
  \hat{Y_i}  = \hat\beta_0 + \hat\beta_1 x_i
\end{equation}

Observe that the values of Equation $\eqref{eq:meanres}$ and $\eqref{eq:indpred}$ are same. However, due to the **error term in the prediction**, it has larger standard error.


### Properties of LSE {#lseprop}

Parameters $\beta_0$ and $\beta_1$ have some properties related to the expectation and variance. We can notice that these lse's are **unbiased linear estimator**. In fact, these are the *best unbiased linear estimator*. This will be covered in the Gauss-Markov theorem.


```{lemma, sxy}
$$S_{XX} = \sum_{i = 1}^n x_i^2 - \frac{1}{n}\bigg(\sum\limits_{i = 1}^n x_i\bigg)^2 = \sum_{i = 1}^n x_i(x_i - \overline{x})$$

$$S_{XY} = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg) = \sum_{i = 1}^n Y_i(x_i - \overline{x})$$
```

```{proof}
We already proven the first part of $S_{XX}$. See the Equation $\eqref{eq:sxx}$. The second part is tivial. Since $\sum (x_i - \overline{x}) = 0$,

$$S_{XX} = \sum_{i = 1}^n (x_i - \overline{x})^2 = \sum_{i = 1}^n (x_i - \overline{x})x_i$$

For the first part of $S_{XY}$,

\begin{equation*}
  \begin{split}
    S_{XY} & = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    & = \sum_{i = 1}^n x_i Y_i - \overline{x} \sum_{i = 1}^n Y_i - \overline{Y} \sum_{i = 1}^n x_i + n \overline{x} \overline{Y} \\
    & = \sum_{i = 1}^n x_i Y_i - \frac{1}{n}\bigg(\sum_{i = 1}^n x_i\bigg)\bigg(\sum_{i = 1}^n Y_i\bigg)
  \end{split}
\end{equation*}

Second part of $S_{XY}$ also can be proven from the definition.

\begin{equation*}
  \begin{split}
    S_{XY} & = \sum_{i = 1}^n (x_i - \overline{x})(Y_i - \overline{Y}) \\
    & = \sum_{i = 1}^n Y_i (x_i - \overline{x}) - \overline{Y} \sum_{i = 1}^n (x_i - \overline{x}) \\
    & = \sum_{i = 1}^n Y_i (x_i - \overline{x}) \qquad \because \sum_{i = 1}^n (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}
```

```{lemma, linbet, name = "Linearity"}
Each coefficient is a linear estimator.

$$\hat\beta_1 = \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i$$

$$\hat\beta_0 = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})}{S_{XX}} \bigg) Y_i$$
```

```{proof}
From lemma \@ref(lem:sxy),

\begin{equation*}
  \begin{split}
    \hat\beta_1 & = \frac{S_{XY}}{S_{XX}} \\
    & = \frac{1}{S_{XX}}\sum_{i = 1}^n (x_i - \overline{x}) Y_i
  \end{split}
\end{equation*}

It gives that

\begin{equation*}
  \begin{split}
    \hat\beta_0 & = \overline{Y} - \hat\beta_1 \overline{x} \\
    & = \frac{1}{n}\sum_{i = 1}^n Y_i - \overline{x} \sum_{i = 1}^n\frac{(x_i - \overline{x})}{S_{XX}}Y_i \\
    & = \sum_{i = 1}^n\bigg(\frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)Y_i
  \end{split}
\end{equation*}
```

```{proposition, ue, name = "Unbiasedness"}
Both coefficients are unbiased.

$\text{(a)}\: E\hat\beta_1 = \beta_1$

$\text{(b)}\: E\hat\beta_0 = \beta_0$
```

From the model, $Y_1, \ldots, Y_n \stackrel{indep}{\sim} (\beta_0 + \beta_1 x_i, \sigma^2)$.

```{proof}
From lemma \@ref(lem:sxy),

\begin{equation*}
  \begin{split}
    E\hat\beta_1 & = \sum_{i = 1}^n \bigg[ \frac{(x_i - \overline{x})}{S_{XX}} E(Y_i) \bigg] \\
    & = \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}(\beta_0 + \beta_1 x_i) \\
    & = \frac{\beta_1 \sum (x_i - \overline{x})x_i}{\sum (x_i - \overline{x})x_i} \qquad \because \sum (x_i - \overline{x}) = 0 \\
    & = \beta_1
  \end{split}
\end{equation*}

It follows that

\begin{equation*}
  \begin{split}
    E\hat\beta_0 & = E(\overline{Y} - \hat\beta_1 \overline{x}) \\
    & = E(\overline{Y}) - \overline{x}E(\hat\beta_1) \\
    & = E(\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) - \beta_1 \overline{x} \\
    & = \beta_0 + \beta_1 \overline{x} - \beta_1 \overline{x} \\
    & = \beta_0
  \end{split}
\end{equation*}
```

```{proposition, vb, name = "Variances"}
Variances and covariance of coefficients

$\text{(a)}\: Var\hat\beta_1 = \frac{\sigma^2}{S_{XX}}$

$\text{(b)}\: Var\hat\beta_0 = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2$

$\text{(c)}\: Cov(\hat\beta_0, \hat\beta_1) = - \frac{\overline{x}}{S_{XX}} \sigma^2$
```

```{proof}
Proving is just arithmetic.

(a)

\begin{equation*}
  \begin{split}
    Var\hat\beta_1 & = \frac{1}{S_{XX}^2}\sum_{i = 1}^n \bigg[ (x_i - \overline{x})^2 Var(Y_i) \bigg] + \frac{1}{S_{XX}^2} \sum_{j \neq k}^n \bigg[ (x_j - \overline{x})(x_k - \overline{x}) Cov(Y_j, Y_k) \bigg] \\
    & = \frac{\sigma^2}{S_{XX}} \qquad \because Cov(Y_j, Y_k) = 0 \: \text{if} \: j \neq k
  \end{split}
\end{equation*}

(b)

\begin{equation*}
  \begin{split}
    Var\hat\beta_0 & = \sum_{i = 1}^n \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)^2Var(Y_i) + \sum_{j \neq k} \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\bigg( \frac{1}{n} - \frac{(x_k - \overline{x})\overline{x}}{S_{XX}} \bigg) Cov(Y_j, Y_k) \\
    & = \frac{\sigma^2}{n} - 2 \sigma^2 \frac{\overline{x}}{S_{XX}} \sum_{i = 1}^n (x_i - \overline{x}) + \frac{\sigma^2 \overline{x}^2 \sum (x_i - \overline{x})^2}{S_{XX}^2} \\
    & = \bigg( \frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg) \sigma^2 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}

(c)

\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, \hat\beta_1) & = Cov(\overline{Y} - \hat\beta_1 \overline{x}, \hat\beta_1) \\
    & = - \overline{x} Var\hat\beta_1 \\
    & = - \frac{\overline{x}}{S_{XX}} \sigma^2
  \end{split}
\end{equation*}
```


### Gauss-Markov Theorem

Chapter \@ref(lseprop) shows that the $\beta_0^{LSE}$ and $\beta_1^{LSE}$ are the **linear unbiased estimators**. Are these good? Good compared to _what estimators_? Here we consider *linear unbiased estimator*. If variances in the proposition \@ref(prp:vb) are lower than any parameters in this parameter family, $\beta_0^{LSE}$ and $\beta_1^{LSE}$ are the **best linear unbiased estimators**.

```{theorem, gmt, name = "Gauss Markov Theorem"}
$\hat\beta_0$ and $\hat\beta_1$ are BLUE, i.e. the best linear unbiased estimator.

$$Var(\hat\beta_0) \le Var\Big( \sum_{i = 1}^n a_i Y_i \Big) \: \forall a_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n a_i Y_i \Big) = \beta_0$$

$$Var(\hat\beta_1) \le Var\Big( \sum_{i = 1}^n b_i Y_i \Big) \: \forall b_i \in \mathbb{R} \: \text{s.t.} \: E\Big( \sum_{i = 1}^n b_i Y_i \Big) = \beta_1$$
```

```{proof, name = "Bestness of beta1"}
Consider $\Theta := \bigg\{ \sum\limits_{i = 1}^n b_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n b_i Y_i \Big) = \beta_1 \bigg\}$.

Claim: $Var( \sum b_i Y_i) - Var(\hat\beta_1) \ge 0$

Let $\sum b_i Y_i \in \Theta$. Then $E(\sum b_i Y_i) = \beta_1$.

Since $E(Y_i) = \beta_0 + \beta_1 x_i$,

$$\beta_0 \sum b_i + \beta_1 \sum b_i x_i = \beta_1$$

It gives

\begin{equation} \label{eq:ule}
  \begin{cases}
    \sum b_i = 0 \\
    \sum b_i x_i = 1
  \end{cases}
\end{equation}

Then

\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum b_i Y_i - \hat\beta_1\Big) & = Var\Big( \sum b_i Y_i - \sum \frac{(x_i - \bar{x})}{S_{XX}} Y_i \Big) \\
    & \stackrel{indep}{=} \sum \bigg( b_i - \frac{(x_i - \bar{x})}{S_{XX}} \bigg)^2 \sigma^2 \\
    & = \sum \bigg( b_i^2 - \frac{2b_i (x_i - \bar{x})}{S_{XX}} + \frac{(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \sigma^2 \\
    & = \sum b_i^2 \sigma^2 - \frac{2 \sigma^2}{S_{XX}} \sum b_i x_i + \frac{2 \bar{x} \sigma^2}{S_{XX}} \sum b_i + \sigma^2 \frac{\sum (x_i - \bar{x})^2}{S_{XX}^2} \\
    & = \sum b_i^2 \sigma^2 - \frac{\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule} \:\text{and}\: S_{XX} = \sum (x_i - \bar{x})^2 \\
    & = Var(\sum b_i Y_i) - Var(\hat\beta_1)
  \end{split}
\end{equation*}

Hence,

$$Var(\sum b_i Y_i) \ge Var(\hat\beta_1)$$
```


```{proof, name = "Bestness of beta0"}
Consider $\Theta := \bigg\{ \sum\limits_{i = 1}^n a_i Y_i \in \mathbb{R} : E\Big( \sum\limits_{i = 1}^n a_i Y_i \Big) = \beta_0 \bigg\}$.

Claim: $Var( \sum a_i Y_i) - Var(\hat\beta_0) \ge 0$

Let $\sum a_i Y_i \in \Theta$. Then $E(\sum a_i Y_i) = \beta_0$.

Since $E(Y_i) = \beta_0 + \beta_1 x_i$,

$$\beta_0 \sum a_i + \beta_1 \sum a_i x_i = \beta_0$$

It gives

\begin{equation} \label{eq:ule0}
  \begin{cases}
    \sum a_i = 1 \\
    \sum a_i x_i = 0
  \end{cases}
\end{equation}

Then

\begin{equation*}
  \begin{split}
    0 \le Var\Big(\sum a_iY_i - \hat\beta_0 \Big) & = Var\bigg[\sum a_iY_i - \sum\Big( \frac{1}{n} - \frac{(x_k - \bar{x})\bar{x}}{S_{XX}} \Big) Y_k \bigg] \\
    & = \sum \bigg(a_i - \frac{1}{n} +  \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \bigg)^2\sigma^2 \\
    & = \sum \bigg[ a_i^2 - 2a_i\Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big) + \Big( \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{XX}} \Big)^2 \bigg]\sigma^2 \\
		& = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n}\sum a_i + \frac{2\bar{x}\sigma^2\sum a_ix_i}{S_{XX}} - \frac{2\bar{x}^2\sigma^2\sum a_i}{S_{XX}} \\
		& \qquad + \sigma^2\bigg( \frac{1}{n} - \frac{2\bar{x}}{nS_{XX}} \sum(x_i - \bar{x}) + \frac{\bar{x}^2\sum(x_i - \bar{x})^2}{S_{XX}^2} \bigg) \\
		& = \sum a_i^2\sigma^2 -\frac{2\sigma^2}{n} - \frac{2\bar{x}^2\sigma^2}{S_{XX}} \qquad \because \eqref{eq:ule0} \\
		& \qquad + \bigg(\frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \qquad \because \sum(x_i - \bar{x}) = 0 \: \text{and} \: S_{XX} := \sum (x_i - \bar{x})^2 \\
		& = \sum a_i^2\sigma^2 - \bigg( \frac{1}{n} + \frac{\bar{x}^2}{S_{XX}} \bigg)\sigma^2 \\
		& = Var\Big( \sum a_i Y_i \Big) - Var\hat\beta_0
  \end{split}
\end{equation*}

Hence,

$$Var(\sum a_i Y_i) \ge Var(\hat\beta_0)$$
```


```{example, usingnormal}
Show that $\sum (Y_i - \hat{Y_i}) = 0$, $\sum x_i (Y_i - \hat{Y_i}) = 0$, and $\sum \hat{Y_i} (Y_i - \hat{Y_i}) = 0$.
```

```{solution}
Consider the two normal equations $\eqref{eq:normbeta0}$ and $\eqref{eq:normbeta1}$. Note that $\hat{Y_i} = \hat\beta_0 + \hat\beta_1 x_i$.

From the Equation $\eqref{eq:normbeta0}$, we have $\sum (Y_i - \hat{Y_i}) = 0$.

From the Equation $\eqref{eq:normbeta1}$, we have $\sum x_i (Y_i - \hat{Y_i}) = 0$.

It follows that

\begin{equation*}
  \begin{split}
    \sum \hat{Y_i} (Y_i - \hat{Y_i}) & = \sum (\hat\beta_0 + \hat\beta_1 x_i) (Y_i - \hat{Y_i}) \\
    & = \hat\beta_0 \sum (Y_i - \hat{Y_i}) + \hat\beta_1 \sum x_i (Y_i - \hat{Y_i}) \\
    & = 0
  \end{split}
\end{equation*}
```


### Estimation of $\sigma^2$

There is the last parameter, $\sigma^2 = Var(Y_i)$. In the *least squares estimation literary*, we estimate $\sigma^2$ by

\begin{equation} \label{eq:siglse}
  \hat\sigma^2 = \frac{1}{n - 2} \sum_{i = 1}^n (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2
\end{equation}

Why $n - 2$? This makes the estimator unbiased.

```{proposition, sigex, name = "Unbiasedness"}
$$E(\hat\sigma^2) = \sigma^2$$
```

```{proof}
Note that

$$(Y_i - \hat\beta_0 - \hat\beta_1 x_i) = (Y_i - \overline{Y}) - \hat\beta_1(x_i - \overline{x})$$

Then

\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) & = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \hat\beta_0 - \hat\beta_1 x_i)^2 \bigg] \\
    & = \frac{1}{n - 2} E \bigg[ \sum (Y_i - \overline{Y})^2 + \hat\beta_1^2 \sum (x_i - \overline{x})^2 -2\hat\beta_1 \sum (Y_i - \overline{Y})(x_i - \overline{x}) \bigg] \\
    & = \frac{1}{n - 2} E ( S_{YY} + \hat\beta_1^2 S_{XX} - 2 \hat\beta_1 S_{XY}) \\
    & = \frac{1}{n - 2} E ( S_{YY} - \hat\beta_1^2 S_{XX}) \qquad \because S_{XY} = \hat\beta_1 S_{XX} \\
    & = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{XX} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big)
  \end{split}
\end{equation*}

(a)

\begin{equation*}
  \begin{split}
    ES_{YY} & = E\Big[ \sum (Y_i - \overline{Y})^2 \Big] \\
    & = E \Big[ \sum \Big( (\beta_0 + \beta_1 x_i + \epsilon_i) - (\beta_0 + \beta_1 \overline{x} + \overline{\epsilon}) \Big)^2 \Big] \\
    & = E \Big[ \sum \Big( \beta_1 (x_i - \overline{x}) + (\epsilon_i - \overline{\epsilon}) \Big)^2 \Big] \\
    & = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) + 2\beta_1 \sum (x_i - \overline{x}) E(\epsilon_i - \overline{\epsilon}) \\
    & = \beta_1^2 S_{XX} + E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big)
  \end{split}
\end{equation*}

Since $E(\bar\epsilon) = 0$ and $Var(\bar\epsilon) = \frac{\sigma^2}{n}$,

\begin{equation*}
  \begin{split}
    E\Big( \sum (\epsilon_i - \overline{\epsilon})^2 \Big) & = E \Big( \sum (\epsilon_i^2 + \bar\epsilon^2 - 2\epsilon_i \bar\epsilon) \Big) \\
    & = \sum E(\epsilon_i^2) - n E(\bar\epsilon^2) \qquad \because \sum \epsilon = n \bar\epsilon \\
    & = \sum (Var(\epsilon_i) + E(\epsilon_i)^2) - n(Var(\bar\epsilon) + E(\bar\epsilon)^2) \\
    & = n\sigma^2 - \sigma^2 \\
    & = (n - 1)\sigma^2
  \end{split}
\end{equation*}

Thus,

$$ES_{YY} = \beta_1^2 S_{XX} + (n - 1)\sigma^2$$

(b)

\begin{equation*}
  \begin{split}
    E\hat\beta_1^2 & = Var\hat\beta_1 + E(\hat\beta_1)^2 \\
    & = \frac{\sigma^2}{S_{XX}} + \beta_1^2
  \end{split}
\end{equation*}

It follows that

\begin{equation*}
  \begin{split}
    E(\hat\sigma^2) & = \frac{1}{n - 2} \Big(  \underset{(a)}{\underline{ES_{YY}}} - S_{YY} \underset{(b)}{\underline{E\hat\beta_1^2}} \Big) \\
    & = \frac{1}{n - 2} \bigg( \Big(\beta_1^2 S_{XX} + (n - 1)\sigma^2 \Big) - S_{XX}\Big(\frac{\sigma^2}{S_{XX}} + \beta_1^2 \Big) \bigg) \\
    & = \frac{1}{n - 2}((n - 2)\sigma^2) \\
    & = \sigma^2
  \end{split}
\end{equation*}
```


## Maximum Likelihood Estimation

In this section, we add an assumption to an random errors $\epsilon_i$.

$$\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)$$

```{example, gmle, name = "Gaussian Likelihood"}
Note that $Y_i \stackrel{indep}{\sim} N(\beta_0 + \beta_1 x_i, \sigma^2)$. Then the likelihood function is

$$L(\beta_0, \beta_1, \sigma^2) = \prod_{i = 1}^n\bigg( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \bigg(- \frac{(Y_i - \beta_0 - \beta_1 x_i)^2}{2 \sigma^2} \bigg) \bigg)$$

and so the log-likelihood function can be computed as

$$l(\beta_0, \beta_1, \sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2$$
```


### Likelihood equations

```{definition, mledef, name = "Maximum Likelihood Estimator"}
$$(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) := \arg\sup L(\beta_0, \beta_1, \sigma^2)$$
```

Since $l(\cdot) = \ln L(\cdot)$ is monotone,

```{remark, mlemax}
$$(\hat\beta_0^{MLE}, \hat\beta_1^{MLE}, \hat\sigma^{2MLE}) = \arg\sup l(\beta_0, \beta_1, \sigma^2)$$
```

We can find the maximum of this *quadratic* function by making first derivative.

\begin{equation} \label{eq:mlbeta0}
  \frac{\partial l}{\partial \beta_0} = \frac{1}{\sigma^2} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i) = 0
\end{equation}

\begin{equation} \label{eq:mlbeta1}
  \frac{\partial l}{\partial \beta_1} = \frac{1}{\sigma^2} \sum_{i = 1}^n x_i (Y_i - \beta_0 - \beta_1 x_i) = 0
\end{equation}

\begin{equation} \label{eq:mlsig}
  \frac{\partial l}{\partial \sigma^2} = - \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = 0
\end{equation}

Denote that Equations $\eqref{eq:mlbeta0}$ and $\eqref{eq:mlbeta1}$ given $\hat\sigma^2$ are equivalent to the normal equations. Thus,

$$\hat\beta_0^{MLE} = \hat\beta_0^{LSE}, \quad \hat\beta_1^{MLE} = \hat\beta_1^{LSE}$$

From $\eqref{eq:mlsig}$,

$$\hat\sigma^{2MLE} = \frac{1}{n}\sum_{i = 1}^n(Y_i - \beta_0 - \beta_1 x_i)^2 = \frac{n - 2}{n} \hat\sigma^{2LSE}$$

Recall that $\hat\sigma^{2LSE}$ is an unbiased, i.e. this *MLE is not an unbiased estimator*. Since $\hat\sigma^{2MLE} \approx \hat\sigma^{2LSE}$ for large $n$, howerver, it is *asymptotically unbiased*.

```{theorem, rclb, name = "Rao-Cramer Lower Bound, univariate case"}
Let $X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta)$. If $\hat\theta$ is an unbiased estimator of $\theta$,

$$Var(\hat\theta) \ge \frac{1}{I_n(\theta)}$$

where $I_n(\theta) = -E\bigg(\frac{\partial^2 l(\theta)}{\partial \theta^2} \bigg)$
```

To apply this theorem \@ref(thm:rclb) in the simple linear regression setting, i.e. $(\beta_0, \beta_1)$, we need to look at the *bivariate case*.

```{theorem, rclb2, name = "Rao-Cramer Lower Bound, bivariate case"}
Let $X_1, \ldots, X_n \stackrel{iid}{\sim} f(x ; \theta1, \theta_2)$ and let $\boldsymbol{\theta} = (\theta_1, \theta_2)^T$. If each $\hat\theta_1$, $\hat\theta_2$ is an unbiased estimator of $\theta_1$ and $\theta_2$, then

$$
Var(\boldsymbol{\theta}) := \begin{bmatrix}
Var(\hat\theta_1) & Cov(\hat\theta_1, \hat\theta_2) \\
Cov(\hat\theta_1, \hat\theta_2) & Var(\hat\theta_2)
\end{bmatrix} \ge I_n^{-1}(\theta_1, \theta_2)
$$

where

$$
I_n(\theta_1, \theta_2) = - \begin{bmatrix}
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1^2} \bigg) & E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) \\
  E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_1 \partial \theta_2} \bigg) & E\bigg( \frac{\partial^2 l(\theta_1, \theta_2)}{\partial \theta_2^2} \bigg)
\end{bmatrix}
$$
```

Assume that $\sigma^2$ is **known**. From the Equations $\eqref{eq:mlbeta0}$ and $\eqref{eq:mlbeta1}$,

$$
\begin{cases}
  \frac{\partial^2 l}{\partial \beta_0^2} = - \frac{n}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_1^2} = - \frac{\sum x_i^2}{\sigma^2} \\
  \frac{\partial^2 l}{\partial \beta_0 \partial \beta_1} = - \frac{\sum x_i}{\sigma^2}
\end{cases}
$$

Thus,

$$
I_n(\beta_0, \beta_1) = \begin{bmatrix}
  \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} \\
  \frac{\sum x_i}{\sigma^2} & \frac{\sum x_i^2}{\sigma^2}
\end{bmatrix}
$$

Applying gaussian elimination,

\begin{equation*}
  \begin{split}
    \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} & 1 & 0 \\
      \frac{\sum x_i}{\sigma^2} & \frac{\sum x_i^2}{\sigma^2} & 0 & 1
    \end{array}
    \right] & \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} & 1 & 0 \\
      \frac{\sum x_i}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) & \frac{\sum x_i^2}{\sigma^2}\Big(\frac{n}{\sum x_i} \Big) & 0 & \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    & \leftrightarrow \left[
    \begin{array}{cc|cc}
      \frac{n}{\sigma^2} & \frac{\sum x_i}{\sigma^2} & 1 & 0 \\
      0 & \frac{\sum x_i^2 - \overline{x}\sum x_i}{\sigma^2\overline{x}} = \frac{S_{XX}}{\sigma^2\overline{x}} & -1 & \frac{1}{\overline{x}}
    \end{array}
    \right] \\
    & \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 & \overline{x} & \frac{\sigma^2}{n} & 0 \\
      0 & 1 & -\frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right] \\
    & \leftrightarrow \left[
    \begin{array}{cc|cc}
      1 & 0 & \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 & -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
      0 & 1 & -\frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
    \end{array}
    \right]
  \end{split}
\end{equation*}

Hence,

$$
I_n^{-1}(\beta_0, \beta_1) = \begin{bmatrix}
  \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2 & -\frac{\overline{x}}{S_{XX}}\sigma^2 \\
  -\frac{\overline{x}}{S_{XX}}\sigma^2 & \frac{\sigma^2}{S_{XX}}
\end{bmatrix} = \begin{bmatrix}
  Var(\hat\beta_0) & Cov(\hat\beta_0, \hat\beta_1) \\
  Cov(\hat\beta_0, \hat\beta_1) & Var(\hat\beta_1)
\end{bmatrix}
$$

Since $Var(\boldsymbol{\hat\beta}) - I^{-1} = 0$ is non-negative definite, each $Var(\hat\beta_0) = \bigg(\frac{1}{n} + \frac{\overline{x}^2}{S_{XX}} \bigg)\sigma^2$ and $Var(\hat\beta_1) = \frac{\sigma^2}{S_{XX}}$ is a theoretical bound.

```{remark, lsemle}
This says that $\hat\beta_0^{LSE} = \hat\beta_0^{MLE}$ and $\hat\beta_1^{LSE} = \hat\beta_1^{MLE}$ have the smallest variance among all unbiased estimator.
```

This result is *stronger than Gauss-Markov theorem* \@ref(thm:gmt), where the LSE has the smalleset variance among all *linear unbiased* estimators. It can be simply obtained from the *Lehmann-Scheffe Theorem*: If some unbiased estimator is a function of complete sufficient statistic, then this estimator is the unique MVUE [@Hogg:2018aa].

```{remark, regcss, name = "Lehmann and Scheffe for regression coefficients"}
$u\Big(\sum Y_i, S_{XY} \Big)$ is CSS in this regression problem, i.e. known $\sigma^2$.
```

```{proof}
From the example \@ref(exm:gmle),

\begin{equation*}
  \begin{split}
    L(\beta_0, \beta_1) & = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum(Y_i - \beta_0 - \beta_1 x_i)^2 \bigg] \\
    & = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \sum \Big(Y_i^2 - (\beta_0 + \beta_1 x_i)Y_i + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg] \\
    & = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\bigg[-\frac{1}{2\sigma^2} \Big( -\beta_0 \sum Y_i - \beta_1 \sum x_i Y_i \Big) \bigg] \exp\bigg[-\frac{1}{2\sigma^2} \Big( \sum Y_i^2 + (\beta_0 + \beta_1 x_i)^2 \Big) \bigg]
  \end{split}
\end{equation*}

By the Factorization theorem, both $\sum Y_i$ and $\sum x_i Y_i$ are sufficient statistics. Since $S_{XY}$ is one-to-one function of $\sum x_i Y_i$, it is also a sufficient statistic.

Denote that the normal distribution is in exponential family.

Hence, $(\sum Y_i, S_{XY})$ are CSS.
```


## Residuals

```{definition, res, name = "Residuals"}
$$e_i := Y_i - \hat{Y_i}$$
```

### Prediction error

```{r regplot, fig.cap=fig$cap("regplot", "Fit and residuals")}
delv %>% 
  mutate(yhat = predict(lm(y ~ x))) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point() +
  geom_linerange(aes(ymin = y, ymax = yhat), col = I("red"), alpha = .7) +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```

See $`r fig$ref("regplot")`$. Each red line is $e_i$. As we can see, $e_i$ represents the difference between *observed* response and *predicted* response. A large $\lvert e_i \rvert$ indicates a large prediction error. You can call this $e_i$ for each $Y_i$ by `lm()$residuals` or `residuals()`.

```{r}
delv_fit <- lm(y ~ x, data = delv)
delv_fit$residuals
```

$\sum e_i^2$, which has been minimized in the procedure of LSE, can be used to see *overall size of prediction errors*.

```{definition, sse, name = "Residual Sum of Squares"}
$$SSE := \sum_{i = 1}^n e_i^2$$
```

### Residuals and the variance

$e_i$ is a random quantity, which contains the information for $\epsilon_i$. $\sum e_i^2$ can give information about $\sigma^2 = Var(\epsilon_i)$. For this, it is expected that $e_i$ and $\epsilon_i$ have similar feature.


```{lemma, yandbet}
Covriance between Y and each coefficient

$\text{(a)}\: Cov(\hat\beta_0, Y_i) = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2$

$\text{(b)}\: Cov(\hat\beta_1, Y_i) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2$
```

```{proof}
(a)

\begin{equation*}
  \begin{split}
    Cov(\hat\beta_0, Y_i) & = Cov(\sum a_i Y_i, Y_i) \\
    & = \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2
  \end{split}
\end{equation*}

(b)

\begin{equation*}
  \begin{split}
    Cov(\hat\beta_1, Y_i) & = Cov(\sum b_i Y_i, Y_i) \\
    & = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2
  \end{split}
\end{equation*}
```


```{proposition, resprop, name = "Properties of residuals"}
Mean and variance of the residual

$\text{(a)}\: E(e_i) = 0$

$\text{(b)}\: Var(e_i) \neq \sigma^2$

$\text{(c)}\: \forall i \neq j : Cov(e_i, e_j) \neq 0$
```

```{proof}
(a) Recall that this is the assumption of the regression model.

(b) Lemma \@ref(lem:yandbet) implies that

\begin{equation*}
  \begin{split}
    Cov(\overline{Y}, \hat\beta_1) & = Cov(\frac{1}{n}\sum Y_i, \hat\beta_1) \\
    & = \frac{1}{n} \sum_{i = 1}^n \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    & = 0 \qquad \because \sum (x_i - \overline{x}) = 0
  \end{split}
\end{equation*}

Then

\begin{equation} \label{eq:predvar}
  \begin{split}
    Var(\hat{Y_i}) & = Var(\hat\beta_0 + \hat\beta_1 x_i) \\
    & = Var \bigg[ \overline{Y} + (x_i - \overline{x}) \hat\beta_1 \bigg] \qquad \because \hat\beta_0 = \overline{Y} - \hat\beta_1 \overline{x} \\
    & = Var(\overline{Y}) + (x_i - \overline{x})^2 Var(\hat\beta_1) + 2(x_i - \overline{x}) Cov(\overline{Y}, \hat\beta_1) \\
    & = \frac{\sigma^2}{n} + (x_i - \overline{x})^2\frac{\sigma^2}{S_{XX}} + 0 \\
    & = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
\end{equation}

From the same lemma \@ref(lem:yandbet),
    
\begin{equation} \label{eq:yyhat}
  \begin{split}
    Cov(Y_i, \hat{Y_i}) & = Cov(Y_i, \overline{Y} + (x_i - \overline{x}) \hat\beta_1) \\
    & = Cov(Y_i, \overline{Y}) + (x_i - \overline{x}) Cov(Y_i, \hat\beta_1) \\
    & = \frac{\sigma^2}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}}\sigma^2 \qquad \because Cov(Y_i, \hat\beta_1) = \frac{(x_i - \overline{x})}{S_{XX}}\sigma^2 \\
    & = \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2
  \end{split}
\end{equation}

These $\eqref{eq:predvar}$ and $\eqref{eq:yyhat}$ give that

\begin{equation*}
  \begin{split}
    Var(e_i) & = Var(Y_i) + Var(\hat{Y_i}) -2Cov(Y_i, \hat{Y_i}) \\
    & = \sigma^2 + \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 - 2 \bigg( \frac{1}{n} + \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    & = \bigg(1 - \frac{1}{n} - \frac{(x_i - \overline{x})^2}{S_{XX}} \bigg)\sigma^2 \\
    & \neq \sigma^2
  \end{split}
\end{equation*}

(c) Let $i \neq j$. Then

\begin{equation*}
  \begin{split}
    Cov(e_i, e_j) & = Cov\Big( Y_i - (\hat\beta_0 + \hat\beta_1 x_i), Y_j - (\hat\beta_0 + \hat\beta_1 x_j) \Big) \\
    & = Cov(Y_i, Y_j) - Cov\Big(Y_i, (\hat\beta_0 + \hat\beta_1 x_j) \Big) - Cov((\hat\beta_0 + \hat\beta_1 x_i), Y_j) + Cov((\hat\beta_0 + \hat\beta_1 x_i), (\hat\beta_0 + \hat\beta_1 x_j)) \\
    & = 0 - \bigg( \frac{1}{n} - \frac{(x_i - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_j}{S_{XX}}\sigma^2 \\
    & \qquad - \bigg( \frac{1}{n} - \frac{(x_j - \overline{x})\overline{x}}{S_{XX}} \bigg)\sigma^2 - \frac{(x_i - \overline{x})x_i}{S_{XX}}\sigma^2 \\
    & \qquad + \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    & = - \bigg( \frac{1}{n} + \frac{\overline{x}^2 + x_i x_j - \overline{x}(x_i + x_j)}{S_{XX}} \bigg)\sigma^2 \\
    & \neq 0
  \end{split}
\end{equation*}
```


## Decomposition of Total Variability

### Total sum of squares

```{definition, unsst, name = "Uncorrected Total Sum of Squares"}
$$SST_{uncor} := \sum_{i = 1}^n Y_i^2$$
```

```{definition, sst, name = "Corrected Total Sum of Squares"}
$$SST := \sum_{i = 1}^n (Y_i - \overline{Y})^2$$
```

What does this total sum of squares mean? To know this, we should know $\overline{Y}$ first.

```{r ybarpred, fig.cap=fig$cap("ybarpred", "Regression without predictor")}
delv %>% 
  ggplot(aes(x = x, y = y)) +
  geom_smooth(method = "lm", formula = y ~ 1, se = FALSE) +
  geom_point() +
  labs(
    x = "Number of Cases",
    y = "Delivery Time"
  )
```

See $`r fig$ref("ybarpred")`$. The line represents the closest line when we use only intercept term for the regression model. In other words, *if we use no information for the response*, i.e. no predictor variables, we will get just average of the response variable. Consider

$$Y_i = \beta_0 + \epsilon_i$$

Then we can get only one normal equation

$$\sum (Y_i - \hat\beta_0) = 0$$

Hence,

$$\hat\beta_0 = \frac{1}{n} \sum_{i = 1}^n Y_i \equiv \overline{Y}$$

From this fact, $SST$ implies **total variance**.

### Regression sum of squares

```{definition, ssr, name = "Regression Sum of Squares"}
$$SSR := \sum_{i = 1}^n (hat{Y_i} - \overline{Y})^2$$
```

This $SSR$ compares $\hat{Y_i}$ versus $\overline{Y}$, computing the sum of squares for difference between predicted values from *regression model* and *model not using predictors*.

### Residual sum of squares

Now consider the *residual sum of squares* $SSE$ in the definition \@ref(def:sse). As mentioned, this is related to the *prediction errors*, which the regression model could not explain the data.

### Decomposition of total sum of squares {#decompsst}

$SST$ can be decomposed by construction of sum of squares.

```{proposition, decom, name = "Decomposition of SST"}
$$SST = SSR + SSE$$

where $SST = \sum (Y_i - \overline{Y})^2$, $SSR = \sum (\hat{Y_i} - \overline{Y})^2$, and $SSE = \sum (Y_i - \hat{Y_i})$
```

```{proof}
From the Example \@ref(exm:usingnormal),

\begin{equation*}
  \begin{split}
    \sum_{i = 1}^n (Y_i - \overline{Y})^2 & = \sum_{i = 1}^n (Y_i - \hat{Y_i} + \hat{Y_i} - \overline{Y})^2 \\
    & = \sum_{i = 1}^n (Y_i - \hat{Y_i})^2 + 2 \sum_{i = 1}^n (Y_i - \hat{Y_i})(\hat{Y_i} - \overline{Y}) + \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2 \\
    & = \sum_{i = 1}^n (Y_i - \hat{Y_i})^2 + \sum_{i = 1}^n (\hat{Y_i} - \overline{Y})^2 \qquad \because \sum (Y_i - \hat{Y_i}) = 0 \: \text{and} \: \sum (Y_i - \hat{Y_i})\hat{Y_i} = 0
  \end{split}
\end{equation*}
```

This represents each $SSR$ and $SSE$ divides total variability as following.

$$\overset{SST}{\text{total variability}} = \overset{SSR}{\text{left unexplained by regression}} + \overset{SSE}{\text{explained by regression}}$$

Denote that the total variability $SST$ is *constant given data set*. If our model is good, $SSR$ grows and $SSE$ flattens. Thus the larger $SSR$ is, the better. The lower $SSE$ is, the better.

### Coefficient of determination

We have discussed in the previous section \@ref(decompsst) that $SSR$ and $SSE$ splits the total variability into *explained part and not-explained part by our regression model*. Our first interest is whether the model works well for the data well, so we can think about the *proportion of explained part to the total variance*. The following measure $R^2$ computes this kind of value.

```{definition, rsq, name = "Coefficient of Determination"}
$$R^2 := \frac{SSR}{SST} = 1 - \frac{1 - SSE}{SST}$$
```

By construction,

$$0 \le R^2 \le 1$$

As $R^2$ goes to $0$, the model goes wrong. As $R^2$ is close to $1$, large proportion of variability has been explained. So we prefer large values rather than small.

```{proposition, rsqlin}
$R^2$ shows the strength of linear relation between two variables $x$ and $Y$ in the simple linear regression.

$$R^2 = \hat\rho_{XY}$$

where $\hat\rho_{XY} := \frac{\sum (X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum (X_i - \overline{X})^2} \sqrt{\sum (Y_i - \overline{Y})^2}}$ is the sample correlation coefficients
```

```{proof}
Note that $\hat{Y_i} - \overline{Y} = \hat\beta_1 (x_i - \overline{x}) = \frac{S_{XY}}{S_{XX}} (x_i - \overline{x})$. Then

\begin{equation*}
  \begin{split}
    \sum (\hat{Y_i} - \overline{Y})^2 & = \frac{S_{XY}^2}{S_{XX}^2} \sum (x_i - \overline{x})^2 \\
    & = \frac{S_{XY}^2}{S_{XX}}
  \end{split}
\end{equation*}

It follows that

\begin{equation*}
  \begin{split}
    R^2 & = \frac{\sum (\hat{Y_i} - \overline{Y})^2}{\sum (Y_i - \overline{Y})^2} \\
    & = \frac{S_{XY}^2}{S_{XX}S_{YY}} \\
    & =: \hat\rho_{XY}^2
  \end{split}
\end{equation*}
```

In this relation, we can know that $R^2$ statistic performs as a measure of the linear relationship in the simple linear regression setting.


## Geometric Interpretations

### Fundamental subspaces

These linear algebra concepts might be more useful for *multiple linear regression*, but let's briefly recap [@Leon:2014aa].

```{definition, subspace, name = "Fundamental Subspaces"}
Let $X \in \mathbb{R}^{n \times (p + 1)}$.

Then the Null space is defined by

$$N(X) := \{ \mathbf{b} \in \mathbb{R}^n \mid X\mathbf{b} = \mathbf{0} \}$$

The Row space is defined by

$$Row(X) := sp(\{\mathbf{r}_1, \ldots, \mathbf{r}_{p + 1} \}) \quad \text{where} \: X^T = [\mathbf{r}_1^T, \ldots, \mathbf{r}_{n}^T]$$

The Column space is defined by

$$Col(X) := sp(\{\mathbf{c}_1, \ldots, \mathbf{c}_{n} \}) \quad \text{where} \: X = [\mathbf{c}_1, \ldots, \mathbf{c}_{p + 1}]$$

The Range of $X$ is defined by

$$R(X) := \{ \mathbf{y} \in \mathbb{R}^n \mid \mathbf{y} = X\mathbf{b} \quad \text{for some} \: \mathbf{b} \in \mathbb{R}^{p + 1} \}$$
```

These spaces have some constructional relationship.

```{theorem, fundsub, name = "Fundamental Subspaces Theorem"}
Let $X \in \mathbb{R}^{n \times (p + 1)}$. Then

$$N(X) = R(X^T)^{\perp} = Col(X^T)^{\perp} = Row(X)^{\perp}$$

Transposed matrix also satisfy this.

$$N(X^T) = R(X)^{\perp} = Col(X)^{\perp}$$
```

```{proof}
Let $\mathbf{a} \in N(X)$. Then $X\mathbf{a} = \mathbf{0}$.

Let $\mathbf{y} \in R(X^T)$. Then $X^T \mathbf{b} = \mathbf{y}$ for some $\mathbf{b} \in \mathbb{R}^{p + 1}$.

Choose $\mathbf{b} \in \mathbb{R}^{p + 1}$ such that $X^T \mathbf{b} = \mathbf{y}$. Then

\begin{equation*}
  \begin{split}
    \mathbf{0} & = X\mathbf{a} \\
    & = \mathbf{b}^T X\mathbf{a} \\
    & = \mathbf{y}^T \mathbf{a}
  \end{split}
\end{equation*}

Hence,

$$N(X) \perp R(X^T)$$

Since

$$X^T \mathbf{b} = \mathbf{c}_1 \mathbf{b} + \cdots + \mathbf{c}_{p + 1} \mathbf{b}$$

it is trivial that $R(X) = Col(X)$ and $R(X^T) = Col(X^T)$.

If $\mathbf{a} \in N(X)$, then

$$
X\mathbf{a} = \begin{bmatrix}
  \mathbf{r}_1 \\
  \mathbf{r}_2 \\
  \cdots \\
  \mathbf{r}_n
\end{bmatrix} \begin{bmatrix}
  a_1 \\
  \cdots \\
  a_{p + 1}
\end{bmatrix} = \begin{bmatrix}
  0 \\
  0 \\
  \cdots \\
  0
\end{bmatrix}
$$

Thus,

$$\forall i :  \mathbf{a}^T \mathbf{r}_i = 0$$

and so

$$N(X) \subseteq Row(X)^{\perp}$$

Conversely, if $\mathbf{a} \in Row(X)^{\perp}$, then $\forall i :  \mathbf{a}^T \mathbf{r}_i = 0$. This implies that $X\mathbf{a} = \mathbf{0}$. Thus,

$$Row(X)^{\perp} \subseteq N(X)$$

and so

$$N(X) = Row(X)^{\perp}$$
```

$N(X^T) = R(X)^{\perp}$ part in Theorem \@ref(thm:fundsub) will give the geometric insight to *least squares solution*.

```{theorem, perpbasis}
Let $S$ be a subspace of $\mathbb{R}^n$. Then

$$dim S + dim S^{\perp} = n$$

If $\{ \mathbf{x}_1, \ldots, \mathbf{r} \}$ is a basis for $S$ and $\{ \mathbf{x}_{r + 1}, \ldots, \mathbf{n} \}$ is a basis for $S^{\perp}$, then $\{ \mathbf{x}_1, \ldots, \mathbf{x}_r, \mathbf{x}_{r + 1}, \ldots, \mathbf{n} \}$ is a basis for $\mathbb{R}^n$.
```

```{theorem, dsum}
Let $S$ be a subspace of $\mathbb{R}^n$. Then

$$\mathbb{R}^n = S \oplus S^{\perp}$$
```


### Simple linear regression

```{theorem, projection}
Let $S$ be a subspace of $\mathbb{R}^n$. For each $\mathbf{y} \in \mathbf{R}^n$, there exists a unique $\mathbf{p} \in S$ that is closest to $\mathbf{y}$, i.e.

$$\Vert \mathbf{y} - \mathbf{p}  \Vert > \Vert \mathbf{y} - \mathbf{\hat{y}} \Vert$$

for any $\mathbf{p} \neq \mathbf{\hat{y}}$. Furthermore, a given vector $\mathbf{p} \in S$ will be the closest to a given vector $\mathbf{y} \in \mathbb{R}^n$ if and only if

$$\mathbf{y} - \mathbf{\hat{y}} \in S^{\perp}$$
```

Least square estimator $(\hat\beta_0, \hat\beta_1)^T$ minimizes

$$\sum_{i = 1}^n (Y_i - \beta_0 - \beta_1 x_i)^2 = \Vert \mathbf{Y} - (\beta_0 \mathbf{1} + \beta_1 \mathbf{x}) \Vert^2$$

with respect to $(\hat\beta_0, \hat\beta_1)^T \in \mathbb{R}^2$ (where $\mathbf{1} := (1, 1)^T$). Recall that the normal equation gives

$$\sum_{i = 1}^n(Y_i - \hat\beta_0 - \hat\beta_1 x_i) = \Big( \mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \Big)^T \mathbf{1} = 0$$

and

$$\sum_{i = 1}^n (Y_i - \hat\beta_0 - \hat\beta_1 x_i)x_i = \Big( \mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \Big)^T \mathbf{x} = 0$$

These two relation give

$$\mathbf{Y} - (\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}) \perp sp(\{ \mathbf{1}, \mathbf{x} \})^{\perp}$$

i.e. $\mathbf{\hat{Y}} = \hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x}$ is the projection of $\mathbf{Y}$.

Theorem \@ref(thm:projection) can give the same result.

$$\hat\beta_0 \mathbf{1} + \hat\beta_1 \mathbf{x} \in R([\mathbf{1}, \mathbf{x}])^{\perp} = sp(\{ \mathbf{1}, \mathbf{x} \})^{\perp}$$


```{r simpledraw, echo=FALSE, fig.cap=fig$cap("simpledraw", "Geometric Illustration of Simple Linear Regression")}
tibble(
  z1 = seq(0, 5, length.out = 100),
  z2 = seq(0, 3, length.out = 100),
  y = seq(0, 2, length.out = 100)
) %>% 
  mutate(z2y = z2, yy = 5/2*y) %>%
  mutate(yhat = 1/2*y) %>%
  ggplot() +
  geom_line(aes(z1, 0), size = 1, arrow = arrow()) +
  geom_line(aes(z2, z2y), size = .3, arrow = arrow()) +
  geom_line(aes(y, yy), arrow = arrow()) +
  geom_line(aes(y, yhat), size = .5, arrow = arrow(), linetype = "dashed", col = I("red")) +
  geom_segment(aes(x = 2, y = 1, xend = 2, yend = 5), size = .4, linetype = "dashed", col = I("red")) +
  # geom_segment(aes(x = 1.5, y = 1.5, xend = 2, yend = 5), size = .2, linetype = "dashed", col = I("blue")) +
  # geom_segment(aes(x = 1.8, y = 0, xend = 2, yend = 5), size = .5, linetype = "dashed", col = I("blue")) +
  # geom_segment(aes(x = 1.5, y = 1.5, xend = 2, yend = 1), size = .25, linetype = "dashed", col = I("blue")) +
  # geom_segment(aes(x = 1.8, y = 0, xend = 2, yend = 1), size = .45, linetype = "dashed", col = I("blue")) +
  annotate("text", x = 5, y = .2, label = "bold(1)", parse = T) +
  annotate("text", x = 3, y = 3.2, label = "bold(x)[1]", parse = T) +
  annotate("text", x = 1.8, y = 5, label = "bold(y)", parse = T) +
  annotate("text", x = 2.7, y = 1, label = "hat(bold(y))^LSE == bar(italic(y))*bold(1) + hat(beta)[0]*bold(1) + hat(beta)[1]*bold(x)[1]", parse = T, col = I("red")) +
  # annotate("text", x = 1.4, y = 1.6, label = "hat(beta)[2]", parse = T, col = I("blue")) +
  # annotate("text", x = 2, y = .2, label = "hat(beta)[1]", parse = T, col = I("blue")) +
  annotate("text", x = 0, y = .3, label = "bar(italic(y))*bold(1)", parse = T) +
  annotate("text", x = 3.5, y = 2, label = "Col(X)", size = 5) +
  theme(axis.line = element_blank(), axis.text = element_blank(), axis.title = element_blank())
```

We can see the details from $`r fig$ref("simpledraw")`$. In fact, decomposition of $SST$ and $R^2$ are also in here.

```{r simpledraw2, echo=FALSE, fig.cap=fig$cap("simpledraw2", "Geometric Illustration of Decomposing SST")}
tibble(
  z1 = seq(0, 5, length.out = 100),
  z2 = seq(0, 3, length.out = 100),
  y = seq(0, 2, length.out = 100)
) %>% 
  mutate(z2y = z2, yy = 5/2*y) %>%
  mutate(yhat = 1/2*y) %>%
  ggplot() +
  geom_line(aes(z1, 0), size = 1, arrow = arrow()) +
  geom_line(aes(z2, z2y), size = .3, arrow = arrow()) +
  geom_line(aes(y, yy), arrow = arrow()) +
  geom_line(aes(y, yhat), size = .5, arrow = arrow(), linetype = "dashed", col = I("red")) +
  geom_segment(aes(x = 2, y = 1, xend = 2, yend = 5), size = .4, linetype = "dashed", col = I("red")) +
  annotate("text", x = 5, y = .2, label = "bold(1)", parse = T) +
  annotate("text", x = 3, y = 3.2, label = "bold(x)[1]", parse = T) +
  annotate("text", x = 1.8, y = 5, label = "bold(y)", parse = T) +
  annotate("text", x = 2.7, y = 1, label = "hat(bold(y))^LSE == bar(italic(y))*bold(1) + hat(beta)[0]*bold(1) + hat(beta)[1]*bold(x)[1]", parse = T, col = I("red")) +
  annotate("text", x = 0, y = .3, label = "bar(italic(y))*bold(1)", parse = T) +
  annotate("text", x = 3.5, y = 2, label = "Col(X)", size = 5) +
  annotate("text", x = 1, y = 3, label = "sqrt(SST)", size = 5, parse = T) +
  annotate("text", x = 1.5, y = 1, label = "sqrt(SSR)", size = 5, parse = T, col = I("red")) +
  annotate("text", x = 2, y = 3, label = "sqrt(SSE)", size = 5, parse = T, col = I("red")) +
  theme(axis.line = element_blank(), axis.text = element_blank(), axis.title = element_blank())
```

See $`r fig$ref("simpledraw2")`$.

$$
\begin{cases}
  SST = \lVert \mathbf{Y} - \overline{Y} \mathbf{1} \rVert^2 \\
  SSR = \lVert \mathbf{\hat{Y}} - \overline{Y} \mathbf{1} \rVert^2 \\
  SSE = \lVert \mathbf{Y} - \mathbf{\hat{Y}} \rVert^2
\end{cases}
$$

Pythagorean law implies that

$$SST = SSR + SSE$$

Also,

$$R^2 = \frac{SSR}{SST} = cos^2\theta = \hat\rho_{XY}^2$$


## Distributions






